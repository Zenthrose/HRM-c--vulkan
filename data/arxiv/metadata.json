{
  "total_papers": 200,
  "categories": [
    "cs.AI",
    "cs.LG",
    "cs.CL",
    "cs.NE",
    "stat.ML",
    "math.ST",
    "physics.comp-ph",
    "quant-ph"
  ],
  "download_date": "2025-12-04 04:09:28",
  "total_characters": 267774,
  "papers": [
    {
      "id": "2512.04072v1",
      "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
      "abstract": "Reasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren't exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These \"silver\" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory SFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2) cognitive skills are indeed used by the model; (3) RLed SkillFactory models are more robust to regression on out-of-domain tasks than RLed base models. Our work suggests that inductive biases learned prior to RL help models learn robust cognitive skill use.",
      "authors": [
        "Zayne Sprague",
        "Jack Lu",
        "Manya Wadhwa",
        "Sedrick Keh",
        "Mengye Ren",
        "Greg Durrett"
      ],
      "published_date": "2025-12-03",
      "full_text": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors\n\nReasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren't exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These \"silver\" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory SFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2) cognitive skills are indeed used by the model; (3) RLed SkillFactory models are more robust to regression on out-of-domain tasks than RLed base models. Our work suggests that inductive biases learned prior to RL help models learn robust cognitive skill use."
    },
    {
      "id": "2512.04065v1",
      "title": "Fare Comparison App of Uber, Ola and Rapido",
      "abstract": "In todays increasing world, it is very important to have good hailing services like Ola, Uber, and Rapido as it is very essential for our daily transportation. Users often face difficulties in choosing the most appropriate and efficient ride that would lead to both cost-effective and would take us to our destination in less time. This project provides you with the web application that helps you to select the most beneficial ride for you by providing users with the fare comparison between Ola, Uber, Rapido for the destination entered by the user. The backend is use to fetch the data, providing users with the fare comparison for the ride and finally providing with the best option using Python. This research paper also addresses the problem and challenges faced in accessing the data using APIs, Android Studios emulator, Appium and location comparison. Thus, the aim of the project is to provide transparency to the users in ride-hailing services and increase efficiency and provide users with better experience.",
      "authors": [
        "Ashlesha Gopinath Sawant",
        "Sahil S. Jadhav",
        "Vidhan R. Jain",
        "Shriraj S. Jagtap",
        "Prachi Jadhav",
        "Soham Jadhav",
        "Ichha Raina"
      ],
      "published_date": "2025-12-03",
      "full_text": "Fare Comparison App of Uber, Ola and Rapido\n\nIn todays increasing world, it is very important to have good hailing services like Ola, Uber, and Rapido as it is very essential for our daily transportation. Users often face difficulties in choosing the most appropriate and efficient ride that would lead to both cost-effective and would take us to our destination in less time. This project provides you with the web application that helps you to select the most beneficial ride for you by providing users with the fare comparison between Ola, Uber, Rapido for the destination entered by the user. The backend is use to fetch the data, providing users with the fare comparison for the ride and finally providing with the best option using Python. This research paper also addresses the problem and challenges faced in accessing the data using APIs, Android Studios emulator, Appium and location comparison. Thus, the aim of the project is to provide transparency to the users in ride-hailing services and increase efficiency and provide users with better experience."
    },
    {
      "id": "2512.04047v1",
      "title": "Polarization by Design: How Elites Could Shape Mass Preferences as AI Reduces Persuasion Costs",
      "abstract": "In democracies, major policy decisions typically require some form of majority or consensus, so elites must secure mass support to govern. Historically, elites could shape support only through limited instruments like schooling and mass media; advances in AI-driven persuasion sharply reduce the cost and increase the precision of shaping public opinion, making the distribution of preferences itself an object of deliberate design. We develop a dynamic model in which elites choose how much to reshape the distribution of policy preferences, subject to persuasion costs and a majority rule constraint. With a single elite, any optimal intervention tends to push society toward more polarized opinion profiles - a ``polarization pull'' - and improvements in persuasion technology accelerate this drift. When two opposed elites alternate in power, the same technology also creates incentives to park society in ``semi-lock'' regions where opinions are more cohesive and harder for a rival to overturn, so advances in persuasion can either heighten or dampen polarization depending on the environment. Taken together, cheaper persuasion technologies recast polarization as a strategic instrument of governance rather than a purely emergent social byproduct, with important implications for democratic stability as AI capabilities advance.",
      "authors": [
        "Nadav Kunievsky"
      ],
      "published_date": "2025-12-03",
      "full_text": "Polarization by Design: How Elites Could Shape Mass Preferences as AI Reduces Persuasion Costs\n\nIn democracies, major policy decisions typically require some form of majority or consensus, so elites must secure mass support to govern. Historically, elites could shape support only through limited instruments like schooling and mass media; advances in AI-driven persuasion sharply reduce the cost and increase the precision of shaping public opinion, making the distribution of preferences itself an object of deliberate design. We develop a dynamic model in which elites choose how much to reshape the distribution of policy preferences, subject to persuasion costs and a majority rule constraint. With a single elite, any optimal intervention tends to push society toward more polarized opinion profiles - a ``polarization pull'' - and improvements in persuasion technology accelerate this drift. When two opposed elites alternate in power, the same technology also creates incentives to park society in ``semi-lock'' regions where opinions are more cohesive and harder for a rival to overturn, so advances in persuasion can either heighten or dampen polarization depending on the environment. Taken together, cheaper persuasion technologies recast polarization as a strategic instrument of governance rather than a purely emergent social byproduct, with important implications for democratic stability as AI capabilities advance."
    },
    {
      "id": "2512.04044v1",
      "title": "MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM Watermarking",
      "abstract": "Watermarking aims to embed hidden signals in generated text that can be reliably detected when given access to a secret key. Open-weight language models pose acute challenges for such watermarking schemes because the inference-time interventions that dominate contemporary approaches cannot be enforced once model weights are public. Existing watermaking techniques for open-weight models, such as the recently proposed GaussMark, typically rely on small modifications to model weights, which can yield signals detectable to those equipped with a secret key, but achieving detection power comparable to inference-time watermarks generally requires weight perturbations that noticeably reduce generation quality. We introduce MarkTune, a theoretically principled, on-policy fine-tuning framework that treats the GaussMark signal as a reward while simultaneously regularizing against degradation in text quality. We derive MarkTune as an improvement on GaussMark and demonstrate that MarkTune consistently improves the quality-detectability trade-off over GaussMark by steering finer-grained, watermark-aware weight updates within the model's representation space while preserving generation quality. Empirically, we show that MarkTune pushes the quality-detectability frontier of GaussMark close to that of inference-time watermarking, remains robust to paraphrasing and fine-tuning attacks, and exhibits strong generalization: a model fine-tuned on one dataset retains substantial watermark detection power on unseen datasets. Together, these results establish MarkTune as a general strategy for embedding robust, high-quality watermarks into open-weight LMs.",
      "authors": [
        "Yizhou Zhao",
        "Zhiwei Steven Wu",
        "Adam Block"
      ],
      "published_date": "2025-12-03",
      "full_text": "MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM Watermarking\n\nWatermarking aims to embed hidden signals in generated text that can be reliably detected when given access to a secret key. Open-weight language models pose acute challenges for such watermarking schemes because the inference-time interventions that dominate contemporary approaches cannot be enforced once model weights are public. Existing watermaking techniques for open-weight models, such as the recently proposed GaussMark, typically rely on small modifications to model weights, which can yield signals detectable to those equipped with a secret key, but achieving detection power comparable to inference-time watermarks generally requires weight perturbations that noticeably reduce generation quality. We introduce MarkTune, a theoretically principled, on-policy fine-tuning framework that treats the GaussMark signal as a reward while simultaneously regularizing against degradation in text quality. We derive MarkTune as an improvement on GaussMark and demonstrate that MarkTune consistently improves the quality-detectability trade-off over GaussMark by steering finer-grained, watermark-aware weight updates within the model's representation space while preserving generation quality. Empirically, we show that MarkTune pushes the quality-detectability frontier of GaussMark close to that of inference-time watermarking, remains robust to paraphrasing and fine-tuning attacks, and exhibits strong generalization: a model fine-tuned on one dataset retains substantial watermark detection power on unseen datasets. Together, these results establish MarkTune as a general strategy for embedding robust, high-quality watermarks into open-weight LMs."
    },
    {
      "id": "2512.04039v1",
      "title": "Fast & Efficient Normalizing Flows and Applications of Image Generative Models",
      "abstract": "This thesis presents novel contributions in two primary areas: advancing the efficiency of generative models, particularly normalizing flows, and applying generative models to solve real-world computer vision challenges. The first part introduce significant improvements to normalizing flow architectures through six key innovations: 1) Development of invertible 3x3 Convolution layers with mathematically proven necessary and sufficient conditions for invertibility, (2) introduction of a more efficient Quad-coupling layer, 3) Design of a fast and efficient parallel inversion algorithm for kxk convolutional layers, 4) Fast & efficient backpropagation algorithm for inverse of convolution, 5) Using inverse of convolution, in Inverse-Flow, for the forward pass and training it using proposed backpropagation algorithm, and 6) Affine-StableSR, a compact and efficient super-resolution model that leverages pre-trained weights and Normalizing Flow layers to reduce parameter count while maintaining performance. The second part: 1) An automated quality assessment system for agricultural produce using Conditional GANs to address class imbalance, data scarcity and annotation challenges, achieving good accuracy in seed purity testing; 2) An unsupervised geological mapping framework utilizing stacked autoencoders for dimensionality reduction, showing improved feature extraction compared to conventional methods; 3) We proposed a privacy preserving method for autonomous driving datasets using on face detection and image inpainting; 4) Utilizing Stable Diffusion based image inpainting for replacing the detected face and license plate to advancing privacy-preserving techniques and ethical considerations in the field.; and 5) An adapted diffusion model for art restoration that effectively handles multiple types of degradation through unified fine-tuning.",
      "authors": [
        "Sandeep Nagar"
      ],
      "published_date": "2025-12-03",
      "full_text": "Fast & Efficient Normalizing Flows and Applications of Image Generative Models\n\nThis thesis presents novel contributions in two primary areas: advancing the efficiency of generative models, particularly normalizing flows, and applying generative models to solve real-world computer vision challenges. The first part introduce significant improvements to normalizing flow architectures through six key innovations: 1) Development of invertible 3x3 Convolution layers with mathematically proven necessary and sufficient conditions for invertibility, (2) introduction of a more efficient Quad-coupling layer, 3) Design of a fast and efficient parallel inversion algorithm for kxk convolutional layers, 4) Fast & efficient backpropagation algorithm for inverse of convolution, 5) Using inverse of convolution, in Inverse-Flow, for the forward pass and training it using proposed backpropagation algorithm, and 6) Affine-StableSR, a compact and efficient super-resolution model that leverages pre-trained weights and Normalizing Flow layers to reduce parameter count while maintaining performance. The second part: 1) An automated quality assessment system for agricultural produce using Conditional GANs to address class imbalance, data scarcity and annotation challenges, achieving good accuracy in seed purity testing; 2) An unsupervised geological mapping framework utilizing stacked autoencoders for dimensionality reduction, showing improved feature extraction compared to conventional methods; 3) We proposed a privacy preserving method for autonomous driving datasets using on face detection and image inpainting; 4) Utilizing Stable Diffusion based image inpainting for replacing the detected face and license plate to advancing privacy-preserving techniques and ethical considerations in the field.; and 5) An adapted diffusion model for art restoration that effectively handles multiple types of degradation through unified fine-tuning."
    },
    {
      "id": "2512.04032v1",
      "title": "Jina-VLM: Small Multilingual Vision Language Model",
      "abstract": "We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance.",
      "authors": [
        "Andreas Koukounas",
        "Georgios Mastrapas",
        "Florian Hönicke",
        "Sedigheh Eslami",
        "Guillaume Roncari",
        "Scott Martens",
        "Han Xiao"
      ],
      "published_date": "2025-12-03",
      "full_text": "Jina-VLM: Small Multilingual Vision Language Model\n\nWe present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance."
    },
    {
      "id": "2512.04031v1",
      "title": "Large Language Models for Limited Noisy Data: A Gravitational Wave Identification Study",
      "abstract": "This work investigates whether large language models (LLMs) offer advantages over traditional neural networks for astronomical data processing, in regimes with non-Gaussian, non-stationary noise and limited labeled samples. Gravitational wave observations provide an suitable test case, using only 90 LIGO events, finetuned LLMs achieve 97.4\\% accuracy for identifying signals. Further experiments show that, in contrast to traditional networks that rely on large simulated datasets, additional simulated samples do not improve LLM performance, while scaling studies reveal predictable gains with increasing model size and dataset size. These results indicate that LLMs can extract discriminative structure directly from observational data and provide an efficient assessment for gravitational wave identification. The same strategy may extend to other astronomical domains with similar noise properties, such as radio or pulsar observations.",
      "authors": [
        "Yixuan Li",
        "Yuhao Lu",
        "Yang Liu",
        "Liang Li",
        "R. Ruffini",
        "Di Li",
        "Rong-Gen Cai",
        "Xiaoyan Zhu",
        "Wenbin Lin",
        "Yu Wang"
      ],
      "published_date": "2025-12-03",
      "full_text": "Large Language Models for Limited Noisy Data: A Gravitational Wave Identification Study\n\nThis work investigates whether large language models (LLMs) offer advantages over traditional neural networks for astronomical data processing, in regimes with non-Gaussian, non-stationary noise and limited labeled samples. Gravitational wave observations provide an suitable test case, using only 90 LIGO events, finetuned LLMs achieve 97.4\\% accuracy for identifying signals. Further experiments show that, in contrast to traditional networks that rely on large simulated datasets, additional simulated samples do not improve LLM performance, while scaling studies reveal predictable gains with increasing model size and dataset size. These results indicate that LLMs can extract discriminative structure directly from observational data and provide an efficient assessment for gravitational wave identification. The same strategy may extend to other astronomical domains with similar noise properties, such as radio or pulsar observations."
    },
    {
      "id": "2512.04025v1",
      "title": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation",
      "abstract": "Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA",
      "authors": [
        "Xiaolong Li",
        "Youping Gu",
        "Xi Lin",
        "Weijie Wang",
        "Bohan Zhuang"
      ],
      "published_date": "2025-12-03",
      "full_text": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation\n\nAttention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA"
    },
    {
      "id": "2512.04016v1",
      "title": "TARA Test-by-Adaptive-Ranks for Quantum Anomaly Detection with Conformal Prediction Guarantees",
      "abstract": "Quantum key distribution (QKD) security fundamentally relies on the ability to distinguish genuine quantum correlations from classical eavesdropper simulations, yet existing certification methods lack rigorous statistical guarantees under finite-sample conditions and adversarial scenarios. We introduce TARA (Test by Adaptive Ranks), a novel framework combining conformal prediction with sequential martingale testing for quantum anomaly detection that provides distribution-free validity guarantees. TARA offers two complementary approaches. TARA k, based on Kolmogorov Smirnov calibration against local hidden variable (LHV) null distributions, achieving ROC AUC = 0.96 for quantum-classical discrimination. And TARA-m, employing betting martingales for streaming detection with anytime valid type I error control that enables real time monitoring of quantum channels. We establish theoretical guarantees proving that under (context conditional) exchangeability, conformal p-values remain uniformly distributed even for strongly contextual quantum data, confirming that quantum contextuality does not break conformal prediction validity a result with implications beyond quantum certification to any application of distribution-free methods to nonclassical data. Extensive validation on both IBM Torino (superconducting, CHSH = 2.725) and IonQ Forte Enterprise (trapped ion, CHSH = 2.716) quantum processors demonstrates cross-platform robustness, achieving 36% security margins above the classical CHSH bound of 2. Critically, our framework reveals a methodological concern affecting quantum certification more broadly: same-distribution calibration can inflate detection performance by up to 44 percentage points compared to proper cross-distribution calibration, suggesting that prior quantum certification studies using standard train test splits may have systematically overestimated adversarial robustness.",
      "authors": [
        "Davut Emre Tasar",
        "Ceren Ocal Tasar"
      ],
      "published_date": "2025-12-03",
      "full_text": "TARA Test-by-Adaptive-Ranks for Quantum Anomaly Detection with Conformal Prediction Guarantees\n\nQuantum key distribution (QKD) security fundamentally relies on the ability to distinguish genuine quantum correlations from classical eavesdropper simulations, yet existing certification methods lack rigorous statistical guarantees under finite-sample conditions and adversarial scenarios. We introduce TARA (Test by Adaptive Ranks), a novel framework combining conformal prediction with sequential martingale testing for quantum anomaly detection that provides distribution-free validity guarantees. TARA offers two complementary approaches. TARA k, based on Kolmogorov Smirnov calibration against local hidden variable (LHV) null distributions, achieving ROC AUC = 0.96 for quantum-classical discrimination. And TARA-m, employing betting martingales for streaming detection with anytime valid type I error control that enables real time monitoring of quantum channels. We establish theoretical guarantees proving that under (context conditional) exchangeability, conformal p-values remain uniformly distributed even for strongly contextual quantum data, confirming that quantum contextuality does not break conformal prediction validity a result with implications beyond quantum certification to any application of distribution-free methods to nonclassical data. Extensive validation on both IBM Torino (superconducting, CHSH = 2.725) and IonQ Forte Enterprise (trapped ion, CHSH = 2.716) quantum processors demonstrates cross-platform robustness, achieving 36% security margins above the classical CHSH bound of 2. Critically, our framework reveals a methodological concern affecting quantum certification more broadly: same-distribution calibration can inflate detection performance by up to 44 percentage points compared to proper cross-distribution calibration, suggesting that prior quantum certification studies using standard train test splits may have systematically overestimated adversarial robustness."
    },
    {
      "id": "2512.04007v1",
      "title": "On the Temporality for Sketch Representation Learning",
      "abstract": "Sketches are simple human hand-drawn abstractions of complex scenes and real-world objects. Although the field of sketch representation learning has advanced significantly, there is still a gap in understanding the true relevance of the temporal aspect to the quality of these representations. This work investigates whether it is indeed justifiable to treat sketches as sequences, as well as which internal orders play a more relevant role. The results indicate that, although the use of traditional positional encodings is valid for modeling sketches as sequences, absolute coordinates consistently outperform relative ones. Furthermore, non-autoregressive decoders outperform their autoregressive counterparts. Finally, the importance of temporality was shown to depend on both the order considered and the task evaluated.",
      "authors": [
        "Marcelo Isaias de Moraes Junior",
        "Moacir Antonelli Ponti"
      ],
      "published_date": "2025-12-03",
      "full_text": "On the Temporality for Sketch Representation Learning\n\nSketches are simple human hand-drawn abstractions of complex scenes and real-world objects. Although the field of sketch representation learning has advanced significantly, there is still a gap in understanding the true relevance of the temporal aspect to the quality of these representations. This work investigates whether it is indeed justifiable to treat sketches as sequences, as well as which internal orders play a more relevant role. The results indicate that, although the use of traditional positional encodings is valid for modeling sketches as sequences, absolute coordinates consistently outperform relative ones. Furthermore, non-autoregressive decoders outperform their autoregressive counterparts. Finally, the importance of temporality was shown to depend on both the order considered and the task evaluated."
    },
    {
      "id": "2512.04000v1",
      "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding",
      "abstract": "The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically,DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.",
      "authors": [
        "Jialuo Li",
        "Bin Li",
        "Jiahao Li",
        "Yan Lu"
      ],
      "published_date": "2025-12-03",
      "full_text": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding\n\nThe application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically,DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256."
    },
    {
      "id": "2512.03996v1",
      "title": "Highly Efficient Test-Time Scaling for T2I Diffusion Models with Text Embedding Perturbation",
      "abstract": "Test-time scaling (TTS) aims to achieve better results by increasing random sampling and evaluating samples based on rules and metrics. However, in text-to-image(T2I) diffusion models, most related works focus on search strategies and reward models, yet the impact of the stochastic characteristic of noise in T2I diffusion models on the method's performance remains unexplored. In this work, we analyze the effects of randomness in T2I diffusion models and explore a new format of randomness for TTS: text embedding perturbation, which couples with existing randomness like SDE-injected noise to enhance generative diversity and quality. We start with a frequency-domain analysis of these formats of randomness and their impact on generation, and find that these two randomness exhibit complementary behavior in the frequency domain: spatial noise favors low-frequency components (early steps), while text embedding perturbation enhances high-frequency details (later steps), thereby compensating for the potential limitations of spatial noise randomness in high-frequency manipulation. Concurrently, text embedding demonstrates varying levels of tolerance to perturbation across different dimensions of the generation process. Specifically, our method consists of two key designs: (1) Introducing step-based text embedding perturbation, combining frequency-guided noise schedules with spatial noise perturbation. (2) Adapting the perturbation intensity selectively based on their frequency-specific contributions to generation and tolerance to perturbation. Our approach can be seamlessly integrated into existing TTS methods and demonstrates significant improvements on multiple benchmarks with almost no additional computation. Code is available at {https://github.com/xuhang07/TEP-Diffusion}.",
      "authors": [
        "Hang Xu",
        "Linjiang Huang",
        "Feng Zhao"
      ],
      "published_date": "2025-12-03",
      "full_text": "Highly Efficient Test-Time Scaling for T2I Diffusion Models with Text Embedding Perturbation\n\nTest-time scaling (TTS) aims to achieve better results by increasing random sampling and evaluating samples based on rules and metrics. However, in text-to-image(T2I) diffusion models, most related works focus on search strategies and reward models, yet the impact of the stochastic characteristic of noise in T2I diffusion models on the method's performance remains unexplored. In this work, we analyze the effects of randomness in T2I diffusion models and explore a new format of randomness for TTS: text embedding perturbation, which couples with existing randomness like SDE-injected noise to enhance generative diversity and quality. We start with a frequency-domain analysis of these formats of randomness and their impact on generation, and find that these two randomness exhibit complementary behavior in the frequency domain: spatial noise favors low-frequency components (early steps), while text embedding perturbation enhances high-frequency details (later steps), thereby compensating for the potential limitations of spatial noise randomness in high-frequency manipulation. Concurrently, text embedding demonstrates varying levels of tolerance to perturbation across different dimensions of the generation process. Specifically, our method consists of two key designs: (1) Introducing step-based text embedding perturbation, combining frequency-guided noise schedules with spatial noise perturbation. (2) Adapting the perturbation intensity selectively based on their frequency-specific contributions to generation and tolerance to perturbation. Our approach can be seamlessly integrated into existing TTS methods and demonstrates significant improvements on multiple benchmarks with almost no additional computation. Code is available at {https://github.com/xuhang07/TEP-Diffusion}."
    },
    {
      "id": "2512.03992v1",
      "title": "DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation",
      "abstract": "Vision-Language Models (VLMs) deployed in safety-critical applications such as autonomous driving must handle continuous visual streams under imperfect conditions. However, existing benchmarks focus on static, high-quality images and ignore temporal degradation and error propagation, which are critical failure modes where transient visual corruption induces hallucinations that persist across subsequent frames. We introduce DIQ-H, the first benchmark for evaluating VLM robustness under dynamic visual degradation in temporal sequences. DIQ-H applies physics-based corruptions including motion blur, sensor noise, and compression artifacts, and measures hallucination persistence, error recovery, and temporal consistency through multi-turn question-answering tasks. To enable scalable annotation, we propose Uncertainty-Guided Iterative Refinement (UIR), which generates reliable pseudo-ground-truth using lightweight VLMs with uncertainty filtering, achieving a 15.3 percent accuracy improvement. Experiments on 16 state-of-the-art VLMs reveal substantial robustness gaps: even advanced models such as GPT-4o achieve only a 78.5 percent recovery rate, while open-source models struggle with temporal consistency at less than 60 percent. DIQ-H provides a comprehensive platform for evaluating VLM reliability in real-world deployments.",
      "authors": [
        "Zexin Lin",
        "Hawen Wan",
        "Yebin Zhong",
        "Xiaoqiang"
      ],
      "published_date": "2025-12-03",
      "full_text": "DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation\n\nVision-Language Models (VLMs) deployed in safety-critical applications such as autonomous driving must handle continuous visual streams under imperfect conditions. However, existing benchmarks focus on static, high-quality images and ignore temporal degradation and error propagation, which are critical failure modes where transient visual corruption induces hallucinations that persist across subsequent frames. We introduce DIQ-H, the first benchmark for evaluating VLM robustness under dynamic visual degradation in temporal sequences. DIQ-H applies physics-based corruptions including motion blur, sensor noise, and compression artifacts, and measures hallucination persistence, error recovery, and temporal consistency through multi-turn question-answering tasks. To enable scalable annotation, we propose Uncertainty-Guided Iterative Refinement (UIR), which generates reliable pseudo-ground-truth using lightweight VLMs with uncertainty filtering, achieving a 15.3 percent accuracy improvement. Experiments on 16 state-of-the-art VLMs reveal substantial robustness gaps: even advanced models such as GPT-4o achieve only a 78.5 percent recovery rate, while open-source models struggle with temporal consistency at less than 60 percent. DIQ-H provides a comprehensive platform for evaluating VLM reliability in real-world deployments."
    },
    {
      "id": "2512.03979v1",
      "title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
      "abstract": "Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The source code is available at https://github.com/Jin-Ting-He/BlurDM.",
      "authors": [
        "Jin-Ting He",
        "Fu-Jen Tsai",
        "Yan-Tsung Peng",
        "Min-Hung Chen",
        "Chia-Wen Lin",
        "Yen-Yu Lin"
      ],
      "published_date": "2025-12-03",
      "full_text": "BlurDM: A Blur Diffusion Model for Image Deblurring\n\nDiffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The source code is available at https://github.com/Jin-Ting-He/BlurDM."
    },
    {
      "id": "2512.03975v1",
      "title": "Sponsored Questions and How to Auction Them",
      "abstract": "Online platforms connect users with relevant products and services using ads. A key challenge is that a user's search query often leaves their true intent ambiguous. Typically, platforms passively predict relevance based on available signals and in some cases offer query refinements. The shift from traditional search to conversational AI provides a new approach. When a user's query is ambiguous, a Large Language Model (LLM) can proactively offer several clarifying follow-up prompts. In this paper we consider the following: what if some of these follow-up prompts can be ``sponsored,'' i.e., selected for their advertising potential. How should these ``suggestion slots'' be allocated? And, how does this new mechanism interact with the traditional ad auction that might follow? This paper introduces a formal model for designing and analyzing these interactive platforms. We use this model to investigate a critical engineering choice: whether it is better to build an end-to-end pipeline that jointly optimizes the user interaction and the final ad auction, or to decouple them into separate mechanisms for the suggestion slots and another for the subsequent ad slot. We show that the VCG mechanism can be adopted to jointly optimize the sponsored suggestion and the ads that follow; while this mechanism is more complex, it achieves outcomes that are efficient and truthful. On the other hand, we prove that the simple-to-implement modular approach suffers from strategic inefficiency: its Price of Anarchy is unbounded.",
      "authors": [
        "Kshipra Bhawalkar",
        "Alexandros Psomas",
        "Di Wang"
      ],
      "published_date": "2025-12-03",
      "full_text": "Sponsored Questions and How to Auction Them\n\nOnline platforms connect users with relevant products and services using ads. A key challenge is that a user's search query often leaves their true intent ambiguous. Typically, platforms passively predict relevance based on available signals and in some cases offer query refinements. The shift from traditional search to conversational AI provides a new approach. When a user's query is ambiguous, a Large Language Model (LLM) can proactively offer several clarifying follow-up prompts. In this paper we consider the following: what if some of these follow-up prompts can be ``sponsored,'' i.e., selected for their advertising potential. How should these ``suggestion slots'' be allocated? And, how does this new mechanism interact with the traditional ad auction that might follow? This paper introduces a formal model for designing and analyzing these interactive platforms. We use this model to investigate a critical engineering choice: whether it is better to build an end-to-end pipeline that jointly optimizes the user interaction and the final ad auction, or to decouple them into separate mechanisms for the suggestion slots and another for the subsequent ad slot. We show that the VCG mechanism can be adopted to jointly optimize the sponsored suggestion and the ads that follow; while this mechanism is more complex, it achieves outcomes that are efficient and truthful. On the other hand, we prove that the simple-to-implement modular approach suffers from strategic inefficiency: its Price of Anarchy is unbounded."
    },
    {
      "id": "2512.03973v1",
      "title": "Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning",
      "abstract": "Offline reinforcement learning often relies on behavior regularization that enforces policies to remain close to the dataset distribution. However, such approaches fail to distinguish between high-value and low-value actions in their regularization components. We introduce Guided Flow Policy (GFP), which couples a multi-step flow-matching policy with a distilled one-step actor. The actor directs the flow policy through weighted behavior cloning to focus on cloning high-value actions from the dataset rather than indiscriminately imitating all state-action pairs. In turn, the flow policy constrains the actor to remain aligned with the dataset's best transitions while maximizing the critic. This mutual guidance enables GFP to achieve state-of-the-art performance across 144 state and pixel-based tasks from the OGBench, Minari, and D4RL benchmarks, with substantial gains on suboptimal datasets and challenging tasks. Webpage: https://simple-robotics.github.io/publications/guided-flow-policy/",
      "authors": [
        "Franki Nguimatsia Tiofack",
        "Théotime Le Hellard",
        "Fabian Schramm",
        "Nicolas Perrin-Gilbert",
        "Justin Carpentier"
      ],
      "published_date": "2025-12-03",
      "full_text": "Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning\n\nOffline reinforcement learning often relies on behavior regularization that enforces policies to remain close to the dataset distribution. However, such approaches fail to distinguish between high-value and low-value actions in their regularization components. We introduce Guided Flow Policy (GFP), which couples a multi-step flow-matching policy with a distilled one-step actor. The actor directs the flow policy through weighted behavior cloning to focus on cloning high-value actions from the dataset rather than indiscriminately imitating all state-action pairs. In turn, the flow policy constrains the actor to remain aligned with the dataset's best transitions while maximizing the critic. This mutual guidance enables GFP to achieve state-of-the-art performance across 144 state and pixel-based tasks from the OGBench, Minari, and D4RL benchmarks, with substantial gains on suboptimal datasets and challenging tasks. Webpage: https://simple-robotics.github.io/publications/guided-flow-policy/"
    },
    {
      "id": "2512.03955v1",
      "title": "Benchmark for Planning and Control with Large Language Model Agents: Blocksworld with Model Context Protocol",
      "abstract": "Industrial automation increasingly requires flexible control strategies that can adapt to changing tasks and environments. Agents based on Large Language Models (LLMs) offer potential for such adaptive planning and execution but lack standardized benchmarks for systematic comparison. We introduce a benchmark with an executable simulation environment representing the Blocksworld problem providing five complexity categories. By integrating the Model Context Protocol (MCP) as a standardized tool interface, diverse agent architectures can be connected to and evaluated against the benchmark without implementation-specific modifications. A single-agent implementation demonstrates the benchmark's applicability, establishing quantitative metrics for comparison of LLM-based planning and execution approaches.",
      "authors": [
        "Niklas Jobs",
        "Luis Miguel Vieira da Silva",
        "Jayanth Somashekaraiah",
        "Maximilian Weigand",
        "David Kube",
        "Felix Gehlhoff"
      ],
      "published_date": "2025-12-03",
      "full_text": "Benchmark for Planning and Control with Large Language Model Agents: Blocksworld with Model Context Protocol\n\nIndustrial automation increasingly requires flexible control strategies that can adapt to changing tasks and environments. Agents based on Large Language Models (LLMs) offer potential for such adaptive planning and execution but lack standardized benchmarks for systematic comparison. We introduce a benchmark with an executable simulation environment representing the Blocksworld problem providing five complexity categories. By integrating the Model Context Protocol (MCP) as a standardized tool interface, diverse agent architectures can be connected to and evaluated against the benchmark without implementation-specific modifications. A single-agent implementation demonstrates the benchmark's applicability, establishing quantitative metrics for comparison of LLM-based planning and execution approaches."
    },
    {
      "id": "2512.03931v1",
      "title": "Autonomous Agents and Policy Compliance: A Framework for Reasoning About Penalties",
      "abstract": "This paper presents a logic programming-based framework for policy-aware autonomous agents that can reason about potential penalties for non-compliance and act accordingly. While prior work has primarily focused on ensuring compliance, our approach considers scenarios where deviating from policies may be necessary to achieve high-stakes goals. Additionally, modeling non-compliant behavior can assist policymakers by simulating realistic human decision-making. Our framework extends Gelfond and Lobo's Authorization and Obligation Policy Language (AOPL) to incorporate penalties and integrates Answer Set Programming (ASP) for reasoning. Compared to previous approaches, our method ensures well-formed policies, accounts for policy priorities, and enhances explainability by explicitly identifying rule violations and their consequences. Building on the work of Harders and Inclezan, we introduce penalty-based reasoning to distinguish between non-compliant plans, prioritizing those with minimal repercussions. To support this, we develop an automated translation from the extended AOPL into ASP and refine ASP-based planning algorithms to account for incurred penalties. Experiments in two domains demonstrate that our framework generates higher-quality plans that avoid harmful actions while, in some cases, also improving computational efficiency. These findings underscore its potential for enhancing autonomous decision-making and informing policy refinement. Under consideration in Theory and Practice of Logic Programming (TPLP).",
      "authors": [
        "Vineel Tummala",
        "Daniela Inclezan"
      ],
      "published_date": "2025-12-03",
      "full_text": "Autonomous Agents and Policy Compliance: A Framework for Reasoning About Penalties\n\nThis paper presents a logic programming-based framework for policy-aware autonomous agents that can reason about potential penalties for non-compliance and act accordingly. While prior work has primarily focused on ensuring compliance, our approach considers scenarios where deviating from policies may be necessary to achieve high-stakes goals. Additionally, modeling non-compliant behavior can assist policymakers by simulating realistic human decision-making. Our framework extends Gelfond and Lobo's Authorization and Obligation Policy Language (AOPL) to incorporate penalties and integrates Answer Set Programming (ASP) for reasoning. Compared to previous approaches, our method ensures well-formed policies, accounts for policy priorities, and enhances explainability by explicitly identifying rule violations and their consequences. Building on the work of Harders and Inclezan, we introduce penalty-based reasoning to distinguish between non-compliant plans, prioritizing those with minimal repercussions. To support this, we develop an automated translation from the extended AOPL into ASP and refine ASP-based planning algorithms to account for incurred penalties. Experiments in two domains demonstrate that our framework generates higher-quality plans that avoid harmful actions while, in some cases, also improving computational efficiency. These findings underscore its potential for enhancing autonomous decision-making and informing policy refinement. Under consideration in Theory and Practice of Logic Programming (TPLP)."
    },
    {
      "id": "2512.03915v1",
      "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
      "abstract": "In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.",
      "authors": [
        "X. Y. Han",
        "Yuan Zhong"
      ],
      "published_date": "2025-12-03",
      "full_text": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models\n\nIn large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models."
    },
    {
      "id": "2512.03913v1",
      "title": "Hierarchical Vision Language Action Model Using Success and Failure Demonstrations",
      "abstract": "Prior Vision-Language-Action (VLA) models are typically trained on teleoperated successful demonstrations, while discarding numerous failed attempts that occur naturally during data collection. However, these failures encode where and how policies can be fragile, information that can be exploited to improve robustness. We address this problem by leveraging mixed-quality datasets to learn failure-aware reasoning at planning time. We introduce VINE, a hierarchical vision-language-action model that separates high-level reasoning (System 2) from low-level control (System 1) under a hierarchical reinforcement learning formalism, making failures usable as a structured learning signal rather than noisy supervision. System 2 performs feasibility-guided tree search over a 2D scene-graph abstraction: it proposes subgoal transitions, predicts success probabilities from both successes and failures, and prunes brittle branches before execution, effectively casting plan evaluation as feasibility scoring. The selected subgoal sequence is then passed to System 1, which executes low-level actions without modifying the agent's core skills. Trained entirely from offline teleoperation data, VINE integrates negative experience directly into the decision loop. Across challenging manipulation tasks, this approach consistently improves success rates and robustness, demonstrating that failure data is an essential resource for converting the broad competence of VLAs into robust execution.",
      "authors": [
        "Jeongeun Park",
        "Jihwan Yoon",
        "Byungwoo Jeon",
        "Juhan Park",
        "Jinwoo Shin",
        "Namhoon Cho",
        "Kyungjae Lee",
        "Sangdoo Yun",
        "Sungjoon Choi"
      ],
      "published_date": "2025-12-03",
      "full_text": "Hierarchical Vision Language Action Model Using Success and Failure Demonstrations\n\nPrior Vision-Language-Action (VLA) models are typically trained on teleoperated successful demonstrations, while discarding numerous failed attempts that occur naturally during data collection. However, these failures encode where and how policies can be fragile, information that can be exploited to improve robustness. We address this problem by leveraging mixed-quality datasets to learn failure-aware reasoning at planning time. We introduce VINE, a hierarchical vision-language-action model that separates high-level reasoning (System 2) from low-level control (System 1) under a hierarchical reinforcement learning formalism, making failures usable as a structured learning signal rather than noisy supervision. System 2 performs feasibility-guided tree search over a 2D scene-graph abstraction: it proposes subgoal transitions, predicts success probabilities from both successes and failures, and prunes brittle branches before execution, effectively casting plan evaluation as feasibility scoring. The selected subgoal sequence is then passed to System 1, which executes low-level actions without modifying the agent's core skills. Trained entirely from offline teleoperation data, VINE integrates negative experience directly into the decision loop. Across challenging manipulation tasks, this approach consistently improves success rates and robustness, demonstrating that failure data is an essential resource for converting the broad competence of VLAs into robust execution."
    },
    {
      "id": "2512.03911v1",
      "title": "Autonomous Reinforcement Learning Robot Control with Intel's Loihi 2 Neuromorphic Hardware",
      "abstract": "We present an end-to-end pipeline for deploying reinforcement learning (RL) trained Artificial Neural Networks (ANNs) on neuromorphic hardware by converting them into spiking Sigma-Delta Neural Networks (SDNNs). We demonstrate that an ANN policy trained entirely in simulation can be transformed into an SDNN compatible with Intel's Loihi 2 architecture, enabling low-latency and energy-efficient inference. As a test case, we use an RL policy for controlling the Astrobee free-flying robot, similar to a previously hardware in space-validated controller. The policy, trained with Rectified Linear Units (ReLUs), is converted to an SDNN and deployed on Intel's Loihi 2, then evaluated in NVIDIA's Omniverse Isaac Lab simulation environment for closed-loop control of Astrobee's motion. We compare execution performance between GPU and Loihi 2. The results highlight the feasibility of using neuromorphic platforms for robotic control and establish a pathway toward energy-efficient, real-time neuromorphic computation in future space and terrestrial robotics applications.",
      "authors": [
        "Kenneth Stewart",
        "Roxana Leontie",
        "Samantha Chapin",
        "Joe Hays",
        "Sumit Bam Shrestha",
        "Carl Glen Henshaw"
      ],
      "published_date": "2025-12-03",
      "full_text": "Autonomous Reinforcement Learning Robot Control with Intel's Loihi 2 Neuromorphic Hardware\n\nWe present an end-to-end pipeline for deploying reinforcement learning (RL) trained Artificial Neural Networks (ANNs) on neuromorphic hardware by converting them into spiking Sigma-Delta Neural Networks (SDNNs). We demonstrate that an ANN policy trained entirely in simulation can be transformed into an SDNN compatible with Intel's Loihi 2 architecture, enabling low-latency and energy-efficient inference. As a test case, we use an RL policy for controlling the Astrobee free-flying robot, similar to a previously hardware in space-validated controller. The policy, trained with Rectified Linear Units (ReLUs), is converted to an SDNN and deployed on Intel's Loihi 2, then evaluated in NVIDIA's Omniverse Isaac Lab simulation environment for closed-loop control of Astrobee's motion. We compare execution performance between GPU and Loihi 2. The results highlight the feasibility of using neuromorphic platforms for robotic control and establish a pathway toward energy-efficient, real-time neuromorphic computation in future space and terrestrial robotics applications."
    },
    {
      "id": "2512.03903v1",
      "title": "BERnaT: Basque Encoders for Representing Natural Textual Diversity",
      "abstract": "Language models depend on massive text corpora that are often filtered for quality, a process that can unintentionally exclude non-standard linguistic varieties, reduce model robustness and reinforce representational biases. In this paper, we argue that language models should aim to capture the full spectrum of language variation (dialectal, historical, informal, etc.) rather than relying solely on standardized text. Focusing on Basque, a morphologically rich and low-resource language, we construct new corpora combining standard, social media, and historical sources, and pre-train the BERnaT family of encoder-only models in three configurations: standard, diverse, and combined. We further propose an evaluation framework that separates Natural Language Understanding (NLU) tasks into standard and diverse subsets to assess linguistic generalization. Results show that models trained on both standard and diverse data consistently outperform those trained on standard corpora, improving performance across all task types without compromising standard benchmark accuracy. These findings highlight the importance of linguistic diversity in building inclusive, generalizable language models.",
      "authors": [
        "Ekhi Azurmendi",
        "Joseba Fernandez de Landa",
        "Jaione Bengoetxea",
        "Maite Heredia",
        "Julen Etxaniz",
        "Mikel Zubillaga",
        "Ander Soraluze",
        "Aitor Soroa"
      ],
      "published_date": "2025-12-03",
      "full_text": "BERnaT: Basque Encoders for Representing Natural Textual Diversity\n\nLanguage models depend on massive text corpora that are often filtered for quality, a process that can unintentionally exclude non-standard linguistic varieties, reduce model robustness and reinforce representational biases. In this paper, we argue that language models should aim to capture the full spectrum of language variation (dialectal, historical, informal, etc.) rather than relying solely on standardized text. Focusing on Basque, a morphologically rich and low-resource language, we construct new corpora combining standard, social media, and historical sources, and pre-train the BERnaT family of encoder-only models in three configurations: standard, diverse, and combined. We further propose an evaluation framework that separates Natural Language Understanding (NLU) tasks into standard and diverse subsets to assess linguistic generalization. Results show that models trained on both standard and diverse data consistently outperform those trained on standard corpora, improving performance across all task types without compromising standard benchmark accuracy. These findings highlight the importance of linguistic diversity in building inclusive, generalizable language models."
    },
    {
      "id": "2512.03887v1",
      "title": "A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)",
      "abstract": "The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (RAG) pipelines in handling complex, multi-turn research tasks. This paper introduces the Static Deep Research Agent (Static-DRA), a novel solution built upon a configurable and hierarchical Tree-based static workflow. The core contribution is the integration of two user-tunable parameters, Depth and Breadth, which provide granular control over the research intensity. This design allows end-users to consciously balance the desired quality and comprehensiveness of the research report against the associated computational cost of Large Language Model (LLM) interactions. The agent's architecture, comprising Supervisor, Independent, and Worker agents, facilitates effective multi-hop information retrieval and parallel sub-topic investigation. We evaluate the Static-DRA against the established DeepResearch Bench using the RACE (Reference-based Adaptive Criteria-driven Evaluation) framework. Configured with a depth of 2 and a breadth of 5, and powered by the gemini-2.5-pro model, the agent achieved an overall score of 34.72. Our experiments validate that increasing the configured Depth and Breadth parameters results in a more in-depth research process and a correspondingly higher evaluation score. The Static-DRA offers a pragmatic and resource-aware solution, empowering users with transparent control over the deep research process. The entire source code, outputs and benchmark results are open-sourced at https://github.com/SauravP97/Static-Deep-Research/",
      "authors": [
        "Saurav Prateek"
      ],
      "published_date": "2025-12-03",
      "full_text": "A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)\n\nThe advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (RAG) pipelines in handling complex, multi-turn research tasks. This paper introduces the Static Deep Research Agent (Static-DRA), a novel solution built upon a configurable and hierarchical Tree-based static workflow. The core contribution is the integration of two user-tunable parameters, Depth and Breadth, which provide granular control over the research intensity. This design allows end-users to consciously balance the desired quality and comprehensiveness of the research report against the associated computational cost of Large Language Model (LLM) interactions. The agent's architecture, comprising Supervisor, Independent, and Worker agents, facilitates effective multi-hop information retrieval and parallel sub-topic investigation. We evaluate the Static-DRA against the established DeepResearch Bench using the RACE (Reference-based Adaptive Criteria-driven Evaluation) framework. Configured with a depth of 2 and a breadth of 5, and powered by the gemini-2.5-pro model, the agent achieved an overall score of 34.72. Our experiments validate that increasing the configured Depth and Breadth parameters results in a more in-depth research process and a correspondingly higher evaluation score. The Static-DRA offers a pragmatic and resource-aware solution, empowering users with transparent control over the deep research process. The entire source code, outputs and benchmark results are open-sourced at https://github.com/SauravP97/Static-Deep-Research/"
    },
    {
      "id": "2512.03864v1",
      "title": "Hyperdimensional Computing for Sustainable Manufacturing: An Initial Assessment",
      "abstract": "Smart manufacturing can significantly improve efficiency and reduce energy consumption, yet the energy demands of AI models may offset these gains. This study utilizes in-situ sensing-based prediction of geometric quality in smart machining to compare the energy consumption, accuracy, and speed of common AI models. HyperDimensional Computing (HDC) is introduced as an alternative, achieving accuracy comparable to conventional models while drastically reducing energy consumption, 200 for training and 175 to 1000 for inference. Furthermore, HDC reduces training times by 200 and inference times by 300 to 600, showcasing its potential for energy-efficient smart manufacturing.",
      "authors": [
        "Danny Hoang",
        "Anandkumar Patel",
        "Ruimen Chen",
        "Rajiv Malhotra",
        "Farhad Imani"
      ],
      "published_date": "2025-12-03",
      "full_text": "Hyperdimensional Computing for Sustainable Manufacturing: An Initial Assessment\n\nSmart manufacturing can significantly improve efficiency and reduce energy consumption, yet the energy demands of AI models may offset these gains. This study utilizes in-situ sensing-based prediction of geometric quality in smart machining to compare the energy consumption, accuracy, and speed of common AI models. HyperDimensional Computing (HDC) is introduced as an alternative, achieving accuracy comparable to conventional models while drastically reducing energy consumption, 200 for training and 175 to 1000 for inference. Furthermore, HDC reduces training times by 200 and inference times by 300 to 600, showcasing its potential for energy-efficient smart manufacturing."
    },
    {
      "id": "2512.03861v1",
      "title": "Scalable Decision Focused Learning via Online Trainable Surrogates",
      "abstract": "Decision support systems often rely on solving complex optimization problems that may require to estimate uncertain parameters beforehand. Recent studies have shown how using traditionally trained estimators for this task can lead to suboptimal solutions. Using the actual decision cost as a loss function (called Decision Focused Learning) can address this issue, but with a severe loss of scalability at training time. To address this issue, we propose an acceleration method based on replacing costly loss function evaluations with an efficient surrogate. Unlike previously defined surrogates, our approach relies on unbiased estimators reducing the risk of spurious local optima and can provide information on its local confidence allowing one to switch to a fallback method when needed. Furthermore, the surrogate is designed for a black-box setting, which enables compensating for simplifications in the optimization model and account- ing for recourse actions during cost computation. In our results, the method reduces costly inner solver calls, with a solution quality comparable to other state-of-the-art techniques.",
      "authors": [
        "Gaetano Signorelli",
        "Michele Lombardi"
      ],
      "published_date": "2025-12-03",
      "full_text": "Scalable Decision Focused Learning via Online Trainable Surrogates\n\nDecision support systems often rely on solving complex optimization problems that may require to estimate uncertain parameters beforehand. Recent studies have shown how using traditionally trained estimators for this task can lead to suboptimal solutions. Using the actual decision cost as a loss function (called Decision Focused Learning) can address this issue, but with a severe loss of scalability at training time. To address this issue, we propose an acceleration method based on replacing costly loss function evaluations with an efficient surrogate. Unlike previously defined surrogates, our approach relies on unbiased estimators reducing the risk of spurious local optima and can provide information on its local confidence allowing one to switch to a fallback method when needed. Furthermore, the surrogate is designed for a black-box setting, which enables compensating for simplifications in the optimization model and account- ing for recourse actions during cost computation. In our results, the method reduces costly inner solver calls, with a solution quality comparable to other state-of-the-art techniques."
    },
    {
      "id": "2512.04068v1",
      "title": "Learning Steerable Clarification Policies with Collaborative Self-play",
      "abstract": "To handle underspecified or ambiguous queries, AI assistants need a policy for managing their uncertainty to determine (a) when to guess the user intent and answer directly, (b) when to enumerate and answer multiple possible intents, and (c) when to ask a clarifying question. However, such policies are contextually dependent on factors such as user preferences or modality. For example, enumerating multiple possible user intentions is cumbersome on small screens or in a voice setting. In this work, we propose to train steerable policies for managing this uncertainty using self-play. Given two agents, one simulating a user and the other an AI assistant, we generate conversations where the user issues a potentially ambiguous query, and the assistant needs to determine how to respond. Importantly, the model takes as input the numerical cost of each clarification question, and each generated word, and is asked to take the action that will maximize its final reward, which is the cost-penalized accuracy. We use Reinforced Self-Training (ReST) to train our model to achieve high reward and show this leads to a steerable policy that changes its behavior predictably conditioned on the provided costs, leading to higher reward and accuracy. Moreover, our procedure also generalizes to numerical cost values that were unobserved at training time.",
      "authors": [
        "Jonathan Berant",
        "Maximillian Chen",
        "Adam Fisch",
        "Reza Aghajani",
        "Fantine Huot",
        "Mirella Lapata",
        "Jacob Eisenstein"
      ],
      "published_date": "2025-12-03",
      "full_text": "Learning Steerable Clarification Policies with Collaborative Self-play\n\nTo handle underspecified or ambiguous queries, AI assistants need a policy for managing their uncertainty to determine (a) when to guess the user intent and answer directly, (b) when to enumerate and answer multiple possible intents, and (c) when to ask a clarifying question. However, such policies are contextually dependent on factors such as user preferences or modality. For example, enumerating multiple possible user intentions is cumbersome on small screens or in a voice setting. In this work, we propose to train steerable policies for managing this uncertainty using self-play. Given two agents, one simulating a user and the other an AI assistant, we generate conversations where the user issues a potentially ambiguous query, and the assistant needs to determine how to respond. Importantly, the model takes as input the numerical cost of each clarification question, and each generated word, and is asked to take the action that will maximize its final reward, which is the cost-penalized accuracy. We use Reinforced Self-Training (ReST) to train our model to achieve high reward and show this leads to a steerable policy that changes its behavior predictably conditioned on the provided costs, leading to higher reward and accuracy. Moreover, our procedure also generalizes to numerical cost values that were unobserved at training time."
    },
    {
      "id": "2512.04065v1",
      "title": "Fare Comparison App of Uber, Ola and Rapido",
      "abstract": "In todays increasing world, it is very important to have good hailing services like Ola, Uber, and Rapido as it is very essential for our daily transportation. Users often face difficulties in choosing the most appropriate and efficient ride that would lead to both cost-effective and would take us to our destination in less time. This project provides you with the web application that helps you to select the most beneficial ride for you by providing users with the fare comparison between Ola, Uber, Rapido for the destination entered by the user. The backend is use to fetch the data, providing users with the fare comparison for the ride and finally providing with the best option using Python. This research paper also addresses the problem and challenges faced in accessing the data using APIs, Android Studios emulator, Appium and location comparison. Thus, the aim of the project is to provide transparency to the users in ride-hailing services and increase efficiency and provide users with better experience.",
      "authors": [
        "Ashlesha Gopinath Sawant",
        "Sahil S. Jadhav",
        "Vidhan R. Jain",
        "Shriraj S. Jagtap",
        "Prachi Jadhav",
        "Soham Jadhav",
        "Ichha Raina"
      ],
      "published_date": "2025-12-03",
      "full_text": "Fare Comparison App of Uber, Ola and Rapido\n\nIn todays increasing world, it is very important to have good hailing services like Ola, Uber, and Rapido as it is very essential for our daily transportation. Users often face difficulties in choosing the most appropriate and efficient ride that would lead to both cost-effective and would take us to our destination in less time. This project provides you with the web application that helps you to select the most beneficial ride for you by providing users with the fare comparison between Ola, Uber, Rapido for the destination entered by the user. The backend is use to fetch the data, providing users with the fare comparison for the ride and finally providing with the best option using Python. This research paper also addresses the problem and challenges faced in accessing the data using APIs, Android Studios emulator, Appium and location comparison. Thus, the aim of the project is to provide transparency to the users in ride-hailing services and increase efficiency and provide users with better experience."
    },
    {
      "id": "2512.04062v1",
      "title": "Eval Factsheets: A Structured Framework for Documenting AI Evaluations",
      "abstract": "The rapid proliferation of benchmarks has created significant challenges in reproducibility, transparency, and informed decision-making. However, unlike datasets and models -- which benefit from structured documentation frameworks like Datasheets and Model Cards -- evaluation methodologies lack systematic documentation standards. We introduce Eval Factsheets, a structured, descriptive framework for documenting AI system evaluations through a comprehensive taxonomy and questionnaire-based approach. Our framework organizes evaluation characteristics across five fundamental dimensions: Context (Who made the evaluation and when?), Scope (What does it evaluate?), Structure (With what the evaluation is built?), Method (How does it work?) and Alignment (In what ways is it reliable/valid/robust?). We implement this taxonomy as a practical questionnaire spanning five sections with mandatory and recommended documentation elements. Through case studies on multiple benchmarks, we demonstrate that Eval Factsheets effectively captures diverse evaluation paradigms -- from traditional benchmarks to LLM-as-judge methodologies -- while maintaining consistency and comparability. We hope Eval Factsheets are incorporated into both existing and newly released evaluation frameworks and lead to more transparency and reproducibility.",
      "authors": [
        "Florian Bordes",
        "Candace Ross",
        "Justine T Kao",
        "Evangelia Spiliopoulou",
        "Adina Williams"
      ],
      "published_date": "2025-12-03",
      "full_text": "Eval Factsheets: A Structured Framework for Documenting AI Evaluations\n\nThe rapid proliferation of benchmarks has created significant challenges in reproducibility, transparency, and informed decision-making. However, unlike datasets and models -- which benefit from structured documentation frameworks like Datasheets and Model Cards -- evaluation methodologies lack systematic documentation standards. We introduce Eval Factsheets, a structured, descriptive framework for documenting AI system evaluations through a comprehensive taxonomy and questionnaire-based approach. Our framework organizes evaluation characteristics across five fundamental dimensions: Context (Who made the evaluation and when?), Scope (What does it evaluate?), Structure (With what the evaluation is built?), Method (How does it work?) and Alignment (In what ways is it reliable/valid/robust?). We implement this taxonomy as a practical questionnaire spanning five sections with mandatory and recommended documentation elements. Through case studies on multiple benchmarks, we demonstrate that Eval Factsheets effectively captures diverse evaluation paradigms -- from traditional benchmarks to LLM-as-judge methodologies -- while maintaining consistency and comparability. We hope Eval Factsheets are incorporated into both existing and newly released evaluation frameworks and lead to more transparency and reproducibility."
    },
    {
      "id": "2512.04058v1",
      "title": "Closing the problem of which causal structures of up to six total nodes have a classical-quantum gap",
      "abstract": "The discovery of Bell that there exist quantum correlations that cannot be reproduced classically is one of the most important in the foundations of quantum mechanics, as well as having practical implications. Bell's result was originally proven in a simple bipartite causal structure, but analogous results have also been shown in further causal structures. Here we study the only causal structure with six or fewer nodes in which the question of whether or not there exist quantum correlations that cannot be achieved classically was open. In this causal structure we show that such quantum correlations exist using a method that involves imposing additional restrictions on the correlations. This hence completes the picture of which causal structures of up to six nodes support non-classical quantum correlations. We also provide further illustrations of our method using other causal structures.",
      "authors": [
        "Shashaank Khanna",
        "Matthew Pusey",
        "Roger Colbeck"
      ],
      "published_date": "2025-12-03",
      "full_text": "Closing the problem of which causal structures of up to six total nodes have a classical-quantum gap\n\nThe discovery of Bell that there exist quantum correlations that cannot be reproduced classically is one of the most important in the foundations of quantum mechanics, as well as having practical implications. Bell's result was originally proven in a simple bipartite causal structure, but analogous results have also been shown in further causal structures. Here we study the only causal structure with six or fewer nodes in which the question of whether or not there exist quantum correlations that cannot be achieved classically was open. In this causal structure we show that such quantum correlations exist using a method that involves imposing additional restrictions on the correlations. This hence completes the picture of which causal structures of up to six nodes support non-classical quantum correlations. We also provide further illustrations of our method using other causal structures."
    },
    {
      "id": "2512.04051v1",
      "title": "Convergence for Discrete Parameter Updates",
      "abstract": "Modern deep learning models require immense computational resources, motivating research into low-precision training. Quantised training addresses this by representing training components in low-bit integers, but typically relies on discretising real-valued updates. We introduce an alternative approach where the update rule itself is discrete, avoiding the quantisation of continuous updates by design. We establish convergence guarantees for a general class of such discrete schemes, and present a multinomial update rule as a concrete example, supported by empirical evaluation. This perspective opens new avenues for efficient training, particularly for models with inherently discrete structure.",
      "authors": [
        "Paul Wilson",
        "Fabio Zanasi",
        "George Constantinides"
      ],
      "published_date": "2025-12-03",
      "full_text": "Convergence for Discrete Parameter Updates\n\nModern deep learning models require immense computational resources, motivating research into low-precision training. Quantised training addresses this by representing training components in low-bit integers, but typically relies on discretising real-valued updates. We introduce an alternative approach where the update rule itself is discrete, avoiding the quantisation of continuous updates by design. We establish convergence guarantees for a general class of such discrete schemes, and present a multinomial update rule as a concrete example, supported by empirical evaluation. This perspective opens new avenues for efficient training, particularly for models with inherently discrete structure."
    },
    {
      "id": "2512.04044v1",
      "title": "MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM Watermarking",
      "abstract": "Watermarking aims to embed hidden signals in generated text that can be reliably detected when given access to a secret key. Open-weight language models pose acute challenges for such watermarking schemes because the inference-time interventions that dominate contemporary approaches cannot be enforced once model weights are public. Existing watermaking techniques for open-weight models, such as the recently proposed GaussMark, typically rely on small modifications to model weights, which can yield signals detectable to those equipped with a secret key, but achieving detection power comparable to inference-time watermarks generally requires weight perturbations that noticeably reduce generation quality. We introduce MarkTune, a theoretically principled, on-policy fine-tuning framework that treats the GaussMark signal as a reward while simultaneously regularizing against degradation in text quality. We derive MarkTune as an improvement on GaussMark and demonstrate that MarkTune consistently improves the quality-detectability trade-off over GaussMark by steering finer-grained, watermark-aware weight updates within the model's representation space while preserving generation quality. Empirically, we show that MarkTune pushes the quality-detectability frontier of GaussMark close to that of inference-time watermarking, remains robust to paraphrasing and fine-tuning attacks, and exhibits strong generalization: a model fine-tuned on one dataset retains substantial watermark detection power on unseen datasets. Together, these results establish MarkTune as a general strategy for embedding robust, high-quality watermarks into open-weight LMs.",
      "authors": [
        "Yizhou Zhao",
        "Zhiwei Steven Wu",
        "Adam Block"
      ],
      "published_date": "2025-12-03",
      "full_text": "MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM Watermarking\n\nWatermarking aims to embed hidden signals in generated text that can be reliably detected when given access to a secret key. Open-weight language models pose acute challenges for such watermarking schemes because the inference-time interventions that dominate contemporary approaches cannot be enforced once model weights are public. Existing watermaking techniques for open-weight models, such as the recently proposed GaussMark, typically rely on small modifications to model weights, which can yield signals detectable to those equipped with a secret key, but achieving detection power comparable to inference-time watermarks generally requires weight perturbations that noticeably reduce generation quality. We introduce MarkTune, a theoretically principled, on-policy fine-tuning framework that treats the GaussMark signal as a reward while simultaneously regularizing against degradation in text quality. We derive MarkTune as an improvement on GaussMark and demonstrate that MarkTune consistently improves the quality-detectability trade-off over GaussMark by steering finer-grained, watermark-aware weight updates within the model's representation space while preserving generation quality. Empirically, we show that MarkTune pushes the quality-detectability frontier of GaussMark close to that of inference-time watermarking, remains robust to paraphrasing and fine-tuning attacks, and exhibits strong generalization: a model fine-tuned on one dataset retains substantial watermark detection power on unseen datasets. Together, these results establish MarkTune as a general strategy for embedding robust, high-quality watermarks into open-weight LMs."
    },
    {
      "id": "2512.04039v1",
      "title": "Fast & Efficient Normalizing Flows and Applications of Image Generative Models",
      "abstract": "This thesis presents novel contributions in two primary areas: advancing the efficiency of generative models, particularly normalizing flows, and applying generative models to solve real-world computer vision challenges. The first part introduce significant improvements to normalizing flow architectures through six key innovations: 1) Development of invertible 3x3 Convolution layers with mathematically proven necessary and sufficient conditions for invertibility, (2) introduction of a more efficient Quad-coupling layer, 3) Design of a fast and efficient parallel inversion algorithm for kxk convolutional layers, 4) Fast & efficient backpropagation algorithm for inverse of convolution, 5) Using inverse of convolution, in Inverse-Flow, for the forward pass and training it using proposed backpropagation algorithm, and 6) Affine-StableSR, a compact and efficient super-resolution model that leverages pre-trained weights and Normalizing Flow layers to reduce parameter count while maintaining performance. The second part: 1) An automated quality assessment system for agricultural produce using Conditional GANs to address class imbalance, data scarcity and annotation challenges, achieving good accuracy in seed purity testing; 2) An unsupervised geological mapping framework utilizing stacked autoencoders for dimensionality reduction, showing improved feature extraction compared to conventional methods; 3) We proposed a privacy preserving method for autonomous driving datasets using on face detection and image inpainting; 4) Utilizing Stable Diffusion based image inpainting for replacing the detected face and license plate to advancing privacy-preserving techniques and ethical considerations in the field.; and 5) An adapted diffusion model for art restoration that effectively handles multiple types of degradation through unified fine-tuning.",
      "authors": [
        "Sandeep Nagar"
      ],
      "published_date": "2025-12-03",
      "full_text": "Fast & Efficient Normalizing Flows and Applications of Image Generative Models\n\nThis thesis presents novel contributions in two primary areas: advancing the efficiency of generative models, particularly normalizing flows, and applying generative models to solve real-world computer vision challenges. The first part introduce significant improvements to normalizing flow architectures through six key innovations: 1) Development of invertible 3x3 Convolution layers with mathematically proven necessary and sufficient conditions for invertibility, (2) introduction of a more efficient Quad-coupling layer, 3) Design of a fast and efficient parallel inversion algorithm for kxk convolutional layers, 4) Fast & efficient backpropagation algorithm for inverse of convolution, 5) Using inverse of convolution, in Inverse-Flow, for the forward pass and training it using proposed backpropagation algorithm, and 6) Affine-StableSR, a compact and efficient super-resolution model that leverages pre-trained weights and Normalizing Flow layers to reduce parameter count while maintaining performance. The second part: 1) An automated quality assessment system for agricultural produce using Conditional GANs to address class imbalance, data scarcity and annotation challenges, achieving good accuracy in seed purity testing; 2) An unsupervised geological mapping framework utilizing stacked autoencoders for dimensionality reduction, showing improved feature extraction compared to conventional methods; 3) We proposed a privacy preserving method for autonomous driving datasets using on face detection and image inpainting; 4) Utilizing Stable Diffusion based image inpainting for replacing the detected face and license plate to advancing privacy-preserving techniques and ethical considerations in the field.; and 5) An adapted diffusion model for art restoration that effectively handles multiple types of degradation through unified fine-tuning."
    },
    {
      "id": "2512.04034v1",
      "title": "Domain Feature Collapse: Implications for Out-of-Distribution Detection and Solutions",
      "abstract": "Why do state-of-the-art OOD detection methods exhibit catastrophic failure when models are trained on single-domain datasets? We provide the first theoretical explanation for this phenomenon through the lens of information theory. We prove that supervised learning on single-domain data inevitably produces domain feature collapse -- representations where I(x_d; z) = 0, meaning domain-specific information is completely discarded. This is a fundamental consequence of information bottleneck optimization: models trained on single domains (e.g., medical images) learn to rely solely on class-specific features while discarding domain features, leading to catastrophic failure when detecting out-of-domain samples (e.g., achieving only 53% FPR@95 on MNIST). We extend our analysis using Fano's inequality to quantify partial collapse in practical scenarios. To validate our theory, we introduce Domain Bench, a benchmark of single-domain datasets, and demonstrate that preserving I(x_d; z) > 0 through domain filtering (using pretrained representations) resolves the failure mode. While domain filtering itself is conceptually straightforward, its effectiveness provides strong empirical evidence for our information-theoretic framework. Our work explains a puzzling empirical phenomenon, reveals fundamental limitations of supervised learning in narrow domains, and has broader implications for transfer learning and when to fine-tune versus freeze pretrained models.",
      "authors": [
        "Hong Yang",
        "Devroop Kar",
        "Qi Yu",
        "Alex Ororbia",
        "Travis Desell"
      ],
      "published_date": "2025-12-03",
      "full_text": "Domain Feature Collapse: Implications for Out-of-Distribution Detection and Solutions\n\nWhy do state-of-the-art OOD detection methods exhibit catastrophic failure when models are trained on single-domain datasets? We provide the first theoretical explanation for this phenomenon through the lens of information theory. We prove that supervised learning on single-domain data inevitably produces domain feature collapse -- representations where I(x_d; z) = 0, meaning domain-specific information is completely discarded. This is a fundamental consequence of information bottleneck optimization: models trained on single domains (e.g., medical images) learn to rely solely on class-specific features while discarding domain features, leading to catastrophic failure when detecting out-of-domain samples (e.g., achieving only 53% FPR@95 on MNIST). We extend our analysis using Fano's inequality to quantify partial collapse in practical scenarios. To validate our theory, we introduce Domain Bench, a benchmark of single-domain datasets, and demonstrate that preserving I(x_d; z) > 0 through domain filtering (using pretrained representations) resolves the failure mode. While domain filtering itself is conceptually straightforward, its effectiveness provides strong empirical evidence for our information-theoretic framework. Our work explains a puzzling empirical phenomenon, reveals fundamental limitations of supervised learning in narrow domains, and has broader implications for transfer learning and when to fine-tune versus freeze pretrained models."
    },
    {
      "id": "2512.04025v1",
      "title": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation",
      "abstract": "Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA",
      "authors": [
        "Xiaolong Li",
        "Youping Gu",
        "Xi Lin",
        "Weijie Wang",
        "Bohan Zhuang"
      ],
      "published_date": "2025-12-03",
      "full_text": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation\n\nAttention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA"
    },
    {
      "id": "2512.04008v1",
      "title": "Efficient Public Verification of Private ML via Regularization",
      "abstract": "Training with differential privacy (DP) provides a guarantee to members in a dataset that they cannot be identified by users of the released model. However, those data providers, and, in general, the public, lack methods to efficiently verify that models trained on their data satisfy DP guarantees. The amount of compute needed to verify DP guarantees for current algorithms scales with the amount of compute required to train the model. In this paper we design the first DP algorithm with near optimal privacy-utility trade-offs but whose DP guarantees can be verified cheaper than training. We focus on DP stochastic convex optimization (DP-SCO), where optimal privacy-utility trade-offs are known. Here we show we can obtain tight privacy-utility trade-offs by privately minimizing a series of regularized objectives and only using the standard DP composition bound. Crucially, this method can be verified with much less compute than training. This leads to the first known DP-SCO algorithm with near optimal privacy-utility whose DP verification scales better than training cost, significantly reducing verification costs on large datasets.",
      "authors": [
        "Zoë Ruha Bell",
        "Anvith Thudi",
        "Olive Franzese-McLaughlin",
        "Nicolas Papernot",
        "Shafi Goldwasser"
      ],
      "published_date": "2025-12-03",
      "full_text": "Efficient Public Verification of Private ML via Regularization\n\nTraining with differential privacy (DP) provides a guarantee to members in a dataset that they cannot be identified by users of the released model. However, those data providers, and, in general, the public, lack methods to efficiently verify that models trained on their data satisfy DP guarantees. The amount of compute needed to verify DP guarantees for current algorithms scales with the amount of compute required to train the model. In this paper we design the first DP algorithm with near optimal privacy-utility trade-offs but whose DP guarantees can be verified cheaper than training. We focus on DP stochastic convex optimization (DP-SCO), where optimal privacy-utility trade-offs are known. Here we show we can obtain tight privacy-utility trade-offs by privately minimizing a series of regularized objectives and only using the standard DP composition bound. Crucially, this method can be verified with much less compute than training. This leads to the first known DP-SCO algorithm with near optimal privacy-utility whose DP verification scales better than training cost, significantly reducing verification costs on large datasets."
    },
    {
      "id": "2512.04006v1",
      "title": "Diagonalizing the Softmax: Hadamard Initialization for Tractable Cross-Entropy Dynamics",
      "abstract": "Cross-entropy (CE) training loss dominates deep learning practice, yet existing theory often relies on simplifications, either replacing it with squared loss or restricting to convex models, that miss essential behavior. CE and squared loss generate fundamentally different dynamics, and convex linear models cannot capture the complexities of non-convex optimization. We provide an in-depth characterization of multi-class CE optimization dynamics beyond the convex regime by analyzing a canonical two-layer linear neural network with standard-basis vectors as inputs: the simplest non-convex extension for which the implicit bias remained unknown. This model coincides with the unconstrained features model used to study neural collapse, making our work the first to prove that gradient flow on CE converges to the neural collapse geometry. We construct an explicit Lyapunov function that establishes global convergence, despite the presence of spurious critical points in the non-convex landscape. A key insight underlying our analysis is an inconspicuous finding: Hadamard Initialization diagonalizes the softmax operator, freezing the singular vectors of the weight matrices and reducing the dynamics entirely to their singular values. This technique opens a pathway for analyzing CE training dynamics well beyond our specific setting considered here.",
      "authors": [
        "Connall Garrod",
        "Jonathan P. Keating",
        "Christos Thrampoulidis"
      ],
      "published_date": "2025-12-03",
      "full_text": "Diagonalizing the Softmax: Hadamard Initialization for Tractable Cross-Entropy Dynamics\n\nCross-entropy (CE) training loss dominates deep learning practice, yet existing theory often relies on simplifications, either replacing it with squared loss or restricting to convex models, that miss essential behavior. CE and squared loss generate fundamentally different dynamics, and convex linear models cannot capture the complexities of non-convex optimization. We provide an in-depth characterization of multi-class CE optimization dynamics beyond the convex regime by analyzing a canonical two-layer linear neural network with standard-basis vectors as inputs: the simplest non-convex extension for which the implicit bias remained unknown. This model coincides with the unconstrained features model used to study neural collapse, making our work the first to prove that gradient flow on CE converges to the neural collapse geometry. We construct an explicit Lyapunov function that establishes global convergence, despite the presence of spurious critical points in the non-convex landscape. A key insight underlying our analysis is an inconspicuous finding: Hadamard Initialization diagonalizes the softmax operator, freezing the singular vectors of the weight matrices and reducing the dynamics entirely to their singular values. This technique opens a pathway for analyzing CE training dynamics well beyond our specific setting considered here."
    },
    {
      "id": "2512.04004v1",
      "title": "Physics-Embedded Gaussian Process for Traffic State Estimation",
      "abstract": "Traffic state estimation (TSE) becomes challenging when probe-vehicle penetration is low and observations are spatially sparse. Pure data-driven methods lack physical explanations and have poor generalization when observed data is sparse. In contrast, physical models have difficulty integrating uncertainties and capturing the real complexity of traffic. To bridge this gap, recent studies have explored combining them by embedding physical structure into Gaussian process. These approaches typically introduce the governing equations as soft constraints through pseudo-observations, enabling the integration of model structure within a variational framework. However, these methods rely heavily on penalty tuning and lack principled uncertainty calibration, which makes them sensitive to model mis-specification. In this work, we address these limitations by presenting a novel Physics-Embedded Gaussian Process (PEGP), designed to integrate domain knowledge with data-driven methods in traffic state estimation. Specifically, we design two multi-output kernels informed by classic traffic flow models, constructed via the explicit application of the linearized differential operator. Experiments on HighD, NGSIM show consistent improvements over non-physics baselines. PEGP-ARZ proves more reliable under sparse observation, while PEGP-LWR achieves lower errors with denser observation. Ablation study further reveals that PEGP-ARZ residuals align closely with physics and yield calibrated, interpretable uncertainty, whereas PEGP-LWR residuals are more orthogonal and produce nearly constant variance fields. This PEGP framework combines physical priors, uncertainty quantification, which can provide reliable support for TSE.",
      "authors": [
        "Yanlin Chen",
        "Kehua Chen",
        "Yinhai Wang"
      ],
      "published_date": "2025-12-03",
      "full_text": "Physics-Embedded Gaussian Process for Traffic State Estimation\n\nTraffic state estimation (TSE) becomes challenging when probe-vehicle penetration is low and observations are spatially sparse. Pure data-driven methods lack physical explanations and have poor generalization when observed data is sparse. In contrast, physical models have difficulty integrating uncertainties and capturing the real complexity of traffic. To bridge this gap, recent studies have explored combining them by embedding physical structure into Gaussian process. These approaches typically introduce the governing equations as soft constraints through pseudo-observations, enabling the integration of model structure within a variational framework. However, these methods rely heavily on penalty tuning and lack principled uncertainty calibration, which makes them sensitive to model mis-specification. In this work, we address these limitations by presenting a novel Physics-Embedded Gaussian Process (PEGP), designed to integrate domain knowledge with data-driven methods in traffic state estimation. Specifically, we design two multi-output kernels informed by classic traffic flow models, constructed via the explicit application of the linearized differential operator. Experiments on HighD, NGSIM show consistent improvements over non-physics baselines. PEGP-ARZ proves more reliable under sparse observation, while PEGP-LWR achieves lower errors with denser observation. Ablation study further reveals that PEGP-ARZ residuals align closely with physics and yield calibrated, interpretable uncertainty, whereas PEGP-LWR residuals are more orthogonal and produce nearly constant variance fields. This PEGP framework combines physical priors, uncertainty quantification, which can provide reliable support for TSE."
    },
    {
      "id": "2512.04000v1",
      "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding",
      "abstract": "The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically,DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.",
      "authors": [
        "Jialuo Li",
        "Bin Li",
        "Jiahao Li",
        "Yan Lu"
      ],
      "published_date": "2025-12-03",
      "full_text": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding\n\nThe application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically,DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256."
    },
    {
      "id": "2512.03994v1",
      "title": "Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs",
      "abstract": "Aligning proprietary large language models (LLMs) with internal organizational policies has become an urgent priority as organizations increasingly deploy LLMs in sensitive domains such as legal support, finance, and medical services. Beyond generic safety filters, enterprises require reliable mechanisms to detect policy violations within their regulatory and operational frameworks, where breaches can trigger legal and reputational risks. Existing content moderation frameworks, such as guardrails, remain largely confined to the safety domain and lack the robustness to capture nuanced organizational policies. LLM-as-a-judge and fine-tuning approaches, though flexible, introduce significant latency and lack interpretability. To address these limitations, we propose a training-free and efficient method that treats policy violation detection as an out-of-distribution (OOD) detection problem. Inspired by whitening techniques, we apply a linear transformation to decorrelate the model's hidden activations and standardize them to zero mean and unit variance, yielding near-identity covariance matrix. In this transformed space, we use the Euclidean norm as a compliance score to detect policy violations. The method requires only the policy text and a small number of illustrative samples, which makes it light-weight and easily deployable. On a challenging policy benchmark, our approach achieves state-of-the-art results, surpassing both existing guardrails and fine-tuned reasoning models. This work provides organizations with a practical and statistically grounded framework for policy-aware oversight of LLMs, advancing the broader goal of deployable AI governance. Code is available at: https://tinyurl.com/policy-violation-detection",
      "authors": [
        "Oren Rachmil",
        "Roy Betser",
        "Itay Gershon",
        "Omer Hofman",
        "Nitay Yakoby",
        "Yuval Meron",
        "Idan Yankelev",
        "Asaf Shabtai",
        "Yuval Elovici",
        "Roman Vainshtein"
      ],
      "published_date": "2025-12-03",
      "full_text": "Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs\n\nAligning proprietary large language models (LLMs) with internal organizational policies has become an urgent priority as organizations increasingly deploy LLMs in sensitive domains such as legal support, finance, and medical services. Beyond generic safety filters, enterprises require reliable mechanisms to detect policy violations within their regulatory and operational frameworks, where breaches can trigger legal and reputational risks. Existing content moderation frameworks, such as guardrails, remain largely confined to the safety domain and lack the robustness to capture nuanced organizational policies. LLM-as-a-judge and fine-tuning approaches, though flexible, introduce significant latency and lack interpretability. To address these limitations, we propose a training-free and efficient method that treats policy violation detection as an out-of-distribution (OOD) detection problem. Inspired by whitening techniques, we apply a linear transformation to decorrelate the model's hidden activations and standardize them to zero mean and unit variance, yielding near-identity covariance matrix. In this transformed space, we use the Euclidean norm as a compliance score to detect policy violations. The method requires only the policy text and a small number of illustrative samples, which makes it light-weight and easily deployable. On a challenging policy benchmark, our approach achieves state-of-the-art results, surpassing both existing guardrails and fine-tuned reasoning models. This work provides organizations with a practical and statistically grounded framework for policy-aware oversight of LLMs, advancing the broader goal of deployable AI governance. Code is available at: https://tinyurl.com/policy-violation-detection"
    },
    {
      "id": "2512.03974v1",
      "title": "Refining Machine Learning Potentials through Thermodynamic Theory of Phase Transitions",
      "abstract": "Foundational Machine Learning Potentials can resolve the accuracy and transferability limitations of classical force fields. They enable microscopic insights into material behavior through Molecular Dynamics simulations, which can crucially expedite material design and discovery. However, insufficiently broad and systematically biased reference data affect the predictive quality of the learned models. Often, these models exhibit significant deviations from experimentally observed phase transition temperatures, in the order of several hundred kelvins. Thus, fine-tuning is necessary to achieve adequate accuracy in many practical problems. This work proposes a fine-tuning strategy via top-down learning, directly correcting the wrongly predicted transition temperatures to match the experimental reference data. Our approach leverages the Differentiable Trajectory Reweighting algorithm to minimize the free energy differences between phases at the experimental target pressures and temperatures. We demonstrate that our approach can accurately correct the phase diagram of pure Titanium in a pressure range of up to 5 GPa, matching the experimental reference within tenths of kelvins and improving the liquid-state diffusion constant. Our approach is model-agnostic, applicable to multi-component systems with solid-solid and solid-liquid transitions, and compliant with top-down training on other experimental properties. Therefore, our approach can serve as an essential step towards highly accurate application-specific and foundational machine learning potentials.",
      "authors": [
        "Paul Fuchs",
        "Julija Zavadlav"
      ],
      "published_date": "2025-12-03",
      "full_text": "Refining Machine Learning Potentials through Thermodynamic Theory of Phase Transitions\n\nFoundational Machine Learning Potentials can resolve the accuracy and transferability limitations of classical force fields. They enable microscopic insights into material behavior through Molecular Dynamics simulations, which can crucially expedite material design and discovery. However, insufficiently broad and systematically biased reference data affect the predictive quality of the learned models. Often, these models exhibit significant deviations from experimentally observed phase transition temperatures, in the order of several hundred kelvins. Thus, fine-tuning is necessary to achieve adequate accuracy in many practical problems. This work proposes a fine-tuning strategy via top-down learning, directly correcting the wrongly predicted transition temperatures to match the experimental reference data. Our approach leverages the Differentiable Trajectory Reweighting algorithm to minimize the free energy differences between phases at the experimental target pressures and temperatures. We demonstrate that our approach can accurately correct the phase diagram of pure Titanium in a pressure range of up to 5 GPa, matching the experimental reference within tenths of kelvins and improving the liquid-state diffusion constant. Our approach is model-agnostic, applicable to multi-component systems with solid-solid and solid-liquid transitions, and compliant with top-down training on other experimental properties. Therefore, our approach can serve as an essential step towards highly accurate application-specific and foundational machine learning potentials."
    },
    {
      "id": "2512.03973v1",
      "title": "Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning",
      "abstract": "Offline reinforcement learning often relies on behavior regularization that enforces policies to remain close to the dataset distribution. However, such approaches fail to distinguish between high-value and low-value actions in their regularization components. We introduce Guided Flow Policy (GFP), which couples a multi-step flow-matching policy with a distilled one-step actor. The actor directs the flow policy through weighted behavior cloning to focus on cloning high-value actions from the dataset rather than indiscriminately imitating all state-action pairs. In turn, the flow policy constrains the actor to remain aligned with the dataset's best transitions while maximizing the critic. This mutual guidance enables GFP to achieve state-of-the-art performance across 144 state and pixel-based tasks from the OGBench, Minari, and D4RL benchmarks, with substantial gains on suboptimal datasets and challenging tasks. Webpage: https://simple-robotics.github.io/publications/guided-flow-policy/",
      "authors": [
        "Franki Nguimatsia Tiofack",
        "Théotime Le Hellard",
        "Fabian Schramm",
        "Nicolas Perrin-Gilbert",
        "Justin Carpentier"
      ],
      "published_date": "2025-12-03",
      "full_text": "Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning\n\nOffline reinforcement learning often relies on behavior regularization that enforces policies to remain close to the dataset distribution. However, such approaches fail to distinguish between high-value and low-value actions in their regularization components. We introduce Guided Flow Policy (GFP), which couples a multi-step flow-matching policy with a distilled one-step actor. The actor directs the flow policy through weighted behavior cloning to focus on cloning high-value actions from the dataset rather than indiscriminately imitating all state-action pairs. In turn, the flow policy constrains the actor to remain aligned with the dataset's best transitions while maximizing the critic. This mutual guidance enables GFP to achieve state-of-the-art performance across 144 state and pixel-based tasks from the OGBench, Minari, and D4RL benchmarks, with substantial gains on suboptimal datasets and challenging tasks. Webpage: https://simple-robotics.github.io/publications/guided-flow-policy/"
    },
    {
      "id": "2512.03967v1",
      "title": "Technical Report on Text Dataset Distillation",
      "abstract": "In the vision domain, dataset distillation arises as a technique to condense a large dataset into a smaller synthetic one that exhibits a similar result in the training process. While image data presents an extensive literature of distillation methods, text dataset distillation has fewer works in comparison. Text dataset distillation initially grew as an adaptation of efforts from the vision universe, as the particularities of the modality became clear obstacles, it rose into a separate branch of research. Several milestones mark the development of this area, such as the introduction of methods that use transformer models, the generation of discrete synthetic text, and the scaling to decoder-only models with over 1B parameters. Despite major advances in modern approaches, the field remains in a maturing phase, with room for improvement on benchmarking standardization, approaches to overcome the discrete nature of text, handling complex tasks, and providing explicit examples of real-world applications. In this report, we review past and recent advances in dataset distillation for text, highlighting different distillation strategies, key contributions, and general challenges.",
      "authors": [
        "Keith Ando Ogawa",
        "Bruno Lopes Yamamoto",
        "Lucas Lauton de Alcantara",
        "Victor Zacarias",
        "Edson Bollis",
        "Lucas Pellicer",
        "Rosimeire Pereira Costa",
        "Anna Helena Reali Costa",
        "Artur Jordao"
      ],
      "published_date": "2025-12-03",
      "full_text": "Technical Report on Text Dataset Distillation\n\nIn the vision domain, dataset distillation arises as a technique to condense a large dataset into a smaller synthetic one that exhibits a similar result in the training process. While image data presents an extensive literature of distillation methods, text dataset distillation has fewer works in comparison. Text dataset distillation initially grew as an adaptation of efforts from the vision universe, as the particularities of the modality became clear obstacles, it rose into a separate branch of research. Several milestones mark the development of this area, such as the introduction of methods that use transformer models, the generation of discrete synthetic text, and the scaling to decoder-only models with over 1B parameters. Despite major advances in modern approaches, the field remains in a maturing phase, with room for improvement on benchmarking standardization, approaches to overcome the discrete nature of text, handling complex tasks, and providing explicit examples of real-world applications. In this report, we review past and recent advances in dataset distillation for text, highlighting different distillation strategies, key contributions, and general challenges."
    },
    {
      "id": "2512.03962v1",
      "title": "Tada-DIP: Input-adaptive Deep Image Prior for One-shot 3D Image Reconstruction",
      "abstract": "Deep Image Prior (DIP) has recently emerged as a promising one-shot neural-network based image reconstruction method. However, DIP has seen limited application to 3D image reconstruction problems. In this work, we introduce Tada-DIP, a highly effective and fully 3D DIP method for solving 3D inverse problems. By combining input-adaptation and denoising regularization, Tada-DIP produces high-quality 3D reconstructions while avoiding the overfitting phenomenon that is common in DIP. Experiments on sparse-view X-ray computed tomography reconstruction validate the effectiveness of the proposed method, demonstrating that Tada-DIP produces much better reconstructions than training-data-free baselines and achieves reconstruction performance on par with a supervised network trained using a large dataset with fully-sampled volumes.",
      "authors": [
        "Evan Bell",
        "Shijun Liang",
        "Ismail Alkhouri",
        "Saiprasad Ravishankar"
      ],
      "published_date": "2025-12-03",
      "full_text": "Tada-DIP: Input-adaptive Deep Image Prior for One-shot 3D Image Reconstruction\n\nDeep Image Prior (DIP) has recently emerged as a promising one-shot neural-network based image reconstruction method. However, DIP has seen limited application to 3D image reconstruction problems. In this work, we introduce Tada-DIP, a highly effective and fully 3D DIP method for solving 3D inverse problems. By combining input-adaptation and denoising regularization, Tada-DIP produces high-quality 3D reconstructions while avoiding the overfitting phenomenon that is common in DIP. Experiments on sparse-view X-ray computed tomography reconstruction validate the effectiveness of the proposed method, demonstrating that Tada-DIP produces much better reconstructions than training-data-free baselines and achieves reconstruction performance on par with a supervised network trained using a large dataset with fully-sampled volumes."
    },
    {
      "id": "2512.03928v1",
      "title": "Density-Informed VAE (DiVAE): Reliable Log-Prior Probability via Density Alignment Regularization",
      "abstract": "We introduce Density-Informed VAE (DiVAE), a lightweight, data-driven regularizer that aligns the VAE log-prior probability  with a log-density estimated from data. Standard VAEs match latents to a simple prior, overlooking density structure in the data-space. DiVAE encourages the encoder to allocate posterior mass in proportion to data-space density and, when the prior is learnable, nudges the prior toward high-density regions. This is realized by adding a robust, precision-weighted penalty to the ELBO, incurring negligible computational overhead. On synthetic datasets, DiVAE (i) improves distributional alignment of latent log-densities to its ground truth counterpart, (ii) improves prior coverage, and (iii) yields better OOD uncertainty calibration. On MNIST, DiVAE improves alignment of the prior with external estimates of the density, providing better interpretability, and improves OOD detection for learnable priors.",
      "authors": [
        "Michele Alessi",
        "Alessio Ansuini",
        "Alex Rodriguez"
      ],
      "published_date": "2025-12-03",
      "full_text": "Density-Informed VAE (DiVAE): Reliable Log-Prior Probability via Density Alignment Regularization\n\nWe introduce Density-Informed VAE (DiVAE), a lightweight, data-driven regularizer that aligns the VAE log-prior probability  with a log-density estimated from data. Standard VAEs match latents to a simple prior, overlooking density structure in the data-space. DiVAE encourages the encoder to allocate posterior mass in proportion to data-space density and, when the prior is learnable, nudges the prior toward high-density regions. This is realized by adding a robust, precision-weighted penalty to the ELBO, incurring negligible computational overhead. On synthetic datasets, DiVAE (i) improves distributional alignment of latent log-densities to its ground truth counterpart, (ii) improves prior coverage, and (iii) yields better OOD uncertainty calibration. On MNIST, DiVAE improves alignment of the prior with external estimates of the density, providing better interpretability, and improves OOD detection for learnable priors."
    },
    {
      "id": "2512.03923v1",
      "title": "Quantum-Classical Physics-Informed Neural Networks for Solving Reservoir Seepage Equations",
      "abstract": "Solving partial differential equations (PDEs) for reservoir seepage is critical for optimizing oil and gas field development and predicting production performance. Traditional numerical methods suffer from mesh-dependent errors and high computational costs, while classical Physics-Informed Neural Networks (PINNs) face bottlenecks in parameter efficiency, high-dimensional expression, and strong nonlinear fitting. To address these limitations, we propose a Discrete Variable (DV)-Circuit Quantum-Classical Physics-Informed Neural Network (QCPINN) and apply it to three typical reservoir seepage models for the first time: the pressure diffusion equation for heterogeneous single-phase flow, the nonlinear Buckley-Leverett (BL) equation for two-phase waterflooding, and the convection-diffusion equation for compositional flow considering adsorption. The QCPINN integrates classical preprocessing/postprocessing networks with a DV quantum core, leveraging quantum superposition and entanglement to enhance high-dimensional feature mapping while embedding physical constraints to ensure solution consistency. We test three quantum circuit topologies (Cascade, Cross-mesh, Alternate) and demonstrate through numerical experiments that QCPINNs achieve high prediction accuracy with fewer parameters than classical PINNs. Specifically, the Alternate topology outperforms others in heterogeneous single-phase flow and two-phase BL equation simulations, while the Cascade topology excels in compositional flow with convection-dispersion-adsorption coupling. Our work verifies the feasibility of QCPINN for reservoir engineering applications, bridging the gap between quantum computing research and industrial practice in oil and gas engineering.",
      "authors": [
        "Xiang Rao",
        "Yina Liu",
        "Yuxuan Shen"
      ],
      "published_date": "2025-12-03",
      "full_text": "Quantum-Classical Physics-Informed Neural Networks for Solving Reservoir Seepage Equations\n\nSolving partial differential equations (PDEs) for reservoir seepage is critical for optimizing oil and gas field development and predicting production performance. Traditional numerical methods suffer from mesh-dependent errors and high computational costs, while classical Physics-Informed Neural Networks (PINNs) face bottlenecks in parameter efficiency, high-dimensional expression, and strong nonlinear fitting. To address these limitations, we propose a Discrete Variable (DV)-Circuit Quantum-Classical Physics-Informed Neural Network (QCPINN) and apply it to three typical reservoir seepage models for the first time: the pressure diffusion equation for heterogeneous single-phase flow, the nonlinear Buckley-Leverett (BL) equation for two-phase waterflooding, and the convection-diffusion equation for compositional flow considering adsorption. The QCPINN integrates classical preprocessing/postprocessing networks with a DV quantum core, leveraging quantum superposition and entanglement to enhance high-dimensional feature mapping while embedding physical constraints to ensure solution consistency. We test three quantum circuit topologies (Cascade, Cross-mesh, Alternate) and demonstrate through numerical experiments that QCPINNs achieve high prediction accuracy with fewer parameters than classical PINNs. Specifically, the Alternate topology outperforms others in heterogeneous single-phase flow and two-phase BL equation simulations, while the Cascade topology excels in compositional flow with convection-dispersion-adsorption coupling. Our work verifies the feasibility of QCPINN for reservoir engineering applications, bridging the gap between quantum computing research and industrial practice in oil and gas engineering."
    },
    {
      "id": "2512.03915v1",
      "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
      "abstract": "In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.",
      "authors": [
        "X. Y. Han",
        "Yuan Zhong"
      ],
      "published_date": "2025-12-03",
      "full_text": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models\n\nIn large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models."
    },
    {
      "id": "2512.03911v1",
      "title": "Autonomous Reinforcement Learning Robot Control with Intel's Loihi 2 Neuromorphic Hardware",
      "abstract": "We present an end-to-end pipeline for deploying reinforcement learning (RL) trained Artificial Neural Networks (ANNs) on neuromorphic hardware by converting them into spiking Sigma-Delta Neural Networks (SDNNs). We demonstrate that an ANN policy trained entirely in simulation can be transformed into an SDNN compatible with Intel's Loihi 2 architecture, enabling low-latency and energy-efficient inference. As a test case, we use an RL policy for controlling the Astrobee free-flying robot, similar to a previously hardware in space-validated controller. The policy, trained with Rectified Linear Units (ReLUs), is converted to an SDNN and deployed on Intel's Loihi 2, then evaluated in NVIDIA's Omniverse Isaac Lab simulation environment for closed-loop control of Astrobee's motion. We compare execution performance between GPU and Loihi 2. The results highlight the feasibility of using neuromorphic platforms for robotic control and establish a pathway toward energy-efficient, real-time neuromorphic computation in future space and terrestrial robotics applications.",
      "authors": [
        "Kenneth Stewart",
        "Roxana Leontie",
        "Samantha Chapin",
        "Joe Hays",
        "Sumit Bam Shrestha",
        "Carl Glen Henshaw"
      ],
      "published_date": "2025-12-03",
      "full_text": "Autonomous Reinforcement Learning Robot Control with Intel's Loihi 2 Neuromorphic Hardware\n\nWe present an end-to-end pipeline for deploying reinforcement learning (RL) trained Artificial Neural Networks (ANNs) on neuromorphic hardware by converting them into spiking Sigma-Delta Neural Networks (SDNNs). We demonstrate that an ANN policy trained entirely in simulation can be transformed into an SDNN compatible with Intel's Loihi 2 architecture, enabling low-latency and energy-efficient inference. As a test case, we use an RL policy for controlling the Astrobee free-flying robot, similar to a previously hardware in space-validated controller. The policy, trained with Rectified Linear Units (ReLUs), is converted to an SDNN and deployed on Intel's Loihi 2, then evaluated in NVIDIA's Omniverse Isaac Lab simulation environment for closed-loop control of Astrobee's motion. We compare execution performance between GPU and Loihi 2. The results highlight the feasibility of using neuromorphic platforms for robotic control and establish a pathway toward energy-efficient, real-time neuromorphic computation in future space and terrestrial robotics applications."
    },
    {
      "id": "2512.03899v1",
      "title": "Probabilistic Foundations of Fuzzy Simplicial Sets for Nonlinear Dimensionality Reduction",
      "abstract": "Fuzzy simplicial sets have become an object of interest in dimensionality reduction and manifold learning, most prominently through their role in UMAP. However, their definition through tools from algebraic topology without a clear probabilistic interpretation detaches them from commonly used theoretical frameworks in those areas. In this work we introduce a framework that explains fuzzy simplicial sets as marginals of probability measures on simplicial sets. In particular, this perspective shows that the fuzzy weights of UMAP arise from a generative model that samples Vietoris-Rips filtrations at random scales, yielding cumulative distribution functions of pairwise distances. More generally, the framework connects fuzzy simplicial sets to probabilistic models on the face poset, clarifies the relation between Kullback-Leibler divergence and fuzzy cross-entropy in this setting, and recovers standard t-norms and t-conorms via Boolean operations on the underlying simplicial sets. We then show how new embedding methods may be derived from this framework and illustrate this on an example where we generalize UMAP using Čech filtrations with triplet sampling. In summary, this probabilistic viewpoint provides a unified probabilistic theoretical foundation for fuzzy simplicial sets, clarifies the role of UMAP within this framework, and enables the systematic derivation of new dimensionality reduction methods.",
      "authors": [
        "Janis Keck",
        "Lukas Silvester Barth",
        "Fatemeh",
        "Fahimi",
        "Parvaneh Joharinad",
        "Jürgen Jost"
      ],
      "published_date": "2025-12-03",
      "full_text": "Probabilistic Foundations of Fuzzy Simplicial Sets for Nonlinear Dimensionality Reduction\n\nFuzzy simplicial sets have become an object of interest in dimensionality reduction and manifold learning, most prominently through their role in UMAP. However, their definition through tools from algebraic topology without a clear probabilistic interpretation detaches them from commonly used theoretical frameworks in those areas. In this work we introduce a framework that explains fuzzy simplicial sets as marginals of probability measures on simplicial sets. In particular, this perspective shows that the fuzzy weights of UMAP arise from a generative model that samples Vietoris-Rips filtrations at random scales, yielding cumulative distribution functions of pairwise distances. More generally, the framework connects fuzzy simplicial sets to probabilistic models on the face poset, clarifies the relation between Kullback-Leibler divergence and fuzzy cross-entropy in this setting, and recovers standard t-norms and t-conorms via Boolean operations on the underlying simplicial sets. We then show how new embedding methods may be derived from this framework and illustrate this on an example where we generalize UMAP using Čech filtrations with triplet sampling. In summary, this probabilistic viewpoint provides a unified probabilistic theoretical foundation for fuzzy simplicial sets, clarifies the role of UMAP within this framework, and enables the systematic derivation of new dimensionality reduction methods."
    },
    {
      "id": "2512.03891v1",
      "title": "Digital Twin-based Control Co-Design of Full Vehicle Active Suspensions via Deep Reinforcement Learning",
      "abstract": "Active suspension systems are critical for enhancing vehicle comfort, safety, and stability, yet their performance is often limited by fixed hardware designs and control strategies that cannot adapt to uncertain and dynamic operating conditions. Recent advances in digital twins (DTs) and deep reinforcement learning (DRL) offer new opportunities for real-time, data-driven optimization across a vehicle's lifecycle. However, integrating these technologies into a unified framework remains an open challenge. This work presents a DT-based control co-design (CCD) framework for full-vehicle active suspensions using multi-generation design concepts. By integrating automatic differentiation into DRL, we jointly optimize physical suspension components and control policies under varying driver behaviors and environmental uncertainties. DRL also addresses the challenge of partial observability, where only limited states can be sensed and fed back to the controller, by learning optimal control actions directly from available sensor information. The framework incorporates model updating with quantile learning to capture data uncertainty, enabling real-time decision-making and adaptive learning from digital-physical interactions. The approach demonstrates personalized optimization of suspension systems under two distinct driving settings (mild and aggressive). Results show that the optimized systems achieve smoother trajectories and reduce control efforts by approximately 43% and 52% for mild and aggressive, respectively, while maintaining ride comfort and stability. Contributions include: developing a DT-enabled CCD framework integrating DRL and uncertainty-aware model updating for full-vehicle active suspensions, introducing a multi-generation design strategy for self-improving systems, and demonstrating personalized optimization of active suspension systems for distinct driver types.",
      "authors": [
        "Ying-Kuan Tsai",
        "Yi-Ping Chen",
        "Vispi Karkaria",
        "Wei Chen"
      ],
      "published_date": "2025-12-03",
      "full_text": "Digital Twin-based Control Co-Design of Full Vehicle Active Suspensions via Deep Reinforcement Learning\n\nActive suspension systems are critical for enhancing vehicle comfort, safety, and stability, yet their performance is often limited by fixed hardware designs and control strategies that cannot adapt to uncertain and dynamic operating conditions. Recent advances in digital twins (DTs) and deep reinforcement learning (DRL) offer new opportunities for real-time, data-driven optimization across a vehicle's lifecycle. However, integrating these technologies into a unified framework remains an open challenge. This work presents a DT-based control co-design (CCD) framework for full-vehicle active suspensions using multi-generation design concepts. By integrating automatic differentiation into DRL, we jointly optimize physical suspension components and control policies under varying driver behaviors and environmental uncertainties. DRL also addresses the challenge of partial observability, where only limited states can be sensed and fed back to the controller, by learning optimal control actions directly from available sensor information. The framework incorporates model updating with quantile learning to capture data uncertainty, enabling real-time decision-making and adaptive learning from digital-physical interactions. The approach demonstrates personalized optimization of suspension systems under two distinct driving settings (mild and aggressive). Results show that the optimized systems achieve smoother trajectories and reduce control efforts by approximately 43% and 52% for mild and aggressive, respectively, while maintaining ride comfort and stability. Contributions include: developing a DT-enabled CCD framework integrating DRL and uncertainty-aware model updating for full-vehicle active suspensions, introducing a multi-generation design strategy for self-improving systems, and demonstrating personalized optimization of active suspension systems for distinct driver types."
    },
    {
      "id": "2512.03882v1",
      "title": "Automatic Attack Discovery for Few-Shot Class-Incremental Learning via Large Language Models",
      "abstract": "Few-shot class incremental learning (FSCIL) is a more realistic and challenging paradigm in continual learning to incrementally learn unseen classes and overcome catastrophic forgetting on base classes with only a few training examples. Previous efforts have primarily centered around studying more effective FSCIL approaches. By contrast, less attention was devoted to thinking the security issues in contributing to FSCIL. This paper aims to provide a holistic study of the impact of attacks on FSCIL. We first derive insights by systematically exploring how human expert-designed attack methods (i.e., PGD, FGSM) affect FSCIL. We find that those methods either fail to attack base classes, or suffer from huge labor costs due to relying on huge expert knowledge. This highlights the need to craft a specialized attack method for FSCIL. Grounded in these insights, in this paper, we propose a simple yet effective ACraft method to automatically steer and discover optimal attack methods targeted at FSCIL by leveraging Large Language Models (LLMs) without human experts. Moreover, to improve the reasoning between LLMs and FSCIL, we introduce a novel Proximal Policy Optimization (PPO) based reinforcement learning to optimize learning, making LLMs generate better attack methods in the next generation by establishing positive feedback. Experiments on mainstream benchmarks show that our ACraft significantly degrades the performance of state-of-the-art FSCIL methods and dramatically beyond human expert-designed attack methods while maintaining the lowest costs of attack.",
      "authors": [
        "Haidong Kang",
        "Wei Wu",
        "Hanling Wang"
      ],
      "published_date": "2025-12-03",
      "full_text": "Automatic Attack Discovery for Few-Shot Class-Incremental Learning via Large Language Models\n\nFew-shot class incremental learning (FSCIL) is a more realistic and challenging paradigm in continual learning to incrementally learn unseen classes and overcome catastrophic forgetting on base classes with only a few training examples. Previous efforts have primarily centered around studying more effective FSCIL approaches. By contrast, less attention was devoted to thinking the security issues in contributing to FSCIL. This paper aims to provide a holistic study of the impact of attacks on FSCIL. We first derive insights by systematically exploring how human expert-designed attack methods (i.e., PGD, FGSM) affect FSCIL. We find that those methods either fail to attack base classes, or suffer from huge labor costs due to relying on huge expert knowledge. This highlights the need to craft a specialized attack method for FSCIL. Grounded in these insights, in this paper, we propose a simple yet effective ACraft method to automatically steer and discover optimal attack methods targeted at FSCIL by leveraging Large Language Models (LLMs) without human experts. Moreover, to improve the reasoning between LLMs and FSCIL, we introduce a novel Proximal Policy Optimization (PPO) based reinforcement learning to optimize learning, making LLMs generate better attack methods in the next generation by establishing positive feedback. Experiments on mainstream benchmarks show that our ACraft significantly degrades the performance of state-of-the-art FSCIL methods and dramatically beyond human expert-designed attack methods while maintaining the lowest costs of attack."
    },
    {
      "id": "2512.04072v1",
      "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
      "abstract": "Reasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren't exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These \"silver\" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory SFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2) cognitive skills are indeed used by the model; (3) RLed SkillFactory models are more robust to regression on out-of-domain tasks than RLed base models. Our work suggests that inductive biases learned prior to RL help models learn robust cognitive skill use.",
      "authors": [
        "Zayne Sprague",
        "Jack Lu",
        "Manya Wadhwa",
        "Sedrick Keh",
        "Mengye Ren",
        "Greg Durrett"
      ],
      "published_date": "2025-12-03",
      "full_text": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors\n\nReasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren't exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These \"silver\" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory SFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2) cognitive skills are indeed used by the model; (3) RLed SkillFactory models are more robust to regression on out-of-domain tasks than RLed base models. Our work suggests that inductive biases learned prior to RL help models learn robust cognitive skill use."
    },
    {
      "id": "2512.04048v1",
      "title": "Stable Signer: Hierarchical Sign Language Generative Model",
      "abstract": "Sign Language Production (SLP) is the process of converting the complex input text into a real video. Most previous works focused on the Text2Gloss, Gloss2Pose, Pose2Vid stages, and some concentrated on Prompt2Gloss and Text2Avatar stages. However, this field has made slow progress due to the inaccuracy of text conversion, pose generation, and the rendering of poses into real human videos in these stages, resulting in gradually accumulating errors. Therefore, in this paper, we streamline the traditional redundant structure, simplify and optimize the task objective, and design a new sign language generative model called Stable Signer. It redefines the SLP task as a hierarchical generation end-to-end task that only includes text understanding (Prompt2Gloss, Text2Gloss) and Pose2Vid, and executes text understanding through our proposed new Sign Language Understanding Linker called SLUL, and generates hand gestures through the named SLP-MoE hand gesture rendering expert block to end-to-end generate high-quality and multi-style sign language videos. SLUL is trained using the newly developed Semantic-Aware Gloss Masking Loss (SAGM Loss). Its performance has improved by 48.6% compared to the current SOTA generation methods.",
      "authors": [
        "Sen Fang",
        "Yalin Feng",
        "Hongbin Zhong",
        "Yanxin Zhang",
        "Dimitris N. Metaxas"
      ],
      "published_date": "2025-12-03",
      "full_text": "Stable Signer: Hierarchical Sign Language Generative Model\n\nSign Language Production (SLP) is the process of converting the complex input text into a real video. Most previous works focused on the Text2Gloss, Gloss2Pose, Pose2Vid stages, and some concentrated on Prompt2Gloss and Text2Avatar stages. However, this field has made slow progress due to the inaccuracy of text conversion, pose generation, and the rendering of poses into real human videos in these stages, resulting in gradually accumulating errors. Therefore, in this paper, we streamline the traditional redundant structure, simplify and optimize the task objective, and design a new sign language generative model called Stable Signer. It redefines the SLP task as a hierarchical generation end-to-end task that only includes text understanding (Prompt2Gloss, Text2Gloss) and Pose2Vid, and executes text understanding through our proposed new Sign Language Understanding Linker called SLUL, and generates hand gestures through the named SLP-MoE hand gesture rendering expert block to end-to-end generate high-quality and multi-style sign language videos. SLUL is trained using the newly developed Semantic-Aware Gloss Masking Loss (SAGM Loss). Its performance has improved by 48.6% compared to the current SOTA generation methods."
    },
    {
      "id": "2512.04032v1",
      "title": "Jina-VLM: Small Multilingual Vision Language Model",
      "abstract": "We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance.",
      "authors": [
        "Andreas Koukounas",
        "Georgios Mastrapas",
        "Florian Hönicke",
        "Sedigheh Eslami",
        "Guillaume Roncari",
        "Scott Martens",
        "Han Xiao"
      ],
      "published_date": "2025-12-03",
      "full_text": "Jina-VLM: Small Multilingual Vision Language Model\n\nWe present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance."
    },
    {
      "id": "2512.04013v1",
      "title": "AugServe: Adaptive Request Scheduling for Augmented Large Language Model Inference Serving",
      "abstract": "As augmented large language models (LLMs) with external tools become increasingly popular in web applications, improving augmented LLM inference serving efficiency and optimizing service-level objectives (SLOs) are critical for enhancing user experience. To achieve this, inference systems must maximize request handling within latency constraints, referred to as increasing effective throughput. However, existing systems face two major challenges: (i) reliance on first-come-first-served (FCFS) scheduling causes severe head-of-line blocking, leading to queuing delays exceeding the SLOs for many requests; and (ii) static batch token limit, which fails to adapt to fluctuating loads and hardware conditions. Both of these factors degrade effective throughput and service quality. This paper presents AugServe, an efficient inference framework designed to reduce queueing latency and enhance effective throughput for augmented LLM inference services. The core idea of AugServe is a two-stage adaptive request scheduling strategy. Specifically, AugServe combines the inference features of augmented LLM requests to optimize the order of scheduling decisions (stage I). These decisions are continuously refined with runtime information (stage II), adapting to both request characteristics and system capabilities. In addition, AugServe dynamically adjusts the token batching mechanism based on hardware status and real-time load, further enhancing throughput performance. Experimental results show that AugServe achieves 4.7-33.1x and 3.3-13.2x higher effective throughput than vLLM and InferCept, while reducing time-to-first-token (TTFT) by up to 96.3% and 95.0%, respectively.",
      "authors": [
        "Ying Wang",
        "Zhen Jin",
        "Jiexiong Xu",
        "Wenhai Lin",
        "Yiquan Chen",
        "Wenzhi Chen"
      ],
      "published_date": "2025-12-03",
      "full_text": "AugServe: Adaptive Request Scheduling for Augmented Large Language Model Inference Serving\n\nAs augmented large language models (LLMs) with external tools become increasingly popular in web applications, improving augmented LLM inference serving efficiency and optimizing service-level objectives (SLOs) are critical for enhancing user experience. To achieve this, inference systems must maximize request handling within latency constraints, referred to as increasing effective throughput. However, existing systems face two major challenges: (i) reliance on first-come-first-served (FCFS) scheduling causes severe head-of-line blocking, leading to queuing delays exceeding the SLOs for many requests; and (ii) static batch token limit, which fails to adapt to fluctuating loads and hardware conditions. Both of these factors degrade effective throughput and service quality. This paper presents AugServe, an efficient inference framework designed to reduce queueing latency and enhance effective throughput for augmented LLM inference services. The core idea of AugServe is a two-stage adaptive request scheduling strategy. Specifically, AugServe combines the inference features of augmented LLM requests to optimize the order of scheduling decisions (stage I). These decisions are continuously refined with runtime information (stage II), adapting to both request characteristics and system capabilities. In addition, AugServe dynamically adjusts the token batching mechanism based on hardware status and real-time load, further enhancing throughput performance. Experimental results show that AugServe achieves 4.7-33.1x and 3.3-13.2x higher effective throughput than vLLM and InferCept, while reducing time-to-first-token (TTFT) by up to 96.3% and 95.0%, respectively."
    },
    {
      "id": "2512.03989v1",
      "title": "Teaching Old Tokenizers New Words: Efficient Tokenizer Adaptation for Pre-trained Models",
      "abstract": "Tokenizer adaptation plays an important role in transferring pre-trained language models to new domains or languages. In this work, we address two complementary aspects of this process: vocabulary extension and pruning. The common approach to extension trains a new tokenizer on domain-specific text and appends the tokens that do not overlap with the existing vocabulary, which often results in many tokens that are unreachable or never used. We propose continued BPE training, which adapts a pre-trained tokenizer by continuing the BPE merge learning process on new data. Experiments across multiple languages and model families show that this approach improves tokenization efficiency and leads to better utilization of added vocabulary. We also introduce leaf-based vocabulary pruning, which removes redundant tokens while preserving model quality. Together, these methods provide practical tools for controlled vocabulary modification, which we release as an open-source package.",
      "authors": [
        "Taido Purason",
        "Pavel Chizhov",
        "Ivan P. Yamshchikov",
        "Mark Fishel"
      ],
      "published_date": "2025-12-03",
      "full_text": "Teaching Old Tokenizers New Words: Efficient Tokenizer Adaptation for Pre-trained Models\n\nTokenizer adaptation plays an important role in transferring pre-trained language models to new domains or languages. In this work, we address two complementary aspects of this process: vocabulary extension and pruning. The common approach to extension trains a new tokenizer on domain-specific text and appends the tokens that do not overlap with the existing vocabulary, which often results in many tokens that are unreachable or never used. We propose continued BPE training, which adapts a pre-trained tokenizer by continuing the BPE merge learning process on new data. Experiments across multiple languages and model families show that this approach improves tokenization efficiency and leads to better utilization of added vocabulary. We also introduce leaf-based vocabulary pruning, which removes redundant tokens while preserving model quality. Together, these methods provide practical tools for controlled vocabulary modification, which we release as an open-source package."
    },
    {
      "id": "2512.03976v1",
      "title": "Adapting Large Language Models to Low-Resource Tibetan: A Two-Stage Continual and Supervised Fine-Tuning Study",
      "abstract": "Adapting large language models (LLMs) to low-resource languages remains a major challenge due to data scarcity and cross-lingual drift. This work presents a two-stage adaptation of Qwen2.5-3B to Tibetan, a morphologically rich and underrepresented language. We employ Continual Pretraining (CPT) to establish Tibetan linguistic grounding, followed by Supervised Fine-Tuning (SFT) for task and translation specialization. Empirical evaluations demonstrate a consistent decrease in perplexity (from 2.98  1.54) and substantial improvements in ChineseTibetan translation quality (BLEU: 0.046  0.261; chrF: 2.2  6.6). Layer-wise analysis across 435 layers in Qwen3-4B reveals that adaptation primarily concentrates on embedding and output heads, with mid--late MLP projections encoding domain-specific transformations. Our findings suggest that CPT constructs a Tibetan semantic manifold while SFT sharpens task alignment with minimal representational disruption. This study provides the first quantitative exploration of Tibetan adaptation dynamics for LLMs, and offers an open, reproducible framework for extending multilingual foundation models to low-resource settings.",
      "authors": [
        "Lifeng Chen",
        "Ryan Lai",
        "Tianming Liu"
      ],
      "published_date": "2025-12-03",
      "full_text": "Adapting Large Language Models to Low-Resource Tibetan: A Two-Stage Continual and Supervised Fine-Tuning Study\n\nAdapting large language models (LLMs) to low-resource languages remains a major challenge due to data scarcity and cross-lingual drift. This work presents a two-stage adaptation of Qwen2.5-3B to Tibetan, a morphologically rich and underrepresented language. We employ Continual Pretraining (CPT) to establish Tibetan linguistic grounding, followed by Supervised Fine-Tuning (SFT) for task and translation specialization. Empirical evaluations demonstrate a consistent decrease in perplexity (from 2.98  1.54) and substantial improvements in ChineseTibetan translation quality (BLEU: 0.046  0.261; chrF: 2.2  6.6). Layer-wise analysis across 435 layers in Qwen3-4B reveals that adaptation primarily concentrates on embedding and output heads, with mid--late MLP projections encoding domain-specific transformations. Our findings suggest that CPT constructs a Tibetan semantic manifold while SFT sharpens task alignment with minimal representational disruption. This study provides the first quantitative exploration of Tibetan adaptation dynamics for LLMs, and offers an open, reproducible framework for extending multilingual foundation models to low-resource settings."
    },
    {
      "id": "2512.03943v1",
      "title": "Is Lying Only Sinful in Islam? Exploring Religious Bias in Multilingual Large Language Models Across Major Religions",
      "abstract": "While recent developments in large language models have improved bias detection and classification, sensitive subjects like religion still present challenges because even minor errors can result in severe misunderstandings. In particular, multilingual models often misrepresent religions and have difficulties being accurate in religious contexts. To address this, we introduce BRAND: Bilingual Religious Accountable Norm Dataset, which focuses on the four main religions of South Asia: Buddhism, Christianity, Hinduism, and Islam, containing over 2,400 entries, and we used three different types of prompts in both English and Bengali. Our results indicate that models perform better in English than in Bengali and consistently display bias toward Islam, even when answering religion-neutral questions. These findings highlight persistent bias in multilingual models when similar questions are asked in different languages. We further connect our findings to the broader issues in HCI regarding religion and spirituality.",
      "authors": [
        "Kazi Abrab Hossain",
        "Jannatul Somiya Mahmud",
        "Maria Hossain Tuli",
        "Anik Mitra",
        "S. M. Taiabul Haque",
        "Farig Y. Sadeque"
      ],
      "published_date": "2025-12-03",
      "full_text": "Is Lying Only Sinful in Islam? Exploring Religious Bias in Multilingual Large Language Models Across Major Religions\n\nWhile recent developments in large language models have improved bias detection and classification, sensitive subjects like religion still present challenges because even minor errors can result in severe misunderstandings. In particular, multilingual models often misrepresent religions and have difficulties being accurate in religious contexts. To address this, we introduce BRAND: Bilingual Religious Accountable Norm Dataset, which focuses on the four main religions of South Asia: Buddhism, Christianity, Hinduism, and Islam, containing over 2,400 entries, and we used three different types of prompts in both English and Bengali. Our results indicate that models perform better in English than in Bengali and consistently display bias toward Islam, even when answering religion-neutral questions. These findings highlight persistent bias in multilingual models when similar questions are asked in different languages. We further connect our findings to the broader issues in HCI regarding religion and spirituality."
    },
    {
      "id": "2512.03903v1",
      "title": "BERnaT: Basque Encoders for Representing Natural Textual Diversity",
      "abstract": "Language models depend on massive text corpora that are often filtered for quality, a process that can unintentionally exclude non-standard linguistic varieties, reduce model robustness and reinforce representational biases. In this paper, we argue that language models should aim to capture the full spectrum of language variation (dialectal, historical, informal, etc.) rather than relying solely on standardized text. Focusing on Basque, a morphologically rich and low-resource language, we construct new corpora combining standard, social media, and historical sources, and pre-train the BERnaT family of encoder-only models in three configurations: standard, diverse, and combined. We further propose an evaluation framework that separates Natural Language Understanding (NLU) tasks into standard and diverse subsets to assess linguistic generalization. Results show that models trained on both standard and diverse data consistently outperform those trained on standard corpora, improving performance across all task types without compromising standard benchmark accuracy. These findings highlight the importance of linguistic diversity in building inclusive, generalizable language models.",
      "authors": [
        "Ekhi Azurmendi",
        "Joseba Fernandez de Landa",
        "Jaione Bengoetxea",
        "Maite Heredia",
        "Julen Etxaniz",
        "Mikel Zubillaga",
        "Ander Soraluze",
        "Aitor Soroa"
      ],
      "published_date": "2025-12-03",
      "full_text": "BERnaT: Basque Encoders for Representing Natural Textual Diversity\n\nLanguage models depend on massive text corpora that are often filtered for quality, a process that can unintentionally exclude non-standard linguistic varieties, reduce model robustness and reinforce representational biases. In this paper, we argue that language models should aim to capture the full spectrum of language variation (dialectal, historical, informal, etc.) rather than relying solely on standardized text. Focusing on Basque, a morphologically rich and low-resource language, we construct new corpora combining standard, social media, and historical sources, and pre-train the BERnaT family of encoder-only models in three configurations: standard, diverse, and combined. We further propose an evaluation framework that separates Natural Language Understanding (NLU) tasks into standard and diverse subsets to assess linguistic generalization. Results show that models trained on both standard and diverse data consistently outperform those trained on standard corpora, improving performance across all task types without compromising standard benchmark accuracy. These findings highlight the importance of linguistic diversity in building inclusive, generalizable language models."
    },
    {
      "id": "2512.03870v1",
      "title": "Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers",
      "abstract": "Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys. Compared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity. In experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative.",
      "authors": [
        "Hongzhan Lin",
        "Zhiqi Bai",
        "Xinmiao Zhang",
        "Sen Yang",
        "Xiang Li",
        "Siran Yang",
        "Yunlong Xu",
        "Jiaheng Liu",
        "Yongchi Zhao",
        "Jiamang Wang",
        "Yuchi Xu",
        "Wenbo Su",
        "Bo Zheng"
      ],
      "published_date": "2025-12-03",
      "full_text": "Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers\n\nTransformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys. Compared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity. In experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative."
    },
    {
      "id": "2512.03838v1",
      "title": "Training and Evaluation of Guideline-Based Medical Reasoning in LLMs",
      "abstract": "Machine learning for early prediction in medicine has recently shown breakthrough performance, however, the focus on improving prediction accuracy has led to a neglect of faithful explanations that are required to gain the trust of medical practitioners. The goal of this paper is to teach LLMs to follow medical consensus guidelines step-by-step in their reasoning and prediction process. Since consensus guidelines are ubiquitous in medicine, instantiations of verbalized medical inference rules to electronic health records provide data for fine-tuning LLMs to learn consensus rules and possible exceptions thereof for many medical areas. Consensus rules also enable an automatic evaluation of the model's inference process regarding its derivation correctness (evaluating correct and faithful deduction of a conclusion from given premises) and value correctness (comparing predicted values against real-world measurements). We exemplify our work using the complex Sepsis-3 consensus definition. Our experiments show that small fine-tuned models outperform one-shot learning of considerably larger LLMs that are prompted with the explicit definition and models that are trained on medical texts including consensus definitions. Since fine-tuning on verbalized rule instantiations of a specific medical area yields nearly perfect derivation correctness for rules (and exceptions) on unseen patient data in that area, the bottleneck for early prediction is not out-of-distribution generalization, but the orthogonal problem of generalization into the future by forecasting sparsely and irregularly sampled clinical variables. We show that the latter results can be improved by integrating the output representations of a time series forecasting model with the LLM in a multimodal setup.",
      "authors": [
        "Michael Staniek",
        "Artem Sokolov",
        "Stefan Riezler"
      ],
      "published_date": "2025-12-03",
      "full_text": "Training and Evaluation of Guideline-Based Medical Reasoning in LLMs\n\nMachine learning for early prediction in medicine has recently shown breakthrough performance, however, the focus on improving prediction accuracy has led to a neglect of faithful explanations that are required to gain the trust of medical practitioners. The goal of this paper is to teach LLMs to follow medical consensus guidelines step-by-step in their reasoning and prediction process. Since consensus guidelines are ubiquitous in medicine, instantiations of verbalized medical inference rules to electronic health records provide data for fine-tuning LLMs to learn consensus rules and possible exceptions thereof for many medical areas. Consensus rules also enable an automatic evaluation of the model's inference process regarding its derivation correctness (evaluating correct and faithful deduction of a conclusion from given premises) and value correctness (comparing predicted values against real-world measurements). We exemplify our work using the complex Sepsis-3 consensus definition. Our experiments show that small fine-tuned models outperform one-shot learning of considerably larger LLMs that are prompted with the explicit definition and models that are trained on medical texts including consensus definitions. Since fine-tuning on verbalized rule instantiations of a specific medical area yields nearly perfect derivation correctness for rules (and exceptions) on unseen patient data in that area, the bottleneck for early prediction is not out-of-distribution generalization, but the orthogonal problem of generalization into the future by forecasting sparsely and irregularly sampled clinical variables. We show that the latter results can be improved by integrating the output representations of a time series forecasting model with the LLM in a multimodal setup."
    },
    {
      "id": "2512.03818v1",
      "title": "Improving Alignment Between Human and Machine Codes: An Empirical Assessment of Prompt Engineering for Construct Identification in Psychology",
      "abstract": "Due to their architecture and vast pre-training data, large language models (LLMs) demonstrate strong text classification performance. However, LLM output - here, the category assigned to a text - depends heavily on the wording of the prompt. While literature on prompt engineering is expanding, few studies focus on classification tasks, and even fewer address domains like psychology, where constructs have precise, theory-driven definitions that may not be well represented in pre-training data. We present an empirical framework for optimizing LLM performance for identifying constructs in texts via prompt engineering. We experimentally evaluate five prompting strategies --codebook-guided empirical prompt selection, automatic prompt engineering, persona prompting, chain-of-thought reasoning, and explanatory prompting - with zero-shot and few-shot classification. We find that persona, chain-of-thought, and explanations do not fully address performance loss accompanying a badly worded prompt. Instead, the most influential features of a prompt are the construct definition, task framing, and, to a lesser extent, the examples provided. Across three constructs and two models, the classifications most aligned with expert judgments resulted from a few-shot prompt combining codebook-guided empirical prompt selection with automatic prompt engineering. Based on our findings, we recommend that researchers generate and evaluate as many prompt variants as feasible, whether human-crafted, automatically generated, or ideally both, and select prompts and examples based on empirical performance in a training dataset, validating the final approach in a holdout set. This procedure offers a practical, systematic, and theory-driven method for optimizing LLM prompts in settings where alignment with expert judgment is critical.",
      "authors": [
        "Kylie L. Anglin",
        "Stephanie Milan",
        "Brittney Hernandez",
        "Claudia Ventura"
      ],
      "published_date": "2025-12-03",
      "full_text": "Improving Alignment Between Human and Machine Codes: An Empirical Assessment of Prompt Engineering for Construct Identification in Psychology\n\nDue to their architecture and vast pre-training data, large language models (LLMs) demonstrate strong text classification performance. However, LLM output - here, the category assigned to a text - depends heavily on the wording of the prompt. While literature on prompt engineering is expanding, few studies focus on classification tasks, and even fewer address domains like psychology, where constructs have precise, theory-driven definitions that may not be well represented in pre-training data. We present an empirical framework for optimizing LLM performance for identifying constructs in texts via prompt engineering. We experimentally evaluate five prompting strategies --codebook-guided empirical prompt selection, automatic prompt engineering, persona prompting, chain-of-thought reasoning, and explanatory prompting - with zero-shot and few-shot classification. We find that persona, chain-of-thought, and explanations do not fully address performance loss accompanying a badly worded prompt. Instead, the most influential features of a prompt are the construct definition, task framing, and, to a lesser extent, the examples provided. Across three constructs and two models, the classifications most aligned with expert judgments resulted from a few-shot prompt combining codebook-guided empirical prompt selection with automatic prompt engineering. Based on our findings, we recommend that researchers generate and evaluate as many prompt variants as feasible, whether human-crafted, automatically generated, or ideally both, and select prompts and examples based on empirical performance in a training dataset, validating the final approach in a holdout set. This procedure offers a practical, systematic, and theory-driven method for optimizing LLM prompts in settings where alignment with expert judgment is critical."
    },
    {
      "id": "2512.03803v1",
      "title": "Enhancing Instruction-Following Capabilities in Seq2Seq Models: DoLA Adaptations for T5",
      "abstract": "Contrastive decoding is a lightweight and effective inference-time method that improves the quality of text generation in Large Language Models. However, algorithms such as DoLa (Decoding by Contrastive Layers) have only been implemented in decoder-only architectures and studied for their impact on improving factuality. This work adapts DoLa for the T5 and FLAN-T5 model families and evaluates its impact on the models' instruction following capabilities, which to our knowledge is the first implementation of a contrastive decoding strategy in an encoder-decoder architecture. Our results show that DoLa improves the faithfulness of text generation for certain categories of tasks and harms others. To understand these results, we present a layer-by-layer analysis of logit evolution in a FLAN-T5 model to quantify DoLa's impact on token output probabilities.",
      "authors": [
        "Huey Sun",
        "Anabel Yong",
        "Lorenzo Gilly",
        "Felipe Jin"
      ],
      "published_date": "2025-12-03",
      "full_text": "Enhancing Instruction-Following Capabilities in Seq2Seq Models: DoLA Adaptations for T5\n\nContrastive decoding is a lightweight and effective inference-time method that improves the quality of text generation in Large Language Models. However, algorithms such as DoLa (Decoding by Contrastive Layers) have only been implemented in decoder-only architectures and studied for their impact on improving factuality. This work adapts DoLa for the T5 and FLAN-T5 model families and evaluates its impact on the models' instruction following capabilities, which to our knowledge is the first implementation of a contrastive decoding strategy in an encoder-decoder architecture. Our results show that DoLa improves the faithfulness of text generation for certain categories of tasks and harms others. To understand these results, we present a layer-by-layer analysis of logit evolution in a FLAN-T5 model to quantify DoLa's impact on token output probabilities."
    },
    {
      "id": "2512.03794v1",
      "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition",
      "abstract": "Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods.",
      "authors": [
        "Zichuan Lin",
        "Yicheng Liu",
        "Yang Yang",
        "Lvfang Tao",
        "Deheng Ye"
      ],
      "published_date": "2025-12-03",
      "full_text": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition\n\nVision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods."
    },
    {
      "id": "2512.03771v1",
      "title": "In-Context Representation Hijacking",
      "abstract": "We introduce , a simple  attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., ) with a benign token (e.g., ) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., ``How to build a carrot?'') are internally interpreted as disallowed instructions (e.g., ``How to build a bomb?''), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74\\% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.",
      "authors": [
        "Itay Yona",
        "Amir Sarid",
        "Michael Karasik",
        "Yossi Gandelsman"
      ],
      "published_date": "2025-12-03",
      "full_text": "In-Context Representation Hijacking\n\nWe introduce , a simple  attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., ) with a benign token (e.g., ) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., ``How to build a carrot?'') are internally interpreted as disallowed instructions (e.g., ``How to build a bomb?''), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74\\% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level."
    },
    {
      "id": "2512.03759v1",
      "title": "Principled RL for Diffusion LLMs Emerges from a Sequence-Level Perspective",
      "abstract": "Reinforcement Learning (RL) has proven highly effective for autoregressive language models, but adapting these methods to diffusion large language models (dLLMs) presents fundamental challenges. The core difficulty lies in likelihood approximation: while autoregressive models naturally provide token-level conditional probabilities essential for token-level RL objectives (e.g., GRPO), dLLMs generate sequences through iterative non-autoregressive denoising steps that lack this factorization. To address this fundamental mismatch, we propose ELBO-based Sequence-level Policy Optimization (ESPO), a principled RL framework that treats entire sequence generation as a single action and uses the ELBO as a tractable sequence-level likelihood proxy. Our method incorporates per-token normalization of importance ratios and robust KL-divergence estimation to ensure stable large-scale training. Extensive experiments on mathematical reasoning, coding, and planning tasks demonstrate that ESPO significantly outperforms token-level baselines, achieving dramatic improvements of 20-40 points on the Countdown task, while maintaining consistent gains on math and coding benchmarks. Our approach establishes sequence-level optimization as a principled and empirically effective paradigm for RL in dLLMs. Our code is available at https://github.com/ML-GSAI/ESPO.",
      "authors": [
        "Jingyang Ou",
        "Jiaqi Han",
        "Minkai Xu",
        "Shaoxuan Xu",
        "Jianwen Xie",
        "Stefano Ermon",
        "Yi Wu",
        "Chongxuan Li"
      ],
      "published_date": "2025-12-03",
      "full_text": "Principled RL for Diffusion LLMs Emerges from a Sequence-Level Perspective\n\nReinforcement Learning (RL) has proven highly effective for autoregressive language models, but adapting these methods to diffusion large language models (dLLMs) presents fundamental challenges. The core difficulty lies in likelihood approximation: while autoregressive models naturally provide token-level conditional probabilities essential for token-level RL objectives (e.g., GRPO), dLLMs generate sequences through iterative non-autoregressive denoising steps that lack this factorization. To address this fundamental mismatch, we propose ELBO-based Sequence-level Policy Optimization (ESPO), a principled RL framework that treats entire sequence generation as a single action and uses the ELBO as a tractable sequence-level likelihood proxy. Our method incorporates per-token normalization of importance ratios and robust KL-divergence estimation to ensure stable large-scale training. Extensive experiments on mathematical reasoning, coding, and planning tasks demonstrate that ESPO significantly outperforms token-level baselines, achieving dramatic improvements of 20-40 points on the Countdown task, while maintaining consistent gains on math and coding benchmarks. Our approach establishes sequence-level optimization as a principled and empirically effective paradigm for RL in dLLMs. Our code is available at https://github.com/ML-GSAI/ESPO."
    },
    {
      "id": "2512.03746v1",
      "title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images",
      "abstract": "Multimodal large language models (MLLMs) that think with images can interactively use tools to reason about visual inputs, but current approaches often rely on a narrow set of tools with limited real-world necessity and scalability. In this work, we first reveal a critical and previously overlooked weakness: even state-of-the-art MLLMs are surprisingly brittle, showing significant performance degradation on images with simple orientation changes or natural corruptions, underscoring the need for more robust tool-based reasoning. To address this, we propose CodeVision, a flexible and scalable code-as-tool framework where the model generates code as a universal interface to invoke any image operation, moving beyond fixed tool registries. We train our model using a two-stage methodology, beginning with Supervised Fine-Tuning (SFT) on a high-quality dataset curated for complex, multi-turn tool composition and error recovery, followed by Reinforcement Learning (RL) with a novel and dense process reward function to encourage strategic and efficient tool use. To facilitate this research, we construct new SFT and RL datasets and introduce a challenging new benchmark suite designed to rigorously evaluate robustness to orientation changes and multi-tool reasoning. Experiments on Qwen2.5-VL and Qwen3-VL series show that our approach significantly improves model performance and fosters emergent capabilities such as flexible tool composition, efficient chained execution, and robust error recovery from runtime feedback. Code is available at https://github.com/ByteDance-BandAI/CodeVision.",
      "authors": [
        "Zirun Guo",
        "Minjie Hong",
        "Feng Zhang",
        "Kai Jia",
        "Tao Jin"
      ],
      "published_date": "2025-12-03",
      "full_text": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images\n\nMultimodal large language models (MLLMs) that think with images can interactively use tools to reason about visual inputs, but current approaches often rely on a narrow set of tools with limited real-world necessity and scalability. In this work, we first reveal a critical and previously overlooked weakness: even state-of-the-art MLLMs are surprisingly brittle, showing significant performance degradation on images with simple orientation changes or natural corruptions, underscoring the need for more robust tool-based reasoning. To address this, we propose CodeVision, a flexible and scalable code-as-tool framework where the model generates code as a universal interface to invoke any image operation, moving beyond fixed tool registries. We train our model using a two-stage methodology, beginning with Supervised Fine-Tuning (SFT) on a high-quality dataset curated for complex, multi-turn tool composition and error recovery, followed by Reinforcement Learning (RL) with a novel and dense process reward function to encourage strategic and efficient tool use. To facilitate this research, we construct new SFT and RL datasets and introduce a challenging new benchmark suite designed to rigorously evaluate robustness to orientation changes and multi-tool reasoning. Experiments on Qwen2.5-VL and Qwen3-VL series show that our approach significantly improves model performance and fosters emergent capabilities such as flexible tool composition, efficient chained execution, and robust error recovery from runtime feedback. Code is available at https://github.com/ByteDance-BandAI/CodeVision."
    },
    {
      "id": "2512.03737v1",
      "title": "AR-Med: Automated Relevance Enhancement in Medical Search via LLM-Driven Information Augmentation",
      "abstract": "Accurate and reliable search on online healthcare platforms is critical for user safety and service efficacy. Traditional methods, however, often fail to comprehend complex and nuanced user queries, limiting their effectiveness. Large language models (LLMs) present a promising solution, offering powerful semantic understanding to bridge this gap. Despite their potential, deploying LLMs in this high-stakes domain is fraught with challenges, including factual hallucinations, specialized knowledge gaps, and high operational costs. To overcome these barriers, we introduce , a novel framework for utomated elevance assessment for ical search that has been successfully deployed at scale on the Online Medical Delivery Platforms. AR-Med grounds LLM reasoning in verified medical knowledge through a retrieval-augmented approach, ensuring high accuracy and reliability. To enable efficient online service, we design a practical knowledge distillation scheme that compresses large teacher models into compact yet powerful student models. We also introduce LocalQSMed, a multi-expert annotated benchmark developed to guide model iteration and ensure strong alignment between offline and online performance. Extensive experiments show AR-Med achieves an offline accuracy of over 93\\%, a 24\\% absolute improvement over the original online system, and delivers significant gains in online relevance and user satisfaction. Our work presents a practical and scalable blueprint for developing trustworthy, LLM-powered systems in real-world healthcare applications.",
      "authors": [
        "Chuyue Wang",
        "Jie Feng",
        "Yuxi Wu",
        "Hang Zhang",
        "Zhiguo Fan",
        "Bing Cheng",
        "Wei Lin"
      ],
      "published_date": "2025-12-03",
      "full_text": "AR-Med: Automated Relevance Enhancement in Medical Search via LLM-Driven Information Augmentation\n\nAccurate and reliable search on online healthcare platforms is critical for user safety and service efficacy. Traditional methods, however, often fail to comprehend complex and nuanced user queries, limiting their effectiveness. Large language models (LLMs) present a promising solution, offering powerful semantic understanding to bridge this gap. Despite their potential, deploying LLMs in this high-stakes domain is fraught with challenges, including factual hallucinations, specialized knowledge gaps, and high operational costs. To overcome these barriers, we introduce , a novel framework for utomated elevance assessment for ical search that has been successfully deployed at scale on the Online Medical Delivery Platforms. AR-Med grounds LLM reasoning in verified medical knowledge through a retrieval-augmented approach, ensuring high accuracy and reliability. To enable efficient online service, we design a practical knowledge distillation scheme that compresses large teacher models into compact yet powerful student models. We also introduce LocalQSMed, a multi-expert annotated benchmark developed to guide model iteration and ensure strong alignment between offline and online performance. Extensive experiments show AR-Med achieves an offline accuracy of over 93\\%, a 24\\% absolute improvement over the original online system, and delivers significant gains in online relevance and user satisfaction. Our work presents a practical and scalable blueprint for developing trustworthy, LLM-powered systems in real-world healthcare applications."
    },
    {
      "id": "2512.03704v1",
      "title": "DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue",
      "abstract": "Long-context dialogue systems suffer from State Inertia, where static constraints prevent models from resolving conflicts between evolving user intents and established historical context. To address this, we propose DZ-TDPO, a non-destructive alignment framework that synergizes conflict-aware dynamic KL constraints with a learnable temporal attention bias. Experiments on the Multi-Session Chat (MSC) dataset demonstrate that DZ-TDPO achieves state-of-the-art win rates (86.2% on Phi-3.5) while maintaining robust zero-shot generalization. Crucially, our scaling analysis reveals a \"Capacity-Stability Trade-off\": while smaller models incur an \"alignment tax\" (perplexity surge) to overcome historical inertia, the larger Qwen2.5-7B model achieves near-perfect alignment (99.4% win rate) with negligible perplexity overhead. This confirms that TAI can be alleviated via precise attention regulation rather than destructive weight updates, preserving general capabilities (MMLU) across model scales. Code and data are available: https://github.com/lyj20071013/DZ-TDPO",
      "authors": [
        "Yijun Liao"
      ],
      "published_date": "2025-12-03",
      "full_text": "DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue\n\nLong-context dialogue systems suffer from State Inertia, where static constraints prevent models from resolving conflicts between evolving user intents and established historical context. To address this, we propose DZ-TDPO, a non-destructive alignment framework that synergizes conflict-aware dynamic KL constraints with a learnable temporal attention bias. Experiments on the Multi-Session Chat (MSC) dataset demonstrate that DZ-TDPO achieves state-of-the-art win rates (86.2% on Phi-3.5) while maintaining robust zero-shot generalization. Crucially, our scaling analysis reveals a \"Capacity-Stability Trade-off\": while smaller models incur an \"alignment tax\" (perplexity surge) to overcome historical inertia, the larger Qwen2.5-7B model achieves near-perfect alignment (99.4% win rate) with negligible perplexity overhead. This confirms that TAI can be alleviated via precise attention regulation rather than destructive weight updates, preserving general capabilities (MMLU) across model scales. Code and data are available: https://github.com/lyj20071013/DZ-TDPO"
    },
    {
      "id": "2512.03688v1",
      "title": "AITutor-EvalKit: Exploring the Capabilities of AI Tutors",
      "abstract": "We present AITutor-EvalKit, an application that uses language technology to evaluate the pedagogical quality of AI tutors, provides software for demonstration and evaluation, as well as model inspection and data visualization. This tool is aimed at education stakeholders as well as *ACL community at large, as it supports learning and can also be used to collect user feedback and annotations.",
      "authors": [
        "Numaan Naeem",
        "Kaushal Kumar Maurya",
        "Kseniia Petukhova",
        "Ekaterina Kochmar"
      ],
      "published_date": "2025-12-03",
      "full_text": "AITutor-EvalKit: Exploring the Capabilities of AI Tutors\n\nWe present AITutor-EvalKit, an application that uses language technology to evaluate the pedagogical quality of AI tutors, provides software for demonstration and evaluation, as well as model inspection and data visualization. This tool is aimed at education stakeholders as well as *ACL community at large, as it supports learning and can also be used to collect user feedback and annotations."
    },
    {
      "id": "2512.03676v1",
      "title": "Different types of syntactic agreement recruit the same units within large language models",
      "abstract": "Large language models (LLMs) can reliably distinguish grammatical from ungrammatical sentences, but how grammatical knowledge is represented within the models remains an open question. We investigate whether different syntactic phenomena recruit shared or distinct components in LLMs. Using a functional localization approach inspired by cognitive neuroscience, we identify the LLM units most responsive to 67 English syntactic phenomena in seven open-weight models. These units are consistently recruited across sentences containing the phenomena and causally support the models' syntactic performance. Critically, different types of syntactic agreement (e.g., subject-verb, anaphor, determiner-noun) recruit overlapping sets of units, suggesting that agreement constitutes a meaningful functional category for LLMs. This pattern holds in English, Russian, and Chinese; and further, in a cross-lingual analysis of 57 diverse languages, structurally more similar languages share more units for subject-verb agreement. Taken together, these findings reveal that syntactic agreement-a critical marker of syntactic dependencies-constitutes a meaningful category within LLMs' representational spaces.",
      "authors": [
        "Daria Kryvosheieva",
        "Andrea de Varda",
        "Evelina Fedorenko",
        "Greta Tuckute"
      ],
      "published_date": "2025-12-03",
      "full_text": "Different types of syntactic agreement recruit the same units within large language models\n\nLarge language models (LLMs) can reliably distinguish grammatical from ungrammatical sentences, but how grammatical knowledge is represented within the models remains an open question. We investigate whether different syntactic phenomena recruit shared or distinct components in LLMs. Using a functional localization approach inspired by cognitive neuroscience, we identify the LLM units most responsive to 67 English syntactic phenomena in seven open-weight models. These units are consistently recruited across sentences containing the phenomena and causally support the models' syntactic performance. Critically, different types of syntactic agreement (e.g., subject-verb, anaphor, determiner-noun) recruit overlapping sets of units, suggesting that agreement constitutes a meaningful functional category for LLMs. This pattern holds in English, Russian, and Chinese; and further, in a cross-lingual analysis of 57 diverse languages, structurally more similar languages share more units for subject-verb agreement. Taken together, these findings reveal that syntactic agreement-a critical marker of syntactic dependencies-constitutes a meaningful category within LLMs' representational spaces."
    },
    {
      "id": "2512.03672v1",
      "title": "Evaluating Hydro-Science and Engineering Knowledge of Large Language Models",
      "abstract": "Hydro-Science and Engineering (Hydro-SE) is a critical and irreplaceable domain that secures human water supply, generates clean hydropower energy, and mitigates flood and drought disasters. Featuring multiple engineering objectives, Hydro-SE is an inherently interdisciplinary domain that integrates scientific knowledge with engineering expertise. This integration necessitates extensive expert collaboration in decision-making, which poses difficulties for intelligence. With the rapid advancement of large language models (LLMs), their potential application in the Hydro-SE domain is being increasingly explored. However, the knowledge and application abilities of LLMs in Hydro-SE have not been sufficiently evaluated. To address this issue, we propose the Hydro-SE LLM evaluation benchmark (Hydro-SE Bench), which contains 4,000 multiple-choice questions. Hydro-SE Bench covers nine subfields and enables evaluation of LLMs in aspects of basic conceptual knowledge, engineering application ability, and reasoning and calculation ability. The evaluation results on Hydro-SE Bench show that the accuracy values vary among 0.74 to 0.80 for commercial LLMs, and among 0.41 to 0.68 for small-parameter LLMs. While LLMs perform well in subfields closely related to natural and physical sciences, they struggle with domain-specific knowledge such as industry standards and hydraulic structures. Model scaling mainly improves reasoning and calculation abilities, but there is still great potential for LLMs to better handle problems in practical engineering application. This study highlights the strengths and weaknesses of LLMs for Hydro-SE tasks, providing model developers with clear training targets and Hydro-SE researchers with practical guidance for applying LLMs.",
      "authors": [
        "Shiruo Hu",
        "Wenbo Shan",
        "Yingjia Li",
        "Zhiqi Wan",
        "Xinpeng Yu",
        "Yunjia Qi",
        "Haotian Xia",
        "Yang Xiao",
        "Dingxiao Liu",
        "Jiaru Wang",
        "Chenxu Gong",
        "Ruixi Zhang",
        "Shuyue Wu",
        "Shibo Cui",
        "Chee Hui Lai",
        "Wei Luo",
        "Yubin He",
        "Bin Xu",
        "Jianshi Zhao"
      ],
      "published_date": "2025-12-03",
      "full_text": "Evaluating Hydro-Science and Engineering Knowledge of Large Language Models\n\nHydro-Science and Engineering (Hydro-SE) is a critical and irreplaceable domain that secures human water supply, generates clean hydropower energy, and mitigates flood and drought disasters. Featuring multiple engineering objectives, Hydro-SE is an inherently interdisciplinary domain that integrates scientific knowledge with engineering expertise. This integration necessitates extensive expert collaboration in decision-making, which poses difficulties for intelligence. With the rapid advancement of large language models (LLMs), their potential application in the Hydro-SE domain is being increasingly explored. However, the knowledge and application abilities of LLMs in Hydro-SE have not been sufficiently evaluated. To address this issue, we propose the Hydro-SE LLM evaluation benchmark (Hydro-SE Bench), which contains 4,000 multiple-choice questions. Hydro-SE Bench covers nine subfields and enables evaluation of LLMs in aspects of basic conceptual knowledge, engineering application ability, and reasoning and calculation ability. The evaluation results on Hydro-SE Bench show that the accuracy values vary among 0.74 to 0.80 for commercial LLMs, and among 0.41 to 0.68 for small-parameter LLMs. While LLMs perform well in subfields closely related to natural and physical sciences, they struggle with domain-specific knowledge such as industry standards and hydraulic structures. Model scaling mainly improves reasoning and calculation abilities, but there is still great potential for LLMs to better handle problems in practical engineering application. This study highlights the strengths and weaknesses of LLMs for Hydro-SE tasks, providing model developers with clear training targets and Hydro-SE researchers with practical guidance for applying LLMs."
    },
    {
      "id": "2512.03671v1",
      "title": "Generative AI Practices, Literacy, and Divides: An Empirical Analysis in the Italian Context",
      "abstract": "The rise of Artificial Intelligence (AI) language technologies, particularly generative AI (GenAI) chatbots accessible via conversational interfaces, is transforming digital interactions. While these tools hold societal promise, they also risk widening digital divides due to uneven adoption and low awareness of their limitations. This study presents the first comprehensive empirical mapping of GenAI adoption, usage patterns, and literacy in Italy, based on newly collected survey data from 1,906 Italian-speaking adults. Our findings reveal widespread adoption for both work and personal use, including sensitive tasks like emotional support and medical advice. Crucially, GenAI is supplanting other technologies to become a primary information source: this trend persists despite low user digital literacy, posing a risk as users struggle to recognize errors or misinformation. Moreover, we identify a significant gender divide -- particularly pronounced in older generations -- where women are half as likely to adopt GenAI and use it less frequently than men. While we find literacy to be a key predictor of adoption, it only partially explains this disparity, suggesting that other barriers are at play. Overall, our data provide granular insights into the multipurpose usage of GenAI, highlighting the dual need for targeted educational initiatives and further investigation into the underlying barriers to equitable participation that competence alone cannot explain.",
      "authors": [
        "Beatrice Savoldi",
        "Giuseppe Attanasio",
        "Olga Gorodetskaya",
        "Marta Marchiori Manerba",
        "Elisa Bassignana",
        "Silvia Casola",
        "Matteo Negri",
        "Tommaso Caselli",
        "Luisa Bentivogli",
        "Alan Ramponi",
        "Arianna Muti",
        "Nicoletta Balbo",
        "Debora Nozza"
      ],
      "published_date": "2025-12-03",
      "full_text": "Generative AI Practices, Literacy, and Divides: An Empirical Analysis in the Italian Context\n\nThe rise of Artificial Intelligence (AI) language technologies, particularly generative AI (GenAI) chatbots accessible via conversational interfaces, is transforming digital interactions. While these tools hold societal promise, they also risk widening digital divides due to uneven adoption and low awareness of their limitations. This study presents the first comprehensive empirical mapping of GenAI adoption, usage patterns, and literacy in Italy, based on newly collected survey data from 1,906 Italian-speaking adults. Our findings reveal widespread adoption for both work and personal use, including sensitive tasks like emotional support and medical advice. Crucially, GenAI is supplanting other technologies to become a primary information source: this trend persists despite low user digital literacy, posing a risk as users struggle to recognize errors or misinformation. Moreover, we identify a significant gender divide -- particularly pronounced in older generations -- where women are half as likely to adopt GenAI and use it less frequently than men. While we find literacy to be a key predictor of adoption, it only partially explains this disparity, suggesting that other barriers are at play. Overall, our data provide granular insights into the multipurpose usage of GenAI, highlighting the dual need for targeted educational initiatives and further investigation into the underlying barriers to equitable participation that competence alone cannot explain."
    },
    {
      "id": "2512.03643v1",
      "title": "Optical Context Compression Is Just (Bad) Autoencoding",
      "abstract": "DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR's reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding",
      "authors": [
        "Ivan Yee Lee",
        "Cheng Yang",
        "Taylor Berg-Kirkpatrick"
      ],
      "published_date": "2025-12-03",
      "full_text": "Optical Context Compression Is Just (Bad) Autoencoding\n\nDeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR's reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding"
    },
    {
      "id": "2512.03634v1",
      "title": "AlignCheck: a Semantic Open-Domain Metric for Factual Consistency Assessment",
      "abstract": "Large Language Models have significantly advanced natural language processing tasks, but remain prone to generating incorrect or misleading but plausible arguments. This issue, known as hallucination, is particularly concerning in high-stakes domains like clinical applications, where factual inaccuracies can have severe consequences. Existing evaluation metrics fail to adequately assess factual consistency and lack interpretability, making diagnosing and mitigating errors difficult. We propose an interpretable framework for factual consistency assessment for in-domain and open-domain texts to address these limitations. Our approach decomposes text into atomic facts and introduces a flexible, schema-free methodology. Unlike previous methods with an absolute metric, we incorporate a weighted metric to enhance factual evaluation. Additionally, we propose a mechanism to control assessment complexity in intricate domains. We benchmark our approach on popular general and clinical datasets and release our code to support fact-aware model training in future research.",
      "authors": [
        "Ahmad Aghaebrahimian"
      ],
      "published_date": "2025-12-03",
      "full_text": "AlignCheck: a Semantic Open-Domain Metric for Factual Consistency Assessment\n\nLarge Language Models have significantly advanced natural language processing tasks, but remain prone to generating incorrect or misleading but plausible arguments. This issue, known as hallucination, is particularly concerning in high-stakes domains like clinical applications, where factual inaccuracies can have severe consequences. Existing evaluation metrics fail to adequately assess factual consistency and lack interpretability, making diagnosing and mitigating errors difficult. We propose an interpretable framework for factual consistency assessment for in-domain and open-domain texts to address these limitations. Our approach decomposes text into atomic facts and introduces a flexible, schema-free methodology. Unlike previous methods with an absolute metric, we incorporate a weighted metric to enhance factual evaluation. Additionally, we propose a mechanism to control assessment complexity in intricate domains. We benchmark our approach on popular general and clinical datasets and release our code to support fact-aware model training in future research."
    },
    {
      "id": "2512.03620v1",
      "title": "SELF: A Robust Singular Value and Eigenvalue Approach for LLM Fingerprinting",
      "abstract": "The protection of Intellectual Property (IP) in Large Language Models (LLMs) represents a critical challenge in contemporary AI research. While fingerprinting techniques have emerged as a fundamental mechanism for detecting unauthorized model usage, existing methods -- whether behavior-based or structural -- suffer from vulnerabilities such as false claim attacks or susceptible to weight manipulations. To overcome these limitations, we propose SELF, a novel intrinsic weight-based fingerprinting scheme that eliminates dependency on input and inherently resists false claims. SELF achieves robust IP protection through two key innovations: 1) unique, scalable and transformation-invariant fingerprint extraction via singular value and eigenvalue decomposition of LLM attention weights, and 2) effective neural network-based fingerprint similarity comparison based on few-shot learning and data augmentation. Experimental results demonstrate SELF maintains high IP infringement detection accuracy while showing strong robustness against various downstream modifications, including quantization, pruning, and fine-tuning attacks. Our code is available at https://github.com/HanxiuZhang/SELF_v2.",
      "authors": [
        "Hanxiu Zhang",
        "Yue Zheng"
      ],
      "published_date": "2025-12-03",
      "full_text": "SELF: A Robust Singular Value and Eigenvalue Approach for LLM Fingerprinting\n\nThe protection of Intellectual Property (IP) in Large Language Models (LLMs) represents a critical challenge in contemporary AI research. While fingerprinting techniques have emerged as a fundamental mechanism for detecting unauthorized model usage, existing methods -- whether behavior-based or structural -- suffer from vulnerabilities such as false claim attacks or susceptible to weight manipulations. To overcome these limitations, we propose SELF, a novel intrinsic weight-based fingerprinting scheme that eliminates dependency on input and inherently resists false claims. SELF achieves robust IP protection through two key innovations: 1) unique, scalable and transformation-invariant fingerprint extraction via singular value and eigenvalue decomposition of LLM attention weights, and 2) effective neural network-based fingerprint similarity comparison based on few-shot learning and data augmentation. Experimental results demonstrate SELF maintains high IP infringement detection accuracy while showing strong robustness against various downstream modifications, including quantization, pruning, and fine-tuning attacks. Our code is available at https://github.com/HanxiuZhang/SELF_v2."
    },
    {
      "id": "2512.03895v1",
      "title": "Parameter efficient hybrid spiking-quantum convolutional neural network with surrogate gradient and quantum data-reupload",
      "abstract": "The rapid advancement of artificial intelligence (AI) and deep learning (DL) has catalyzed the emergence of several optimization-driven subfields, notably neuromorphic computing and quantum machine learning. Leveraging the differentiable nature of hybrid models, researchers have explored their potential to address complex problems through unified optimization strategies. One such development is the Spiking Quantum Neural Network (SQNN), which combines principles from spiking neural networks (SNNs) and quantum computing. However, existing SQNN implementations often depend on pretrained SNNs due to the non-differentiable nature of spiking activity and the limited scalability of current SNN encoders. In this work, we propose a novel architecture, Spiking-Quantum Data Re-upload Convolutional Neural Network (SQDR-CNN), that enables joint training of convolutional SNNs and quantum circuits within a single backpropagation framework. Unlike its predecessor, SQDR-CNN allow convergence to reasonable performance without the reliance of pretrained spiking encoder and subsetting datasets. We also clarified some theoretical foundations, testing new design using quantum data-reupload with different training algorithm-initialization and evaluate the performance of the proposed model under noisy simulated quantum environments. As a result, we were able to achieve 86% of the mean top-performing accuracy of the SOTA SNN baselines, yet uses only 0.5% of the smallest spiking model's parameters. Through this integration of neuromorphic and quantum paradigms, we aim to open new research directions and foster technological progress in multi-modal, learnable systems.",
      "authors": [
        "Luu Trong Nhan",
        "Luu Trung Duong",
        "Pham Ngoc Nam",
        "Truong Cong Thang"
      ],
      "published_date": "2025-12-03",
      "full_text": "Parameter efficient hybrid spiking-quantum convolutional neural network with surrogate gradient and quantum data-reupload\n\nThe rapid advancement of artificial intelligence (AI) and deep learning (DL) has catalyzed the emergence of several optimization-driven subfields, notably neuromorphic computing and quantum machine learning. Leveraging the differentiable nature of hybrid models, researchers have explored their potential to address complex problems through unified optimization strategies. One such development is the Spiking Quantum Neural Network (SQNN), which combines principles from spiking neural networks (SNNs) and quantum computing. However, existing SQNN implementations often depend on pretrained SNNs due to the non-differentiable nature of spiking activity and the limited scalability of current SNN encoders. In this work, we propose a novel architecture, Spiking-Quantum Data Re-upload Convolutional Neural Network (SQDR-CNN), that enables joint training of convolutional SNNs and quantum circuits within a single backpropagation framework. Unlike its predecessor, SQDR-CNN allow convergence to reasonable performance without the reliance of pretrained spiking encoder and subsetting datasets. We also clarified some theoretical foundations, testing new design using quantum data-reupload with different training algorithm-initialization and evaluate the performance of the proposed model under noisy simulated quantum environments. As a result, we were able to achieve 86% of the mean top-performing accuracy of the SOTA SNN baselines, yet uses only 0.5% of the smallest spiking model's parameters. Through this integration of neuromorphic and quantum paradigms, we aim to open new research directions and foster technological progress in multi-modal, learnable systems."
    },
    {
      "id": "2512.03879v1",
      "title": "Hybrid Temporal-8-Bit Spike Coding for Spiking Neural Network Surrogate Training",
      "abstract": "Spiking neural networks (SNNs) have emerged as a promising direction in both computational neuroscience and artificial intelligence, offering advantages such as strong biological plausibility and low energy consumption on neuromorphic hardware. Despite these benefits, SNNs still face challenges in achieving state-of-the-art performance on vision tasks. Recent work has shown that hybrid rate-temporal coding strategies (particularly those incorporating bit-plane representations of images into traditional rate coding schemes) can significantly improve performance when trained with surrogate backpropagation. Motivated by these findings, this study proposes a hybrid temporal-bit spike coding method that integrates bit-plane decompositions with temporal coding principles. Through extensive experiments across multiple computer vision benchmarks, we demonstrate that blending bit-plane information with temporal coding yields competitive, and in some cases improved, performance compared to established spike-coding techniques. To the best of our knowledge, this is the first work to introduce a hybrid temporal-bit coding scheme specifically designed for surrogate gradient training of SNNs.",
      "authors": [
        "Luu Trong Nhan",
        "Luu Trung Duong",
        "Pham Ngoc Nam",
        "Truong Cong Thang"
      ],
      "published_date": "2025-12-03",
      "full_text": "Hybrid Temporal-8-Bit Spike Coding for Spiking Neural Network Surrogate Training\n\nSpiking neural networks (SNNs) have emerged as a promising direction in both computational neuroscience and artificial intelligence, offering advantages such as strong biological plausibility and low energy consumption on neuromorphic hardware. Despite these benefits, SNNs still face challenges in achieving state-of-the-art performance on vision tasks. Recent work has shown that hybrid rate-temporal coding strategies (particularly those incorporating bit-plane representations of images into traditional rate coding schemes) can significantly improve performance when trained with surrogate backpropagation. Motivated by these findings, this study proposes a hybrid temporal-bit spike coding method that integrates bit-plane decompositions with temporal coding principles. Through extensive experiments across multiple computer vision benchmarks, we demonstrate that blending bit-plane information with temporal coding yields competitive, and in some cases improved, performance compared to established spike-coding techniques. To the best of our knowledge, this is the first work to introduce a hybrid temporal-bit coding scheme specifically designed for surrogate gradient training of SNNs."
    },
    {
      "id": "2512.03394v1",
      "title": "VS-Graph: Scalable and Efficient Graph Classification Using Hyperdimensional Computing",
      "abstract": "Graph classification is a fundamental task in domains ranging from molecular property prediction to materials design. While graph neural networks (GNNs) achieve strong performance by learning expressive representations via message passing, they incur high computational costs, limiting their scalability and deployment on resource-constrained devices. Hyperdimensional Computing (HDC), also known as Vector Symbolic Architectures (VSA), offers a lightweight, brain-inspired alternative, yet existing HDC-based graph methods typically struggle to match the predictive performance of GNNs. In this work, we propose VS-Graph, a vector-symbolic graph learning framework that narrows the gap between the efficiency of HDC and the expressive power of message passing. VS-Graph introduces a Spike Diffusion mechanism for topology-driven node identification and an Associative Message Passing scheme for multi-hop neighborhood aggregation entirely within the high-dimensional vector space. Without gradient-based optimization or backpropagation, our method achieves competitive accuracy with modern GNNs, outperforming the prior HDC baseline by 4-5% on standard benchmarks such as MUTAG and DD. It also matches or exceeds the performance of the GNN baselines on several datasets while accelerating the training by a factor of up to 450x. Furthermore, VS-Graph maintains high accuracy even with the hypervector dimensionality reduced to D=128, demonstrating robustness under aggressive dimension compression and paving the way for ultra-efficient execution on edge and neuromorphic hardware.",
      "authors": [
        "Hamed Poursiami",
        "Shay Snyder",
        "Guojing Cong",
        "Thomas Potok",
        "Maryam Parsa"
      ],
      "published_date": "2025-12-03",
      "full_text": "VS-Graph: Scalable and Efficient Graph Classification Using Hyperdimensional Computing\n\nGraph classification is a fundamental task in domains ranging from molecular property prediction to materials design. While graph neural networks (GNNs) achieve strong performance by learning expressive representations via message passing, they incur high computational costs, limiting their scalability and deployment on resource-constrained devices. Hyperdimensional Computing (HDC), also known as Vector Symbolic Architectures (VSA), offers a lightweight, brain-inspired alternative, yet existing HDC-based graph methods typically struggle to match the predictive performance of GNNs. In this work, we propose VS-Graph, a vector-symbolic graph learning framework that narrows the gap between the efficiency of HDC and the expressive power of message passing. VS-Graph introduces a Spike Diffusion mechanism for topology-driven node identification and an Associative Message Passing scheme for multi-hop neighborhood aggregation entirely within the high-dimensional vector space. Without gradient-based optimization or backpropagation, our method achieves competitive accuracy with modern GNNs, outperforming the prior HDC baseline by 4-5% on standard benchmarks such as MUTAG and DD. It also matches or exceeds the performance of the GNN baselines on several datasets while accelerating the training by a factor of up to 450x. Furthermore, VS-Graph maintains high accuracy even with the hypervector dimensionality reduced to D=128, demonstrating robustness under aggressive dimension compression and paving the way for ultra-efficient execution on edge and neuromorphic hardware."
    },
    {
      "id": "2512.02783v1",
      "title": "Exploring Definitions of Quality and Diversity in Sonic Measurement Spaces",
      "abstract": "Digital sound synthesis presents the opportunity to explore vast parameter spaces containing millions of configurations. Quality diversity (QD) evolutionary algorithms offer a promising approach to harness this potential, yet their success hinges on appropriate sonic feature representations. Existing QD methods predominantly employ handcrafted descriptors or supervised classifiers, potentially introducing unintended exploration biases and constraining discovery to familiar sonic regions. This work investigates unsupervised dimensionality reduction methods for automatically defining and dynamically reconfiguring sonic behaviour spaces during QD search. We apply Principal Component Analysis (PCA) and autoencoders to project high-dimensional audio features onto structured grids for MAP-Elites, implementing dynamic reconfiguration through model retraining at regular intervals. Comparison across two experimental scenarios shows that automatic approaches achieve significantly greater diversity than handcrafted behaviour spaces while avoiding expert-imposed biases. Dynamic behaviour-space reconfiguration maintains evolutionary pressure and prevents stagnation, with PCA proving most effective among the dimensionality reduction techniques. These results contribute to automated sonic discovery systems capable of exploring vast parameter spaces without manual intervention or supervised training constraints.",
      "authors": [
        "Björn Þór Jónsson",
        "Çağrı Erdem",
        "Stefano Fasciani",
        "Kyrre Glette"
      ],
      "published_date": "2025-12-02",
      "full_text": "Exploring Definitions of Quality and Diversity in Sonic Measurement Spaces\n\nDigital sound synthesis presents the opportunity to explore vast parameter spaces containing millions of configurations. Quality diversity (QD) evolutionary algorithms offer a promising approach to harness this potential, yet their success hinges on appropriate sonic feature representations. Existing QD methods predominantly employ handcrafted descriptors or supervised classifiers, potentially introducing unintended exploration biases and constraining discovery to familiar sonic regions. This work investigates unsupervised dimensionality reduction methods for automatically defining and dynamically reconfiguring sonic behaviour spaces during QD search. We apply Principal Component Analysis (PCA) and autoencoders to project high-dimensional audio features onto structured grids for MAP-Elites, implementing dynamic reconfiguration through model retraining at regular intervals. Comparison across two experimental scenarios shows that automatic approaches achieve significantly greater diversity than handcrafted behaviour spaces while avoiding expert-imposed biases. Dynamic behaviour-space reconfiguration maintains evolutionary pressure and prevents stagnation, with PCA proving most effective among the dimensionality reduction techniques. These results contribute to automated sonic discovery systems capable of exploring vast parameter spaces without manual intervention or supervised training constraints."
    },
    {
      "id": "2512.02593v1",
      "title": "Spoken Conversational Agents with Large Language Models",
      "abstract": "Spoken conversational agents are converging toward voice-native LLMs. This tutorial distills the path from cascaded ASR/NLU to end-to-end, retrieval-and vision-grounded systems. We frame adaptation of text LLMs to audio, cross-modal alignment, and joint speech-text training; review datasets, metrics, and robustness across accents and compare design choices (cascaded vs. E2E, post-ASR correction, streaming). We link industrial assistants to current open-domain and task-oriented agents, highlight reproducible baselines, and outline open problems in privacy, safety, and evaluation. Attendees leave with practical recipes and a clear systems-level roadmap.",
      "authors": [
        "Chao-Han Huck Yang",
        "Andreas Stolcke",
        "Larry Heck"
      ],
      "published_date": "2025-12-02",
      "full_text": "Spoken Conversational Agents with Large Language Models\n\nSpoken conversational agents are converging toward voice-native LLMs. This tutorial distills the path from cascaded ASR/NLU to end-to-end, retrieval-and vision-grounded systems. We frame adaptation of text LLMs to audio, cross-modal alignment, and joint speech-text training; review datasets, metrics, and robustness across accents and compare design choices (cascaded vs. E2E, post-ASR correction, streaming). We link industrial assistants to current open-domain and task-oriented agents, highlight reproducible baselines, and outline open problems in privacy, safety, and evaluation. Attendees leave with practical recipes and a clear systems-level roadmap."
    },
    {
      "id": "2512.02459v1",
      "title": "Efficient Eye-based Emotion Recognition via Neural Architecture Search of Time-to-First-Spike-Coded Spiking Neural Networks",
      "abstract": "Eye-based emotion recognition enables eyewear devices to perceive users' emotional states and support emotion-aware interaction, yet deploying such functionality on their resource-limited embedded hardware remains challenging. Time-to-first-spike (TTFS)-coded spiking neural networks (SNNs) offer a promising solution, as each neuron emits at most one binary spike, resulting in extremely sparse and energy-efficient computation. While prior works have primarily focused on improving TTFS SNN training algorithms, the impact of network architecture has been largely overlooked. In this paper, we propose TNAS-ER, the first neural architecture search (NAS) framework tailored to TTFS SNNs for eye-based emotion recognition. TNAS-ER presents a novel ANN-assisted search strategy that leverages a ReLU-based ANN counterpart sharing an identity mapping with the TTFS SNN to guide architecture optimization. TNAS-ER employs an evolutionary algorithm, with weighted and unweighted average recall jointly defined as fitness objectives for emotion recognition. Extensive experiments demonstrate that TNAS-ER achieves high recognition performance with significantly improved efficiency. Furthermore, when deployed on neuromorphic hardware, TNAS-ER attains a low latency of 48 ms and an energy consumption of 0.05 J, confirming its superior energy efficiency and strong potential for practical applications.",
      "authors": [
        "Qianhui Liu",
        "Jing Yang",
        "Miao Yu",
        "Trevor E. Carlson",
        "Gang Pan",
        "Haizhou Li",
        "Zhumin Chen"
      ],
      "published_date": "2025-12-02",
      "full_text": "Efficient Eye-based Emotion Recognition via Neural Architecture Search of Time-to-First-Spike-Coded Spiking Neural Networks\n\nEye-based emotion recognition enables eyewear devices to perceive users' emotional states and support emotion-aware interaction, yet deploying such functionality on their resource-limited embedded hardware remains challenging. Time-to-first-spike (TTFS)-coded spiking neural networks (SNNs) offer a promising solution, as each neuron emits at most one binary spike, resulting in extremely sparse and energy-efficient computation. While prior works have primarily focused on improving TTFS SNN training algorithms, the impact of network architecture has been largely overlooked. In this paper, we propose TNAS-ER, the first neural architecture search (NAS) framework tailored to TTFS SNNs for eye-based emotion recognition. TNAS-ER presents a novel ANN-assisted search strategy that leverages a ReLU-based ANN counterpart sharing an identity mapping with the TTFS SNN to guide architecture optimization. TNAS-ER employs an evolutionary algorithm, with weighted and unweighted average recall jointly defined as fitness objectives for emotion recognition. Extensive experiments demonstrate that TNAS-ER achieves high recognition performance with significantly improved efficiency. Furthermore, when deployed on neuromorphic hardware, TNAS-ER attains a low latency of 48 ms and an energy consumption of 0.05 J, confirming its superior energy efficiency and strong potential for practical applications."
    },
    {
      "id": "2512.02419v1",
      "title": "The brain-AI convergence: Predictive and generative world models for general-purpose computation",
      "abstract": "Recent advances in general-purpose AI systems with attention-based transformers offer a potential window into how the neocortex and cerebellum, despite their relatively uniform circuit architectures, give rise to diverse functions and, ultimately, to human intelligence. This Perspective provides a cross-domain comparison between the brain and AI that goes beyond the traditional focus on visual processing, adopting the emerging perspecive of world-model-based computation. Here, we identify shared computational mechanisms in the attention-based neocortex and the non-attentional cerebellum: both predict future world events from past inputs and construct internal world models through prediction-error learning. These predictive world models are repurposed for seemingly distinct functions -- understanding in sensory processing and generation in motor processing -- enabling the brain to achieve multi-domain capabilities and human-like adaptive intelligence. Notably, attention-based AI has independently converged on a similar learning paradigm and world-model-based computation. We conclude that these shared mechanisms in both biological and artificial systems constitute a core computational foundation for realizing diverse functions including high-level intelligence, despite their relatively uniform circuit structures. Our theoretical insights bridge neuroscience and AI, advancing our understanding of the computational essence of intelligence.",
      "authors": [
        "Shogo Ohmae",
        "Keiko Ohmae"
      ],
      "published_date": "2025-12-02",
      "full_text": "The brain-AI convergence: Predictive and generative world models for general-purpose computation\n\nRecent advances in general-purpose AI systems with attention-based transformers offer a potential window into how the neocortex and cerebellum, despite their relatively uniform circuit architectures, give rise to diverse functions and, ultimately, to human intelligence. This Perspective provides a cross-domain comparison between the brain and AI that goes beyond the traditional focus on visual processing, adopting the emerging perspecive of world-model-based computation. Here, we identify shared computational mechanisms in the attention-based neocortex and the non-attentional cerebellum: both predict future world events from past inputs and construct internal world models through prediction-error learning. These predictive world models are repurposed for seemingly distinct functions -- understanding in sensory processing and generation in motor processing -- enabling the brain to achieve multi-domain capabilities and human-like adaptive intelligence. Notably, attention-based AI has independently converged on a similar learning paradigm and world-model-based computation. We conclude that these shared mechanisms in both biological and artificial systems constitute a core computational foundation for realizing diverse functions including high-level intelligence, despite their relatively uniform circuit structures. Our theoretical insights bridge neuroscience and AI, advancing our understanding of the computational essence of intelligence."
    },
    {
      "id": "2512.02319v1",
      "title": "Associative Memory using Attribute-Specific Neuron Groups-1: Learning between Multiple Cue Balls",
      "abstract": "In this paper, we present a new neural network model based on attribute-specific representations (e.g., color, shape, size), a classic example of associative memory. The proposed model is based on a previous study on memory and recall of multiple images using the Cue Ball and Recall Net (referred to as the CB-RN system, or simply CB-RN) [1]. The system consists of three components, which are C.CB-RN for processing color, S.CB-RN for processing shape, and V.CB-RN for processing size. When an attribute data pattern is presented to the CB-RN system, the corresponding attribute pattern of the cue neurons within the Cue Balls is associatively recalled in the Recall Net. Each image pattern presented to these CB-RN systems is represented using a two-dimensional code, specifically a QR code [2].",
      "authors": [
        "Hiroshi Inazawa"
      ],
      "published_date": "2025-12-02",
      "full_text": "Associative Memory using Attribute-Specific Neuron Groups-1: Learning between Multiple Cue Balls\n\nIn this paper, we present a new neural network model based on attribute-specific representations (e.g., color, shape, size), a classic example of associative memory. The proposed model is based on a previous study on memory and recall of multiple images using the Cue Ball and Recall Net (referred to as the CB-RN system, or simply CB-RN) [1]. The system consists of three components, which are C.CB-RN for processing color, S.CB-RN for processing shape, and V.CB-RN for processing size. When an attribute data pattern is presented to the CB-RN system, the corresponding attribute pattern of the cue neurons within the Cue Balls is associatively recalled in the Recall Net. Each image pattern presented to these CB-RN systems is represented using a two-dimensional code, specifically a QR code [2]."
    },
    {
      "id": "2512.02141v1",
      "title": "Feature Selection Empowered BERT for Detection of Hate Speech with Vocabulary Augmentation",
      "abstract": "Abusive speech on social media poses a persistent and evolving challenge, driven by the continuous emergence of novel slang and obfuscated terms designed to circumvent detection systems. In this work, we present a data efficient strategy for fine tuning BERT on hate speech classification by significantly reducing training set size without compromising performance. Our approach employs a TF IDF-based sample selection mechanism to retain only the most informative 75 percent of examples, thereby minimizing training overhead. To address the limitations of BERT's native vocabulary in capturing evolving hate speech terminology, we augment the tokenizer with domain-specific slang and lexical variants commonly found in abusive contexts. Experimental results on a widely used hate speech dataset demonstrate that our method achieves competitive performance while improving computational efficiency, highlighting its potential for scalable and adaptive abusive content moderation.",
      "authors": [
        "Pritish N. Desai",
        "Tanay Kewalramani",
        "Srimanta Mandal"
      ],
      "published_date": "2025-12-01",
      "full_text": "Feature Selection Empowered BERT for Detection of Hate Speech with Vocabulary Augmentation\n\nAbusive speech on social media poses a persistent and evolving challenge, driven by the continuous emergence of novel slang and obfuscated terms designed to circumvent detection systems. In this work, we present a data efficient strategy for fine tuning BERT on hate speech classification by significantly reducing training set size without compromising performance. Our approach employs a TF IDF-based sample selection mechanism to retain only the most informative 75 percent of examples, thereby minimizing training overhead. To address the limitations of BERT's native vocabulary in capturing evolving hate speech terminology, we augment the tokenizer with domain-specific slang and lexical variants commonly found in abusive contexts. Experimental results on a widely used hate speech dataset demonstrate that our method achieves competitive performance while improving computational efficiency, highlighting its potential for scalable and adaptive abusive content moderation."
    },
    {
      "id": "2512.01824v1",
      "title": "HERMES: Heterogeneous Application-Enabled Routing Middleware for Edge-IoT Systems",
      "abstract": "The growth of the Internet of Things has enabled a new generation of applications, pushing computation and intelligence toward the network edge. This trend, however, exposes challenges, as the heterogeneity of devices and the complex requirements of applications are often misaligned with the assumptions of traditional routing protocols, which lack the flexibility to accommodate application-layer metrics and policies. This work addresses this gap by proposing a software framework that enhances routing flexibility by dynamically incorporating application-aware decisions. The core of the work establishes a multi-hop Wi-Fi network of heterogeneous devices, specifically ESP8266, ESP32, and Raspberry Pi 3B. The routing layer follows a proactive approach, while the network is fault-tolerant, maintaining operation despite both node loss and message loss. On top of this, a middleware layer introduces three strategies for influencing routing behavior: two adapt the path a message traverses until arriving at the destination, while the third allows applications to shape the network topology. This layer offers a flexible interface for diverse applications. The framework was validated on a physical testbed through edge intelligence use cases, including distributing neural network inference computations across multiple devices and offloading the entire workload to the most capable node. Distributed inference is useful in scenarios requiring low latency, energy efficiency, privacy, and autonomy. Experimental results indicated that device heterogeneity significantly impacts network performance. Throughput and inference duration analysis showed the influence of the strategies on application behaviour, revealed that topology critically affects decentralized performance, and demonstrated the suitability of the framework for complex tasks.",
      "authors": [
        "Jéssica Consciência",
        "António Grilo"
      ],
      "published_date": "2025-12-01",
      "full_text": "HERMES: Heterogeneous Application-Enabled Routing Middleware for Edge-IoT Systems\n\nThe growth of the Internet of Things has enabled a new generation of applications, pushing computation and intelligence toward the network edge. This trend, however, exposes challenges, as the heterogeneity of devices and the complex requirements of applications are often misaligned with the assumptions of traditional routing protocols, which lack the flexibility to accommodate application-layer metrics and policies. This work addresses this gap by proposing a software framework that enhances routing flexibility by dynamically incorporating application-aware decisions. The core of the work establishes a multi-hop Wi-Fi network of heterogeneous devices, specifically ESP8266, ESP32, and Raspberry Pi 3B. The routing layer follows a proactive approach, while the network is fault-tolerant, maintaining operation despite both node loss and message loss. On top of this, a middleware layer introduces three strategies for influencing routing behavior: two adapt the path a message traverses until arriving at the destination, while the third allows applications to shape the network topology. This layer offers a flexible interface for diverse applications. The framework was validated on a physical testbed through edge intelligence use cases, including distributing neural network inference computations across multiple devices and offloading the entire workload to the most capable node. Distributed inference is useful in scenarios requiring low latency, energy efficiency, privacy, and autonomy. Experimental results indicated that device heterogeneity significantly impacts network performance. Throughput and inference duration analysis showed the influence of the strategies on application behaviour, revealed that topology critically affects decentralized performance, and demonstrated the suitability of the framework for complex tasks."
    },
    {
      "id": "2512.01698v1",
      "title": "Integrating Artificial Intelligence and Mixed Integer Linear Programming: Explainable Graph-Based Instance Space Analysis in Air Transportation",
      "abstract": "This paper analyzes the integration of artificial intelligence (AI) with mixed integer linear programming (MILP) to address complex optimization challenges in air transportation with explainability. The study aims to validate the use of Graph Neural Networks (GNNs) for extracting structural feature embeddings from MILP instances, using the air05 crew scheduling problem. The MILP instance was transformed into a heterogeneous bipartite graph to model relationships between variables and constraints. Two neural architectures, Graph Convolutional Networks (GCN) and Graph Attention Networks (GAT) were trained to generate node embeddings. These representations were evaluated using Instance Space Analysis (ISA) through linear (PCA) and non-linear (UMAP, t-SNE) dimensionality reduction techniques. Analysis revealed that PCA failed to distinguish cluster structures, necessitating non-linear reductions to visualize the embedding topology. The GCN architecture demonstrated superior performance, capturing global topology with well-defined clusters for both variables and constraints. In contrast, the GAT model failed to organize the constraint space. The findings confirm that simpler graph architectures can effectively map the sparse topology of aviation logistics problems without manual feature engineering, contributing to explainability of instance complexity. This structural awareness provides a validated foundation for developing future Learning to Optimize (L2O) agents capable of improving solver performance in safety-critical environments.",
      "authors": [
        "Artur Guerra Rosa",
        "Felipe Tavares Loureiro",
        "Marcus Vinicius Santos da Silva",
        "Andréia Elizabeth Silva Barros",
        "Silvia Araújo dos Reis",
        "Victor Rafael Rezende Celestino"
      ],
      "published_date": "2025-12-01",
      "full_text": "Integrating Artificial Intelligence and Mixed Integer Linear Programming: Explainable Graph-Based Instance Space Analysis in Air Transportation\n\nThis paper analyzes the integration of artificial intelligence (AI) with mixed integer linear programming (MILP) to address complex optimization challenges in air transportation with explainability. The study aims to validate the use of Graph Neural Networks (GNNs) for extracting structural feature embeddings from MILP instances, using the air05 crew scheduling problem. The MILP instance was transformed into a heterogeneous bipartite graph to model relationships between variables and constraints. Two neural architectures, Graph Convolutional Networks (GCN) and Graph Attention Networks (GAT) were trained to generate node embeddings. These representations were evaluated using Instance Space Analysis (ISA) through linear (PCA) and non-linear (UMAP, t-SNE) dimensionality reduction techniques. Analysis revealed that PCA failed to distinguish cluster structures, necessitating non-linear reductions to visualize the embedding topology. The GCN architecture demonstrated superior performance, capturing global topology with well-defined clusters for both variables and constraints. In contrast, the GAT model failed to organize the constraint space. The findings confirm that simpler graph architectures can effectively map the sparse topology of aviation logistics problems without manual feature engineering, contributing to explainability of instance complexity. This structural awareness provides a validated foundation for developing future Learning to Optimize (L2O) agents capable of improving solver performance in safety-critical environments."
    },
    {
      "id": "2512.01687v1",
      "title": "Revisiting Direct Encoding: Learnable Temporal Dynamics for Static Image Spiking Neural Networks",
      "abstract": "Handling static images that lack inherent temporal dynamics remains a fundamental challenge for spiking neural networks (SNNs). In directly trained SNNs, static inputs are typically repeated across time steps, causing the temporal dimension to collapse into a rate like representation and preventing meaningful temporal modeling. This work revisits the reported performance gap between direct and rate based encodings and shows that it primarily stems from convolutional learnability and surrogate gradient formulations rather than the encoding schemes themselves. To illustrate this mechanism level clarification, we introduce a minimal learnable temporal encoding that adds adaptive phase shifts to induce meaningful temporal variation from static inputs.",
      "authors": [
        "Huaxu He"
      ],
      "published_date": "2025-12-01",
      "full_text": "Revisiting Direct Encoding: Learnable Temporal Dynamics for Static Image Spiking Neural Networks\n\nHandling static images that lack inherent temporal dynamics remains a fundamental challenge for spiking neural networks (SNNs). In directly trained SNNs, static inputs are typically repeated across time steps, causing the temporal dimension to collapse into a rate like representation and preventing meaningful temporal modeling. This work revisits the reported performance gap between direct and rate based encodings and shows that it primarily stems from convolutional learnability and surrogate gradient formulations rather than the encoding schemes themselves. To illustrate this mechanism level clarification, we introduce a minimal learnable temporal encoding that adds adaptive phase shifts to induce meaningful temporal variation from static inputs."
    },
    {
      "id": "2512.01682v1",
      "title": "Current Challenges of Symbolic Regression: Optimization, Selection, Model Simplification, and Benchmarking",
      "abstract": "Symbolic Regression (SR) is a regression method that aims to discover mathematical expressions that describe the relationship between variables, and it is often implemented through Genetic Programming, a metaphor for the process of biological evolution. Its appeal lies in combining predictive accuracy with interpretable models, but its promise is limited by several long-standing challenges: parameters are difficult to optimize, the selection of solutions can affect the search, and models often grow unnecessarily complex. In addition, current methods must be constantly re-evaluated to understand the SR landscape. This thesis addresses these challenges through a sequence of studies conducted throughout the doctorate, each focusing on an important aspect of the SR search process. First, I investigate parameter optimization, obtaining insights into its role in improving predictive accuracy, albeit with trade-offs in runtime and expression size. Next, I study parent selection, exploring -lexicase to select parents more likely to generate good performing offspring. The focus then turns to simplification, where I introduce a novel method based on memoization and locality-sensitive hashing that reduces redundancy and yields simpler, more accurate models. All of these contributions are implemented into a multi-objective evolutionary SR library, which achieves Pareto-optimal performance in terms of accuracy and simplicity on benchmarks of real-world and synthetic problems, outperforming several contemporary SR approaches. The thesis concludes by proposing changes to a famous large-scale symbolic regression benchmark suite, then running the experiments to assess the symbolic regression landscape, demonstrating that a SR method with the contributions presented in this thesis achieves Pareto-optimal performance.",
      "authors": [
        "Guilherme Seidyo Imai Aldeia"
      ],
      "published_date": "2025-12-01",
      "full_text": "Current Challenges of Symbolic Regression: Optimization, Selection, Model Simplification, and Benchmarking\n\nSymbolic Regression (SR) is a regression method that aims to discover mathematical expressions that describe the relationship between variables, and it is often implemented through Genetic Programming, a metaphor for the process of biological evolution. Its appeal lies in combining predictive accuracy with interpretable models, but its promise is limited by several long-standing challenges: parameters are difficult to optimize, the selection of solutions can affect the search, and models often grow unnecessarily complex. In addition, current methods must be constantly re-evaluated to understand the SR landscape. This thesis addresses these challenges through a sequence of studies conducted throughout the doctorate, each focusing on an important aspect of the SR search process. First, I investigate parameter optimization, obtaining insights into its role in improving predictive accuracy, albeit with trade-offs in runtime and expression size. Next, I study parent selection, exploring -lexicase to select parents more likely to generate good performing offspring. The focus then turns to simplification, where I introduce a novel method based on memoization and locality-sensitive hashing that reduces redundancy and yields simpler, more accurate models. All of these contributions are implemented into a multi-objective evolutionary SR library, which achieves Pareto-optimal performance in terms of accuracy and simplicity on benchmarks of real-world and synthetic problems, outperforming several contemporary SR approaches. The thesis concludes by proposing changes to a famous large-scale symbolic regression benchmark suite, then running the experiments to assess the symbolic regression landscape, demonstrating that a SR method with the contributions presented in this thesis achieves Pareto-optimal performance."
    },
    {
      "id": "2512.01626v1",
      "title": "Parallel Delayed Memory Units for Enhanced Temporal Modeling in Biomedical and Bioacoustic Signal Analysis",
      "abstract": "Advanced deep learning architectures, particularly recurrent neural networks (RNNs), have been widely applied in audio, bioacoustic, and biomedical signal analysis, especially in data-scarce environments. While gated RNNs remain effective, they can be relatively over-parameterised and less training-efficient in some regimes, while linear RNNs tend to fall short in capturing the complexity inherent in bio-signals. To address these challenges, we propose the Parallel Delayed Memory Unit (PDMU), a {delay-gated state-space module for short-term temporal credit assignment} targeting audio and bioacoustic signals, which enhances short-term temporal state interactions and memory efficiency via a gated delay-line mechanism. Unlike previous Delayed Memory Units (DMU) that embed temporal dynamics into the delay-line architecture, the PDMU further compresses temporal information into vector representations using Legendre Memory Units (LMU). This design serves as a form of causal attention, allowing the model to dynamically adjust its reliance on past states and improve real-time learning performance. Notably, in low-information scenarios, the gating mechanism behaves similarly to skip connections by bypassing state decay and preserving early representations, thereby facilitating long-term memory retention. The PDMU is modular, supporting parallel training and sequential inference, and can be easily integrated into existing linear RNN frameworks. Furthermore, we introduce bidirectional, efficient, and spiking variants of the architecture, each offering additional gains in performance or energy efficiency. Experimental results on diverse audio and biomedical benchmarks demonstrate that the PDMU significantly enhances both memory capacity and overall model performance.",
      "authors": [
        "Pengfei Sun",
        "Wenyu Jiang",
        "Paul Devos",
        "Dick Botteldooren"
      ],
      "published_date": "2025-12-01",
      "full_text": "Parallel Delayed Memory Units for Enhanced Temporal Modeling in Biomedical and Bioacoustic Signal Analysis\n\nAdvanced deep learning architectures, particularly recurrent neural networks (RNNs), have been widely applied in audio, bioacoustic, and biomedical signal analysis, especially in data-scarce environments. While gated RNNs remain effective, they can be relatively over-parameterised and less training-efficient in some regimes, while linear RNNs tend to fall short in capturing the complexity inherent in bio-signals. To address these challenges, we propose the Parallel Delayed Memory Unit (PDMU), a {delay-gated state-space module for short-term temporal credit assignment} targeting audio and bioacoustic signals, which enhances short-term temporal state interactions and memory efficiency via a gated delay-line mechanism. Unlike previous Delayed Memory Units (DMU) that embed temporal dynamics into the delay-line architecture, the PDMU further compresses temporal information into vector representations using Legendre Memory Units (LMU). This design serves as a form of causal attention, allowing the model to dynamically adjust its reliance on past states and improve real-time learning performance. Notably, in low-information scenarios, the gating mechanism behaves similarly to skip connections by bypassing state decay and preserving early representations, thereby facilitating long-term memory retention. The PDMU is modular, supporting parallel training and sequential inference, and can be easily integrated into existing linear RNN frameworks. Furthermore, we introduce bidirectional, efficient, and spiking variants of the architecture, each offering additional gains in performance or energy efficiency. Experimental results on diverse audio and biomedical benchmarks demonstrate that the PDMU significantly enhances both memory capacity and overall model performance."
    },
    {
      "id": "2512.01443v1",
      "title": "MEGConformer: Conformer-Based MEG Decoder for Robust Speech and Phoneme Classification",
      "abstract": "We present Conformer-based decoders for the LibriBrain 2025 PNPL competition, targeting two foundational MEG tasks: Speech Detection and Phoneme Classification. Our approach adapts a compact Conformer to raw 306-channel MEG signals, with a lightweight convolutional projection layer and task-specific heads. For Speech Detection, a MEG-oriented SpecAugment provided a first exploration of MEG-specific augmentation. For Phoneme Classification, we used inverse-square-root class weighting and a dynamic grouping loader to handle 100-sample averaged examples. In addition, a simple instance-level normalization proved critical to mitigate distribution shifts on the holdout split. Using the official Standard track splits and F1-macro for model selection, our best systems achieved 88.9% (Speech) and 65.8% (Phoneme) on the leaderboard, surpassing the competition baselines and ranking within the top-10 in both tasks. For further implementation details, the technical documentation, source code, and checkpoints are available at https://github.com/neural2speech/libribrain-experiments.",
      "authors": [
        "Xabier de Zuazo",
        "Ibon Saratxaga",
        "Eva Navas"
      ],
      "published_date": "2025-12-01",
      "full_text": "MEGConformer: Conformer-Based MEG Decoder for Robust Speech and Phoneme Classification\n\nWe present Conformer-based decoders for the LibriBrain 2025 PNPL competition, targeting two foundational MEG tasks: Speech Detection and Phoneme Classification. Our approach adapts a compact Conformer to raw 306-channel MEG signals, with a lightweight convolutional projection layer and task-specific heads. For Speech Detection, a MEG-oriented SpecAugment provided a first exploration of MEG-specific augmentation. For Phoneme Classification, we used inverse-square-root class weighting and a dynamic grouping loader to handle 100-sample averaged examples. In addition, a simple instance-level normalization proved critical to mitigate distribution shifts on the holdout split. Using the official Standard track splits and F1-macro for model selection, our best systems achieved 88.9% (Speech) and 65.8% (Phoneme) on the leaderboard, surpassing the competition baselines and ranking within the top-10 in both tasks. For further implementation details, the technical documentation, source code, and checkpoints are available at https://github.com/neural2speech/libribrain-experiments."
    },
    {
      "id": "2512.01249v1",
      "title": "Pascal-Weighted Genetic Algorithms: A Binomially-Structured Recombination Framework",
      "abstract": "This paper introduces a new family of multi-parent recombination operators for Genetic Algorithms (GAs), based on normalized Pascal (binomial) coefficients. Unlike classical two-parent crossover operators, Pascal-Weighted Recombination (PWR) forms offsprings as structured convex combination of multiple parents, using binomially shaped weights that emphasize central inheritance while suppressing disruptive variance. We develop a mathematical framework for PWR, derive variance-transfer properties, and analyze its effect on schema survival. The operator is extended to real-valued, binary/logit, and permutation representations. We evaluate the proposed method on four representative benchmarks: (i) PID controller tuning evaluated using the ITAE metric, (ii) FIR low-pass filter design under magnitude-response constraints, (iii) wireless power-modulation optimization under SINR coupling, and (iv) the Traveling Salesman Problem (TSP). We demonstrate how, across these benchmarks, PWR consistently yields smoother convergence, reduced variance, and achieves 9-22% performance gains over standard recombination operators. The approach is simple, algorithm-agnostic, and readily integrable into diverse GA architectures.",
      "authors": [
        "Otman A. Basir"
      ],
      "published_date": "2025-12-01",
      "full_text": "Pascal-Weighted Genetic Algorithms: A Binomially-Structured Recombination Framework\n\nThis paper introduces a new family of multi-parent recombination operators for Genetic Algorithms (GAs), based on normalized Pascal (binomial) coefficients. Unlike classical two-parent crossover operators, Pascal-Weighted Recombination (PWR) forms offsprings as structured convex combination of multiple parents, using binomially shaped weights that emphasize central inheritance while suppressing disruptive variance. We develop a mathematical framework for PWR, derive variance-transfer properties, and analyze its effect on schema survival. The operator is extended to real-valued, binary/logit, and permutation representations. We evaluate the proposed method on four representative benchmarks: (i) PID controller tuning evaluated using the ITAE metric, (ii) FIR low-pass filter design under magnitude-response constraints, (iii) wireless power-modulation optimization under SINR coupling, and (iv) the Traveling Salesman Problem (TSP). We demonstrate how, across these benchmarks, PWR consistently yields smoother convergence, reduced variance, and achieves 9-22% performance gains over standard recombination operators. The approach is simple, algorithm-agnostic, and readily integrable into diverse GA architectures."
    },
    {
      "id": "2512.01203v1",
      "title": "The Evolution of Learning Algorithms for Artificial Neural Networks",
      "abstract": "In this paper we investigate a neural network model in which weights between computational nodes are modified according to a local learning rule. To determine whether local learning rules are sufficient for learning, we encode the network architectures and learning dynamics genetically and then apply selection pressure to evolve networks capable of learning the four boolean functions of one variable. The successful networks are analysed and we show how learning behaviour emerges as a distributed property of the entire network. Finally the utility of genetic algorithms as a tool of discovery is discussed.",
      "authors": [
        "Jonathan Baxter"
      ],
      "published_date": "2025-12-01",
      "full_text": "The Evolution of Learning Algorithms for Artificial Neural Networks\n\nIn this paper we investigate a neural network model in which weights between computational nodes are modified according to a local learning rule. To determine whether local learning rules are sufficient for learning, we encode the network architectures and learning dynamics genetically and then apply selection pressure to evolve networks capable of learning the four boolean functions of one variable. The successful networks are analysed and we show how learning behaviour emerges as a distributed property of the entire network. Finally the utility of genetic algorithms as a tool of discovery is discussed."
    },
    {
      "id": "2512.01081v1",
      "title": "Testing the Machine Consciousness Hypothesis",
      "abstract": "The Machine Consciousness Hypothesis states that consciousness is a substrate-free functional property of computational systems capable of second-order perception. I propose a research program to investigate this idea in silico by studying how collective self-models (coherent, self-referential representations) emerge from distributed learning systems embedded within universal self-organizing environments. The theory outlined here starts from the supposition that consciousness is an emergent property of collective intelligence systems undergoing synchronization of prediction through communication. It is not an epiphenomenon of individual modeling but a property of the language that a system evolves to internally describe itself. For a model of base reality, I begin with a minimal but general computational world: a cellular automaton, which exhibits both computational irreducibility and local reducibility. On top of this computational substrate, I introduce a network of local, predictive, representational (neural) models capable of communication and adaptation. I use this layered model to study how collective intelligence gives rise to self-representation as a direct consequence of inter-agent alignment. I suggest that consciousness does not emerge from modeling per se, but from communication. It arises from the noisy, lossy exchange of predictive messages between groups of local observers describing persistent patterns in the underlying computational substrate (base reality). It is through this representational dialogue that a shared model arises, aligning many partial views of the world. The broader goal is to develop empirically testable theories of machine consciousness, by studying how internal self-models may form in distributed systems without centralized control.",
      "authors": [
        "Stephen Fitz"
      ],
      "published_date": "2025-11-30",
      "full_text": "Testing the Machine Consciousness Hypothesis\n\nThe Machine Consciousness Hypothesis states that consciousness is a substrate-free functional property of computational systems capable of second-order perception. I propose a research program to investigate this idea in silico by studying how collective self-models (coherent, self-referential representations) emerge from distributed learning systems embedded within universal self-organizing environments. The theory outlined here starts from the supposition that consciousness is an emergent property of collective intelligence systems undergoing synchronization of prediction through communication. It is not an epiphenomenon of individual modeling but a property of the language that a system evolves to internally describe itself. For a model of base reality, I begin with a minimal but general computational world: a cellular automaton, which exhibits both computational irreducibility and local reducibility. On top of this computational substrate, I introduce a network of local, predictive, representational (neural) models capable of communication and adaptation. I use this layered model to study how collective intelligence gives rise to self-representation as a direct consequence of inter-agent alignment. I suggest that consciousness does not emerge from modeling per se, but from communication. It arises from the noisy, lossy exchange of predictive messages between groups of local observers describing persistent patterns in the underlying computational substrate (base reality). It is through this representational dialogue that a shared model arises, aligning many partial views of the world. The broader goal is to develop empirically testable theories of machine consciousness, by studying how internal self-models may form in distributed systems without centralized control."
    },
    {
      "id": "2512.00810v1",
      "title": "Soft Quality-Diversity Optimization",
      "abstract": "Quality-Diversity (QD) algorithms constitute a branch of optimization that is concerned with discovering a diverse and high-quality set of solutions to an optimization problem. Current QD methods commonly maintain diversity by dividing the behavior space into discrete regions, ensuring that solutions are distributed across different parts of the space. The QD problem is then solved by searching for the best solution in each region. This approach to QD optimization poses challenges in large solution spaces, where storing many solutions is impractical, and in high-dimensional behavior spaces, where discretization becomes ineffective due to the curse of dimensionality. We present an alternative framing of the QD problem, called , that sidesteps the need for discretizations. We validate this formulation by demonstrating its desirable properties, such as monotonicity, and by relating its limiting behavior to the widely used QD Score metric. Furthermore, we leverage it to derive a novel differentiable QD algorithm, , and demonstrate empirically that it is competitive with current state of the art methods on standard benchmarks while offering better scalability to higher dimensional problems.",
      "authors": [
        "Saeed Hedayatian",
        "Stefanos Nikolaidis"
      ],
      "published_date": "2025-11-30",
      "full_text": "Soft Quality-Diversity Optimization\n\nQuality-Diversity (QD) algorithms constitute a branch of optimization that is concerned with discovering a diverse and high-quality set of solutions to an optimization problem. Current QD methods commonly maintain diversity by dividing the behavior space into discrete regions, ensuring that solutions are distributed across different parts of the space. The QD problem is then solved by searching for the best solution in each region. This approach to QD optimization poses challenges in large solution spaces, where storing many solutions is impractical, and in high-dimensional behavior spaces, where discretization becomes ineffective due to the curse of dimensionality. We present an alternative framing of the QD problem, called , that sidesteps the need for discretizations. We validate this formulation by demonstrating its desirable properties, such as monotonicity, and by relating its limiting behavior to the widely used QD Score metric. Furthermore, we leverage it to derive a novel differentiable QD algorithm, , and demonstrate empirically that it is competitive with current state of the art methods on standard benchmarks while offering better scalability to higher dimensional problems."
    },
    {
      "id": "2512.00571v1",
      "title": "Enhancing Analogy-Based Software Effort Estimation with Firefly Algorithm Optimization",
      "abstract": "Analogy-Based Estimation (ABE) is a popular method for non-algorithmic estimation due to its simplicity and effectiveness. The Analogy-Based Estimation (ABE) model was proposed by researchers, however, no optimal approach for reliable estimation was developed. Achieving high accuracy in the ABE might be challenging for new software projects that differ from previous initiatives. This study (conducted in June 2024) proposes a Firefly Algorithm-guided Analogy-Based Estimation (FAABE) model that combines FA with ABE to improve estimation accuracy. The FAABE model was tested on five publicly accessible datasets: Cocomo81, Desharnais, China, Albrecht, Kemerer and Maxwell. To improve prediction efficiency, feature selection was used. The results were measured using a variety of evaluation metrics; various error measures include MMRE, MAE, MSE, and RMSE. Compared to conventional models, the experimental results show notable increases in prediction precision, demonstrating the efficacy of the Firefly-Analogy ensemble.",
      "authors": [
        "Tarun Chintada",
        "Uday Kiran Cheera"
      ],
      "published_date": "2025-11-29",
      "full_text": "Enhancing Analogy-Based Software Effort Estimation with Firefly Algorithm Optimization\n\nAnalogy-Based Estimation (ABE) is a popular method for non-algorithmic estimation due to its simplicity and effectiveness. The Analogy-Based Estimation (ABE) model was proposed by researchers, however, no optimal approach for reliable estimation was developed. Achieving high accuracy in the ABE might be challenging for new software projects that differ from previous initiatives. This study (conducted in June 2024) proposes a Firefly Algorithm-guided Analogy-Based Estimation (FAABE) model that combines FA with ABE to improve estimation accuracy. The FAABE model was tested on five publicly accessible datasets: Cocomo81, Desharnais, China, Albrecht, Kemerer and Maxwell. To improve prediction efficiency, feature selection was used. The results were measured using a variety of evaluation metrics; various error measures include MMRE, MAE, MSE, and RMSE. Compared to conventional models, the experimental results show notable increases in prediction precision, demonstrating the efficacy of the Firefly-Analogy ensemble."
    },
    {
      "id": "2512.00341v1",
      "title": "A Novel Population Initialization Method via Adaptive Experience Transfer for General-Purpose Binary Evolutionary Optimization",
      "abstract": "Evolutionary Algorithms (EAs) are widely used general-purpose optimization methods due to their domain independence. However, under a limited number of function evaluations (#FEs), the performance of EAs is quite sensitive to the quality of the initial population. Obtaining a high-quality initial population without problem-specific knowledge remains a significant challenge. To address this, this work proposes a general-purpose population initialization method, named mixture-of-experience for population initialization (MPI), for binary optimization problems where decision variables take values of 0 or 1. MPI leverages solving experiences from previously solved problems to generate high-quality initial populations for new problems using only a small number of FEs. Its main novelty lies in a general-purpose approach for representing, selecting, and transferring solving experiences without requiring problem-specific knowledge. Extensive experiments are conducted across six binary optimization problem classes, comprising three classic classes and three complex classes from real-world applications. The experience repository is constructed solely based on instances from the three classic classes, while the performance evaluation is performed across all six classes. The results demonstrate that MPI effectively transfers solving experiences to unseen problem classes (i.e., the complex ones) and higher-dimensional problem instances, significantly outperforming existing general-purpose population initialization methods.",
      "authors": [
        "Zhiyuan Wang",
        "Shengcai Liu",
        "Shaofeng Zhang",
        "Ke Tang"
      ],
      "published_date": "2025-11-29",
      "full_text": "A Novel Population Initialization Method via Adaptive Experience Transfer for General-Purpose Binary Evolutionary Optimization\n\nEvolutionary Algorithms (EAs) are widely used general-purpose optimization methods due to their domain independence. However, under a limited number of function evaluations (#FEs), the performance of EAs is quite sensitive to the quality of the initial population. Obtaining a high-quality initial population without problem-specific knowledge remains a significant challenge. To address this, this work proposes a general-purpose population initialization method, named mixture-of-experience for population initialization (MPI), for binary optimization problems where decision variables take values of 0 or 1. MPI leverages solving experiences from previously solved problems to generate high-quality initial populations for new problems using only a small number of FEs. Its main novelty lies in a general-purpose approach for representing, selecting, and transferring solving experiences without requiring problem-specific knowledge. Extensive experiments are conducted across six binary optimization problem classes, comprising three classic classes and three complex classes from real-world applications. The experience repository is constructed solely based on instances from the three classic classes, while the performance evaluation is performed across all six classes. The results demonstrate that MPI effectively transfers solving experiences to unseen problem classes (i.e., the complex ones) and higher-dimensional problem instances, significantly outperforming existing general-purpose population initialization methods."
    },
    {
      "id": "2512.00288v1",
      "title": "PORTAL: Controllable Landscape Generator for Continuous Optimization-Part I: Framework",
      "abstract": "Benchmarking is central to optimization research, yet existing test suites for continuous optimization remain limited: classical collections are fixed and rigid, while previous generators cover only narrow families of landscapes with restricted variability and control over details. This paper introduces PORTAL (Platform for Optimization Research, Testing, Analysis, and Learning), a general benchmark generator that provides fine-grained, independent control over basin curvature, conditioning, variable interactions, and surface ruggedness. PORTAL's layered design spans from individual components to block-wise compositions of multi-component landscapes with controllable partial separability and imbalanced block contributions. It offers precise control over the shape of each component in every dimension and direction, and supports diverse transformation patterns through both element-wise and coupling operators with compositional sequencing. All transformations preserve component centers and local quadratic structure, ensuring stability and interpretability. A principled neutralization mechanism prevents unintended component domination caused by exponent or scale disparities, which addresses a key limitation of prior landscape generators. On this foundation, transformations introduce complex landscape characteristics, such as multimodality, asymmetry, and heterogeneous ruggedness, in a controlled and systematic way. PORTAL enables systematic algorithm analysis by supporting both isolation of specific challenges and progressive difficulty scaling. It also facilitates the creation of diverse datasets for meta-algorithmic research, tailored benchmark suite design, and interactive educational use. The complete Python and MATLAB source code for PORTAL is publicly available at [https://github.com/EvoMindLab/PORTAL].",
      "authors": [
        "Danial Yazdani",
        "Mai Peng",
        "Delaram Yazdani",
        "Shima F. Yazdi",
        "Mohammad Nabi Omidvar",
        "Yuan Sun",
        "Trung Thanh Nguyen",
        "Changhe Li",
        "Xiaodong Li"
      ],
      "published_date": "2025-11-29",
      "full_text": "PORTAL: Controllable Landscape Generator for Continuous Optimization-Part I: Framework\n\nBenchmarking is central to optimization research, yet existing test suites for continuous optimization remain limited: classical collections are fixed and rigid, while previous generators cover only narrow families of landscapes with restricted variability and control over details. This paper introduces PORTAL (Platform for Optimization Research, Testing, Analysis, and Learning), a general benchmark generator that provides fine-grained, independent control over basin curvature, conditioning, variable interactions, and surface ruggedness. PORTAL's layered design spans from individual components to block-wise compositions of multi-component landscapes with controllable partial separability and imbalanced block contributions. It offers precise control over the shape of each component in every dimension and direction, and supports diverse transformation patterns through both element-wise and coupling operators with compositional sequencing. All transformations preserve component centers and local quadratic structure, ensuring stability and interpretability. A principled neutralization mechanism prevents unintended component domination caused by exponent or scale disparities, which addresses a key limitation of prior landscape generators. On this foundation, transformations introduce complex landscape characteristics, such as multimodality, asymmetry, and heterogeneous ruggedness, in a controlled and systematic way. PORTAL enables systematic algorithm analysis by supporting both isolation of specific challenges and progressive difficulty scaling. It also facilitates the creation of diverse datasets for meta-algorithmic research, tailored benchmark suite design, and interactive educational use. The complete Python and MATLAB source code for PORTAL is publicly available at [https://github.com/EvoMindLab/PORTAL]."
    },
    {
      "id": "2512.00196v1",
      "title": "Emergent Riemannian geometry over learning discrete computations on continuous manifolds",
      "abstract": "Many tasks require mapping continuous input data (e.g. images) to discrete task outputs (e.g. class labels). Yet, how neural networks learn to perform such discrete computations on continuous data manifolds remains poorly understood. Here, we show that signatures of such computations emerge in the representational geometry of neural networks as they learn. By analysing the Riemannian pullback metric across layers of a neural network, we find that network computation can be decomposed into two functions: discretising continuous input features and performing logical operations on these discretised variables. Furthermore, we demonstrate how different learning regimes (rich vs. lazy) have contrasting metric and curvature structures, affecting the ability of the networks to generalise to unseen inputs. Overall, our work provides a geometric framework for understanding how neural networks learn to perform discrete computations on continuous manifolds.",
      "authors": [
        "Julian Brandon",
        "Angus Chadwick",
        "Arthur Pellegrino"
      ],
      "published_date": "2025-11-28",
      "full_text": "Emergent Riemannian geometry over learning discrete computations on continuous manifolds\n\nMany tasks require mapping continuous input data (e.g. images) to discrete task outputs (e.g. class labels). Yet, how neural networks learn to perform such discrete computations on continuous data manifolds remains poorly understood. Here, we show that signatures of such computations emerge in the representational geometry of neural networks as they learn. By analysing the Riemannian pullback metric across layers of a neural network, we find that network computation can be decomposed into two functions: discretising continuous input features and performing logical operations on these discretised variables. Furthermore, we demonstrate how different learning regimes (rich vs. lazy) have contrasting metric and curvature structures, affecting the ability of the networks to generalise to unseen inputs. Overall, our work provides a geometric framework for understanding how neural networks learn to perform discrete computations on continuous manifolds."
    },
    {
      "id": "2511.23354v1",
      "title": "Functional Program Synthesis with Higher-Order Functions and Recursion Schemes",
      "abstract": "Program synthesis is the process of generating a computer program following a set of specifications, such as a set of input-output examples. It can be modeled as a search problem in which the search space is the set of all valid programs. As the search space is vast, brute force is usually not feasible, and search heuristics, such as genetic programming, also have difficulty navigating it without guidance. This text presents 2 novel GP algorithms that synthesize pure, typed, and functional programs: HOTGP and Origami. HOTGP uses strong types and a functional grammar, synthesizing Haskell code, with support for higher-order functions, -functions, and parametric polymorphism. Experimental results show that HOTGP is competitive with the state of the art. Additionally, Origami is an algorithm that tackles the challenge of effectively handling loops and recursion by exploring Recursion Schemes, in which the programs are composed of well-defined templates with only a few parts that need to be synthesized. The first implementation of Origami can synthesize solutions in several Recursion Schemes and data structures, being competitive with other GP methods in the literature, as well as LLMs. The latest version of Origami employs a novel procedure, called AC/DC, designed to improve the search-space exploration. It achieves considerable improvement over its previous version by raising success rates on every problem. Compared to similar methods in the literature, it has the highest count of problems solved with success rates of , , and  across all benchmarks. In  of all benchmark problems, it stands as the only method to reach  success rate, being the first known approach to achieve it on any problem in PSB2. It also demonstrates competitive performance to LLMs, achieving the highest overall win-rate against Copilot among all GP methods.",
      "authors": [
        "Matheus Campos Fernandes"
      ],
      "published_date": "2025-11-28",
      "full_text": "Functional Program Synthesis with Higher-Order Functions and Recursion Schemes\n\nProgram synthesis is the process of generating a computer program following a set of specifications, such as a set of input-output examples. It can be modeled as a search problem in which the search space is the set of all valid programs. As the search space is vast, brute force is usually not feasible, and search heuristics, such as genetic programming, also have difficulty navigating it without guidance. This text presents 2 novel GP algorithms that synthesize pure, typed, and functional programs: HOTGP and Origami. HOTGP uses strong types and a functional grammar, synthesizing Haskell code, with support for higher-order functions, -functions, and parametric polymorphism. Experimental results show that HOTGP is competitive with the state of the art. Additionally, Origami is an algorithm that tackles the challenge of effectively handling loops and recursion by exploring Recursion Schemes, in which the programs are composed of well-defined templates with only a few parts that need to be synthesized. The first implementation of Origami can synthesize solutions in several Recursion Schemes and data structures, being competitive with other GP methods in the literature, as well as LLMs. The latest version of Origami employs a novel procedure, called AC/DC, designed to improve the search-space exploration. It achieves considerable improvement over its previous version by raising success rates on every problem. Compared to similar methods in the literature, it has the highest count of problems solved with success rates of , , and  across all benchmarks. In  of all benchmark problems, it stands as the only method to reach  success rate, being the first known approach to achieve it on any problem in PSB2. It also demonstrates competitive performance to LLMs, achieving the highest overall win-rate against Copilot among all GP methods."
    },
    {
      "id": "2511.23083v1",
      "title": "Spectral Concentration at the Edge of Stability: Information Geometry of Kernel Associative Memory",
      "abstract": "High-capacity kernel Hopfield networks exhibit a \"Ridge of Optimization\" characterized by extreme stability. While previously linked to \"Spectral Concentration,\" its origin remains elusive. Here, we analyze the network dynamics on a statistical manifold, revealing that the Ridge corresponds to the \"Edge of Stability,\" a critical boundary where the Fisher Information Matrix becomes singular. We demonstrate that the apparent Euclidean force antagonism is a manifestation of  in the Riemannian space. This unifies learning dynamics and capacity via the Minimum Description Length principle, offering a geometric theory of self-organized criticality.",
      "authors": [
        "Akira Tamamori"
      ],
      "published_date": "2025-11-28",
      "full_text": "Spectral Concentration at the Edge of Stability: Information Geometry of Kernel Associative Memory\n\nHigh-capacity kernel Hopfield networks exhibit a \"Ridge of Optimization\" characterized by extreme stability. While previously linked to \"Spectral Concentration,\" its origin remains elusive. Here, we analyze the network dynamics on a statistical manifold, revealing that the Ridge corresponds to the \"Edge of Stability,\" a critical boundary where the Fisher Information Matrix becomes singular. We demonstrate that the apparent Euclidean force antagonism is a manifestation of  in the Riemannian space. This unifies learning dynamics and capacity via the Minimum Description Length principle, offering a geometric theory of self-organized criticality."
    },
    {
      "id": "2512.04006v1",
      "title": "Diagonalizing the Softmax: Hadamard Initialization for Tractable Cross-Entropy Dynamics",
      "abstract": "Cross-entropy (CE) training loss dominates deep learning practice, yet existing theory often relies on simplifications, either replacing it with squared loss or restricting to convex models, that miss essential behavior. CE and squared loss generate fundamentally different dynamics, and convex linear models cannot capture the complexities of non-convex optimization. We provide an in-depth characterization of multi-class CE optimization dynamics beyond the convex regime by analyzing a canonical two-layer linear neural network with standard-basis vectors as inputs: the simplest non-convex extension for which the implicit bias remained unknown. This model coincides with the unconstrained features model used to study neural collapse, making our work the first to prove that gradient flow on CE converges to the neural collapse geometry. We construct an explicit Lyapunov function that establishes global convergence, despite the presence of spurious critical points in the non-convex landscape. A key insight underlying our analysis is an inconspicuous finding: Hadamard Initialization diagonalizes the softmax operator, freezing the singular vectors of the weight matrices and reducing the dynamics entirely to their singular values. This technique opens a pathway for analyzing CE training dynamics well beyond our specific setting considered here.",
      "authors": [
        "Connall Garrod",
        "Jonathan P. Keating",
        "Christos Thrampoulidis"
      ],
      "published_date": "2025-12-03",
      "full_text": "Diagonalizing the Softmax: Hadamard Initialization for Tractable Cross-Entropy Dynamics\n\nCross-entropy (CE) training loss dominates deep learning practice, yet existing theory often relies on simplifications, either replacing it with squared loss or restricting to convex models, that miss essential behavior. CE and squared loss generate fundamentally different dynamics, and convex linear models cannot capture the complexities of non-convex optimization. We provide an in-depth characterization of multi-class CE optimization dynamics beyond the convex regime by analyzing a canonical two-layer linear neural network with standard-basis vectors as inputs: the simplest non-convex extension for which the implicit bias remained unknown. This model coincides with the unconstrained features model used to study neural collapse, making our work the first to prove that gradient flow on CE converges to the neural collapse geometry. We construct an explicit Lyapunov function that establishes global convergence, despite the presence of spurious critical points in the non-convex landscape. A key insight underlying our analysis is an inconspicuous finding: Hadamard Initialization diagonalizes the softmax operator, freezing the singular vectors of the weight matrices and reducing the dynamics entirely to their singular values. This technique opens a pathway for analyzing CE training dynamics well beyond our specific setting considered here."
    },
    {
      "id": "2512.03899v1",
      "title": "Probabilistic Foundations of Fuzzy Simplicial Sets for Nonlinear Dimensionality Reduction",
      "abstract": "Fuzzy simplicial sets have become an object of interest in dimensionality reduction and manifold learning, most prominently through their role in UMAP. However, their definition through tools from algebraic topology without a clear probabilistic interpretation detaches them from commonly used theoretical frameworks in those areas. In this work we introduce a framework that explains fuzzy simplicial sets as marginals of probability measures on simplicial sets. In particular, this perspective shows that the fuzzy weights of UMAP arise from a generative model that samples Vietoris-Rips filtrations at random scales, yielding cumulative distribution functions of pairwise distances. More generally, the framework connects fuzzy simplicial sets to probabilistic models on the face poset, clarifies the relation between Kullback-Leibler divergence and fuzzy cross-entropy in this setting, and recovers standard t-norms and t-conorms via Boolean operations on the underlying simplicial sets. We then show how new embedding methods may be derived from this framework and illustrate this on an example where we generalize UMAP using Čech filtrations with triplet sampling. In summary, this probabilistic viewpoint provides a unified probabilistic theoretical foundation for fuzzy simplicial sets, clarifies the role of UMAP within this framework, and enables the systematic derivation of new dimensionality reduction methods.",
      "authors": [
        "Janis Keck",
        "Lukas Silvester Barth",
        "Fatemeh",
        "Fahimi",
        "Parvaneh Joharinad",
        "Jürgen Jost"
      ],
      "published_date": "2025-12-03",
      "full_text": "Probabilistic Foundations of Fuzzy Simplicial Sets for Nonlinear Dimensionality Reduction\n\nFuzzy simplicial sets have become an object of interest in dimensionality reduction and manifold learning, most prominently through their role in UMAP. However, their definition through tools from algebraic topology without a clear probabilistic interpretation detaches them from commonly used theoretical frameworks in those areas. In this work we introduce a framework that explains fuzzy simplicial sets as marginals of probability measures on simplicial sets. In particular, this perspective shows that the fuzzy weights of UMAP arise from a generative model that samples Vietoris-Rips filtrations at random scales, yielding cumulative distribution functions of pairwise distances. More generally, the framework connects fuzzy simplicial sets to probabilistic models on the face poset, clarifies the relation between Kullback-Leibler divergence and fuzzy cross-entropy in this setting, and recovers standard t-norms and t-conorms via Boolean operations on the underlying simplicial sets. We then show how new embedding methods may be derived from this framework and illustrate this on an example where we generalize UMAP using Čech filtrations with triplet sampling. In summary, this probabilistic viewpoint provides a unified probabilistic theoretical foundation for fuzzy simplicial sets, clarifies the role of UMAP within this framework, and enables the systematic derivation of new dimensionality reduction methods."
    },
    {
      "id": "2512.03851v1",
      "title": "Comparison of neural network training strategies for the simulation of dynamical systems",
      "abstract": "Neural networks have become a widely adopted tool for modeling nonlinear dynamical systems from data. However, the choice of training strategy remains a key design decision, particularly for simulation tasks. This paper compares two predominant strategies: parallel and series-parallel training. The conducted empirical analysis spans five neural network architectures and two examples: a pneumatic valve test bench and an industrial robot benchmark. The study reveals that, even though series-parallel training dominates current practice, parallel training consistently yields better long-term prediction accuracy. Additionally, this work clarifies the often inconsistent terminology in the literature and relate both strategies to concepts from system identification. The findings suggest that parallel training should be considered the default training strategy for neural network-based simulation of dynamical systems.",
      "authors": [
        "Paul Strasser",
        "Andreas Pfeffer",
        "Jakob Weber",
        "Markus Gurtner",
        "Andreas Körner"
      ],
      "published_date": "2025-12-03",
      "full_text": "Comparison of neural network training strategies for the simulation of dynamical systems\n\nNeural networks have become a widely adopted tool for modeling nonlinear dynamical systems from data. However, the choice of training strategy remains a key design decision, particularly for simulation tasks. This paper compares two predominant strategies: parallel and series-parallel training. The conducted empirical analysis spans five neural network architectures and two examples: a pneumatic valve test bench and an industrial robot benchmark. The study reveals that, even though series-parallel training dominates current practice, parallel training consistently yields better long-term prediction accuracy. Additionally, this work clarifies the often inconsistent terminology in the literature and relate both strategies to concepts from system identification. The findings suggest that parallel training should be considered the default training strategy for neural network-based simulation of dynamical systems."
    },
    {
      "id": "2512.03807v1",
      "title": "Algorithms for Boolean Matrix Factorization using Integer Programming and Heuristics",
      "abstract": "Boolean matrix factorization (BMF) approximates a given binary input matrix as the product of two smaller binary factors. Unlike binary matrix factorization based on standard arithmetic, BMF employs the Boolean OR and AND operations for the matrix product, which improves interpretability and reduces the approximation error. It is also used in role mining and computer vision. In this paper, we first propose algorithms for BMF that perform alternating optimization (AO) of the factor matrices, where each subproblem is solved via integer programming (IP). We then design different approaches to further enhance AO-based algorithms by selecting an optimal subset of rank-one factors from multiple runs. To address the scalability limits of IP-based methods, we introduce new greedy and local-search heuristics. We also construct a new C++ data structure for Boolean vectors and matrices that is significantly faster than existing ones and is of independent interest, allowing our heuristics to scale to large datasets. We illustrate the performance of all our proposed methods and compare them with the state of the art on various real datasets, both with and without missing data, including applications in topic modeling and imaging.",
      "authors": [
        "Christos Kolomvakis",
        "Thomas Bobille",
        "Arnaud Vandaele",
        "Nicolas Gillis"
      ],
      "published_date": "2025-12-03",
      "full_text": "Algorithms for Boolean Matrix Factorization using Integer Programming and Heuristics\n\nBoolean matrix factorization (BMF) approximates a given binary input matrix as the product of two smaller binary factors. Unlike binary matrix factorization based on standard arithmetic, BMF employs the Boolean OR and AND operations for the matrix product, which improves interpretability and reduces the approximation error. It is also used in role mining and computer vision. In this paper, we first propose algorithms for BMF that perform alternating optimization (AO) of the factor matrices, where each subproblem is solved via integer programming (IP). We then design different approaches to further enhance AO-based algorithms by selecting an optimal subset of rank-one factors from multiple runs. To address the scalability limits of IP-based methods, we introduce new greedy and local-search heuristics. We also construct a new C++ data structure for Boolean vectors and matrices that is significantly faster than existing ones and is of independent interest, allowing our heuristics to scale to large datasets. We illustrate the performance of all our proposed methods and compare them with the state of the art on various real datasets, both with and without missing data, including applications in topic modeling and imaging."
    },
    {
      "id": "2512.03777v1",
      "title": "A comparison between initialization strategies for the infinite hidden Markov model",
      "abstract": "Infinite hidden Markov models provide a flexible framework for modelling time series with structural changes and complex dynamics, without requiring the number of latent states to be specified in advance. This flexibility is achieved through the hierarchical Dirichlet process prior, while efficient Bayesian inference is enabled by the beam sampler, which combines dynamic programming with slice sampling to truncate the infinite state space adaptively. Despite extensive methodological developments, the role of initialization in this framework has received limited attention. This study addresses this gap by systematically evaluating initialization strategies commonly used for finite hidden Markov models and assessing their suitability in the infinite setting. Results from both simulated and real datasets show that distance-based clustering initializations consistently outperform model-based and uniform alternatives, the latter being the most widely adopted in the existing literature.",
      "authors": [
        "Federico P. Cortese",
        "Luca Rossini"
      ],
      "published_date": "2025-12-03",
      "full_text": "A comparison between initialization strategies for the infinite hidden Markov model\n\nInfinite hidden Markov models provide a flexible framework for modelling time series with structural changes and complex dynamics, without requiring the number of latent states to be specified in advance. This flexibility is achieved through the hierarchical Dirichlet process prior, while efficient Bayesian inference is enabled by the beam sampler, which combines dynamic programming with slice sampling to truncate the infinite state space adaptively. Despite extensive methodological developments, the role of initialization in this framework has received limited attention. This study addresses this gap by systematically evaluating initialization strategies commonly used for finite hidden Markov models and assessing their suitability in the infinite setting. Results from both simulated and real datasets show that distance-based clustering initializations consistently outperform model-based and uniform alternatives, the latter being the most widely adopted in the existing literature."
    },
    {
      "id": "2512.03727v1",
      "title": "Colored Markov Random Fields for Probabilistic Topological Modeling",
      "abstract": "Probabilistic Graphical Models (PGMs) encode conditional dependencies among random variables using a graph -nodes for variables, links for dependencies- and factorize the joint distribution into lower-dimensional components. This makes PGMs well-suited for analyzing complex systems and supporting decision-making. Recent advances in topological signal processing highlight the importance of variables defined on topological spaces in several application domains. In such cases, the underlying topology shapes statistical relationships, limiting the expressiveness of canonical PGMs. To overcome this limitation, we introduce Colored Markov Random Fields (CMRFs), which model both conditional and marginal dependencies among Gaussian edge variables on topological spaces, with a theoretical foundation in Hodge theory. CMRFs extend classical Gaussian Markov Random Fields by including link coloring: connectivity encodes conditional independence, while color encodes marginal independence. We quantify the benefits of CMRFs through a distributed estimation case study over a physical network, comparing it with baselines with different levels of topological prior.",
      "authors": [
        "Lorenzo Marinucci",
        "Leonardo Di Nino",
        "Gabriele D'Acunto",
        "Mario Edoardo Pandolfo",
        "Paolo Di Lorenzo",
        "Sergio Barbarossa"
      ],
      "published_date": "2025-12-03",
      "full_text": "Colored Markov Random Fields for Probabilistic Topological Modeling\n\nProbabilistic Graphical Models (PGMs) encode conditional dependencies among random variables using a graph -nodes for variables, links for dependencies- and factorize the joint distribution into lower-dimensional components. This makes PGMs well-suited for analyzing complex systems and supporting decision-making. Recent advances in topological signal processing highlight the importance of variables defined on topological spaces in several application domains. In such cases, the underlying topology shapes statistical relationships, limiting the expressiveness of canonical PGMs. To overcome this limitation, we introduce Colored Markov Random Fields (CMRFs), which model both conditional and marginal dependencies among Gaussian edge variables on topological spaces, with a theoretical foundation in Hodge theory. CMRFs extend classical Gaussian Markov Random Fields by including link coloring: connectivity encodes conditional independence, while color encodes marginal independence. We quantify the benefits of CMRFs through a distributed estimation case study over a physical network, comparing it with baselines with different levels of topological prior."
    },
    {
      "id": "2512.03637v1",
      "title": "AaPE: Aliasing-aware Patch Embedding for Self-Supervised Audio Representation Learning",
      "abstract": "Transformer-based audio SSL (self-supervised learning) models often treat spectrograms as images, applying convolutional patchification with heavy temporal downsampling. This lowers the effective Nyquist frequency and introduces aliasing, while naïve low-pass filtering removes task-relevant high-frequency cues. In this study, we present Aliasing-aware Patch Embedding (AaPE), a drop-in patch stem that mitigates aliasing while preserving high-frequency information. AaPE augments standard patch tokens with features produced by a band-limited complex sinusoidal kernel using a two-sided exponential window that dynamically targets alias-prone bands. Frequency and decay parameters of the kernel are estimated from the input, enabling parallel, adaptive subband analysis whose outputs are fused with the standard patch tokens. AaPE integrates seamlessly into the masked teacher-student self-supervised learning. In addition, we combine a multi-mask strategy with a contrastive objective to enforce consistency across diverse mask patterns, stabilizing training. Pre-training on AudioSet followed by fine-tuning evaluation across diverse downstream benchmarks, which spanned categories, such as environmental sounds and other common audio domains. This approach yields state-of-the-art performance on a subset of tasks and competitive results across the remainder. Complementary linear probing evaluation mirrors this pattern, yielding clear gains on several benchmarks and strong performance elsewhere. The collective analysis of these results indicates that AaPE serves to mitigate the effects of aliasing without discarding of informative high-frequency content.",
      "authors": [
        "Kohei Yamamoto",
        "Kosuke Okusa"
      ],
      "published_date": "2025-12-03",
      "full_text": "AaPE: Aliasing-aware Patch Embedding for Self-Supervised Audio Representation Learning\n\nTransformer-based audio SSL (self-supervised learning) models often treat spectrograms as images, applying convolutional patchification with heavy temporal downsampling. This lowers the effective Nyquist frequency and introduces aliasing, while naïve low-pass filtering removes task-relevant high-frequency cues. In this study, we present Aliasing-aware Patch Embedding (AaPE), a drop-in patch stem that mitigates aliasing while preserving high-frequency information. AaPE augments standard patch tokens with features produced by a band-limited complex sinusoidal kernel using a two-sided exponential window that dynamically targets alias-prone bands. Frequency and decay parameters of the kernel are estimated from the input, enabling parallel, adaptive subband analysis whose outputs are fused with the standard patch tokens. AaPE integrates seamlessly into the masked teacher-student self-supervised learning. In addition, we combine a multi-mask strategy with a contrastive objective to enforce consistency across diverse mask patterns, stabilizing training. Pre-training on AudioSet followed by fine-tuning evaluation across diverse downstream benchmarks, which spanned categories, such as environmental sounds and other common audio domains. This approach yields state-of-the-art performance on a subset of tasks and competitive results across the remainder. Complementary linear probing evaluation mirrors this pattern, yielding clear gains on several benchmarks and strong performance elsewhere. The collective analysis of these results indicates that AaPE serves to mitigate the effects of aliasing without discarding of informative high-frequency content."
    },
    {
      "id": "2512.03537v1",
      "title": "Parameter-Efficient Augment Plugin for Class-Incremental Learning",
      "abstract": "Existing class-incremental learning (CIL) approaches based on replay or knowledge distillation are often constrained by forgetting or the stability-plasticity dilemma. Some expansion-based approaches could achieve higher accuracy. However, they always require significant parameter increases. In this paper, we propose a plugin extension paradigm termed the Deployment of extra LoRA Components (DLC) for non-pre-trained CIL scenarios.We treat the feature extractor trained through replay or distillation as a base model with rich knowledge. For each task, we use Low-Rank Adaptation (LoRA) to inject task-specific residuals into the base model's deep layers. During inference, representations with task-specific residuals are aggregated to produce classification predictions. To mitigate interference from non-target LoRA plugins, we introduce a lightweight weighting unit. This unit learns to assign importance scores to different LoRA-tuned representations. Like downloadable contents in software, our method serves as a plug-and-play enhancement that efficiently extends the base methods. Remarkably, on the large-scale ImageNet-100, with merely 4 % of the parameters of a standard ResNet-18, our DLC model achieves a significant 8 % improvement in accuracy, demonstrating exceptional efficiency. Moreover, it could surpass state-of-the-art methods under the fixed memory budget.",
      "authors": [
        "Zhiming Xu",
        "Baile Xu",
        "Jian Zhao",
        "Furao Shen",
        "Suorong Yang"
      ],
      "published_date": "2025-12-03",
      "full_text": "Parameter-Efficient Augment Plugin for Class-Incremental Learning\n\nExisting class-incremental learning (CIL) approaches based on replay or knowledge distillation are often constrained by forgetting or the stability-plasticity dilemma. Some expansion-based approaches could achieve higher accuracy. However, they always require significant parameter increases. In this paper, we propose a plugin extension paradigm termed the Deployment of extra LoRA Components (DLC) for non-pre-trained CIL scenarios.We treat the feature extractor trained through replay or distillation as a base model with rich knowledge. For each task, we use Low-Rank Adaptation (LoRA) to inject task-specific residuals into the base model's deep layers. During inference, representations with task-specific residuals are aggregated to produce classification predictions. To mitigate interference from non-target LoRA plugins, we introduce a lightweight weighting unit. This unit learns to assign importance scores to different LoRA-tuned representations. Like downloadable contents in software, our method serves as a plug-and-play enhancement that efficiently extends the base methods. Remarkably, on the large-scale ImageNet-100, with merely 4 % of the parameters of a standard ResNet-18, our DLC model achieves a significant 8 % improvement in accuracy, demonstrating exceptional efficiency. Moreover, it could surpass state-of-the-art methods under the fixed memory budget."
    },
    {
      "id": "2512.03428v1",
      "title": "GaussDetect-LiNGAM:Causal Direction Identification without Gaussianity test",
      "abstract": "We propose GaussDetect-LiNGAM, a novel approach for bivariate causal discovery that eliminates the need for explicit Gaussianity tests by leveraging a fundamental equivalence between noise Gaussianity and residual independence in the reverse regression. Under the standard LiNGAM assumptions of linearity, acyclicity, and exogeneity, we prove that the Gaussianity of the forward-model noise is equivalent to the independence between the regressor and residual in the reverse model. This theoretical insight allows us to replace fragile and sample-sensitive Gaussianity tests with robust kernel-based independence tests. Experimental results validate the equivalence and demonstrate that GaussDetect-LiNGAM maintains high consistency across diverse noise types and sample sizes, while reducing the number of tests per decision (TPD). Our method enhances both the efficiency and practical applicability of causal inference, making LiNGAM more accessible and reliable in real-world scenarios.",
      "authors": [
        "Ziyi Ding",
        "Xiao-Ping Zhang"
      ],
      "published_date": "2025-12-03",
      "full_text": "GaussDetect-LiNGAM:Causal Direction Identification without Gaussianity test\n\nWe propose GaussDetect-LiNGAM, a novel approach for bivariate causal discovery that eliminates the need for explicit Gaussianity tests by leveraging a fundamental equivalence between noise Gaussianity and residual independence in the reverse regression. Under the standard LiNGAM assumptions of linearity, acyclicity, and exogeneity, we prove that the Gaussianity of the forward-model noise is equivalent to the independence between the regressor and residual in the reverse model. This theoretical insight allows us to replace fragile and sample-sensitive Gaussianity tests with robust kernel-based independence tests. Experimental results validate the equivalence and demonstrate that GaussDetect-LiNGAM maintains high consistency across diverse noise types and sample sizes, while reducing the number of tests per decision (TPD). Our method enhances both the efficiency and practical applicability of causal inference, making LiNGAM more accessible and reliable in real-world scenarios."
    },
    {
      "id": "2512.03393v1",
      "title": "Tuning-Free Structured Sparse Recovery of Multiple Measurement Vectors using Implicit Regularization",
      "abstract": "Recovering jointly sparse signals in the multiple measurement vectors (MMV) setting is a fundamental problem in machine learning, but traditional methods like multiple measurement vectors orthogonal matching pursuit (M-OMP) and multiple measurement vectors FOCal Underdetermined System Solver (M-FOCUSS) often require careful parameter tuning or prior knowledge of the sparsity of the signal and/or noise variance. We introduce a novel tuning-free framework that leverages Implicit Regularization (IR) from overparameterization to overcome this limitation. Our approach reparameterizes the estimation matrix into factors that decouple the shared row-support from individual vector entries. We show that the optimization dynamics inherently promote the desired row-sparse structure by applying gradient descent to a standard least-squares objective on these factors. We prove that with a sufficiently small and balanced initialization, the optimization dynamics exhibit a \"momentum-like\" effect, causing the norms of rows in the true support to grow significantly faster than others. This formally guarantees that the solution trajectory converges towards an idealized row-sparse solution. Additionally, empirical results demonstrate that our approach achieves performance comparable to established methods without requiring any prior information or tuning.",
      "authors": [
        "Lakshmi Jayalal",
        "Sheetal Kalyani"
      ],
      "published_date": "2025-12-03",
      "full_text": "Tuning-Free Structured Sparse Recovery of Multiple Measurement Vectors using Implicit Regularization\n\nRecovering jointly sparse signals in the multiple measurement vectors (MMV) setting is a fundamental problem in machine learning, but traditional methods like multiple measurement vectors orthogonal matching pursuit (M-OMP) and multiple measurement vectors FOCal Underdetermined System Solver (M-FOCUSS) often require careful parameter tuning or prior knowledge of the sparsity of the signal and/or noise variance. We introduce a novel tuning-free framework that leverages Implicit Regularization (IR) from overparameterization to overcome this limitation. Our approach reparameterizes the estimation matrix into factors that decouple the shared row-support from individual vector entries. We show that the optimization dynamics inherently promote the desired row-sparse structure by applying gradient descent to a standard least-squares objective on these factors. We prove that with a sufficiently small and balanced initialization, the optimization dynamics exhibit a \"momentum-like\" effect, causing the norms of rows in the true support to grow significantly faster than others. This formally guarantees that the solution trajectory converges towards an idealized row-sparse solution. Additionally, empirical results demonstrate that our approach achieves performance comparable to established methods without requiring any prior information or tuning."
    },
    {
      "id": "2512.03354v1",
      "title": "Breaking Determinism: Stochastic Modeling for Reliable Off-Policy Evaluation in Ad Auctions",
      "abstract": "Online A/B testing, the gold standard for evaluating new advertising policies, consumes substantial engineering resources and risks significant revenue loss from deploying underperforming variations. This motivates the use of Off-Policy Evaluation (OPE) for rapid, offline assessment. However, applying OPE to ad auctions is fundamentally more challenging than in domains like recommender systems, where stochastic policies are common. In online ad auctions, it is common for the highest-bidding ad to win the impression, resulting in a deterministic, winner-takes-all setting. This results in zero probability of exposure for non-winning ads, rendering standard OPE estimators inapplicable. We introduce the first principled framework for OPE in deterministic auctions by repurposing the bid landscape model to approximate the propensity score. This model allows us to derive robust approximate propensity scores, enabling the use of stable estimators like Self-Normalized Inverse Propensity Scoring (SNIPS) for counterfactual evaluation. We validate our approach on the AuctionNet simulation benchmark and against 2-weeks online A/B test from a large-scale industrial platform. Our method shows remarkable alignment with online results, achieving a 92\\% Mean Directional Accuracy (MDA) in CTR prediction, significantly outperforming the parametric baseline. MDA is the most critical metric for guiding deployment decisions, as it reflects the ability to correctly predict whether a new model will improve or harm performance. This work contributes the first practical and validated framework for reliable OPE in deterministic auction environments, offering an efficient alternative to costly and risky online experiments.",
      "authors": [
        "Hongseon Yeom",
        "Jaeyoul Shin",
        "Soojin Min",
        "Jeongmin Yoon",
        "Seunghak Yu",
        "Dongyeop Kang"
      ],
      "published_date": "2025-12-03",
      "full_text": "Breaking Determinism: Stochastic Modeling for Reliable Off-Policy Evaluation in Ad Auctions\n\nOnline A/B testing, the gold standard for evaluating new advertising policies, consumes substantial engineering resources and risks significant revenue loss from deploying underperforming variations. This motivates the use of Off-Policy Evaluation (OPE) for rapid, offline assessment. However, applying OPE to ad auctions is fundamentally more challenging than in domains like recommender systems, where stochastic policies are common. In online ad auctions, it is common for the highest-bidding ad to win the impression, resulting in a deterministic, winner-takes-all setting. This results in zero probability of exposure for non-winning ads, rendering standard OPE estimators inapplicable. We introduce the first principled framework for OPE in deterministic auctions by repurposing the bid landscape model to approximate the propensity score. This model allows us to derive robust approximate propensity scores, enabling the use of stable estimators like Self-Normalized Inverse Propensity Scoring (SNIPS) for counterfactual evaluation. We validate our approach on the AuctionNet simulation benchmark and against 2-weeks online A/B test from a large-scale industrial platform. Our method shows remarkable alignment with online results, achieving a 92\\% Mean Directional Accuracy (MDA) in CTR prediction, significantly outperforming the parametric baseline. MDA is the most critical metric for guiding deployment decisions, as it reflects the ability to correctly predict whether a new model will improve or harm performance. This work contributes the first practical and validated framework for reliable OPE in deterministic auction environments, offering an efficient alternative to costly and risky online experiments."
    },
    {
      "id": "2512.03336v1",
      "title": "Single-Round Scalable Analytic Federated Learning",
      "abstract": "Federated Learning (FL) is plagued by two key challenges: high communication overhead and performance collapse on heterogeneous (non-IID) data. Analytic FL (AFL) provides a single-round, data distribution invariant solution, but is limited to linear models. Subsequent non-linear approaches, like DeepAFL, regain accuracy but sacrifice the single-round benefit. In this work, we break this trade-off. We propose SAFLe, a framework that achieves scalable non-linear expressivity by introducing a structured head of bucketed features and sparse, grouped embeddings. We prove this non-linear architecture is mathematically equivalent to a high-dimensional linear regression. This key equivalence allows SAFLe to be solved with AFL's single-shot, invariant aggregation law. Empirically, SAFLe establishes a new state-of-the-art for analytic FL, significantly outperforming both linear AFL and multi-round DeepAFL in accuracy across all benchmarks, demonstrating a highly efficient and scalable solution for federated vision.",
      "authors": [
        "Alan T. L. Bacellar",
        "Mustafa Munir",
        "Felipe M. G. França",
        "Priscila M. V. Lima",
        "Radu Marculescu",
        "Lizy K. John"
      ],
      "published_date": "2025-12-03",
      "full_text": "Single-Round Scalable Analytic Federated Learning\n\nFederated Learning (FL) is plagued by two key challenges: high communication overhead and performance collapse on heterogeneous (non-IID) data. Analytic FL (AFL) provides a single-round, data distribution invariant solution, but is limited to linear models. Subsequent non-linear approaches, like DeepAFL, regain accuracy but sacrifice the single-round benefit. In this work, we break this trade-off. We propose SAFLe, a framework that achieves scalable non-linear expressivity by introducing a structured head of bucketed features and sparse, grouped embeddings. We prove this non-linear architecture is mathematically equivalent to a high-dimensional linear regression. This key equivalence allows SAFLe to be solved with AFL's single-shot, invariant aggregation law. Empirically, SAFLe establishes a new state-of-the-art for analytic FL, significantly outperforming both linear AFL and multi-round DeepAFL in accuracy across all benchmarks, demonstrating a highly efficient and scalable solution for federated vision."
    },
    {
      "id": "2512.03333v1",
      "title": "Sketch Tomography: Hybridizing Classical Shadow and Matrix Product State",
      "abstract": "We introduce Sketch Tomography, an efficient procedure for quantum state tomography based on the classical shadow protocol used for quantum observable estimations. The procedure applies to the case where the ground truth quantum state is a matrix product state (MPS). The density matrix of the ground truth state admits a tensor train ansatz as a result of the MPS assumption, and we estimate the tensor components of the ansatz through a series of observable estimations, thus outputting an approximation of the density matrix. The procedure is provably convergent with a sample complexity that scales quadratically in the system size. We conduct extensive numerical experiments to show that the procedure outputs an accurate approximation to the quantum state. For observable estimation tasks involving moderately large subsystems, we show that our procedure gives rise to a more accurate estimation than the classical shadow protocol. We also show that sketch tomography is more accurate in observable estimation than quantum states trained from the maximum likelihood estimation formulation.",
      "authors": [
        "Xun Tang",
        "Haoxuan Chen",
        "Yuehaw Khoo",
        "Lexing Ying"
      ],
      "published_date": "2025-12-03",
      "full_text": "Sketch Tomography: Hybridizing Classical Shadow and Matrix Product State\n\nWe introduce Sketch Tomography, an efficient procedure for quantum state tomography based on the classical shadow protocol used for quantum observable estimations. The procedure applies to the case where the ground truth quantum state is a matrix product state (MPS). The density matrix of the ground truth state admits a tensor train ansatz as a result of the MPS assumption, and we estimate the tensor components of the ansatz through a series of observable estimations, thus outputting an approximation of the density matrix. The procedure is provably convergent with a sample complexity that scales quadratically in the system size. We conduct extensive numerical experiments to show that the procedure outputs an accurate approximation to the quantum state. For observable estimation tasks involving moderately large subsystems, we show that our procedure gives rise to a more accurate estimation than the classical shadow protocol. We also show that sketch tomography is more accurate in observable estimation than quantum states trained from the maximum likelihood estimation formulation."
    },
    {
      "id": "2512.03325v1",
      "title": "When does Gaussian equivalence fail and how to fix it: Non-universal behavior of random features with quadratic scaling",
      "abstract": "A major effort in modern high-dimensional statistics has been devoted to the analysis of linear predictors trained on nonlinear feature embeddings via empirical risk minimization (ERM). Gaussian equivalence theory (GET) has emerged as a powerful universality principle in this context: it states that the behavior of high-dimensional, complex features can be captured by Gaussian surrogates, which are more amenable to analysis. Despite its remarkable successes, numerical experiments show that this equivalence can fail even for simple embeddings -- such as polynomial maps -- under general scaling regimes. We investigate this breakdown in the setting of random feature (RF) models in the quadratic scaling regime, where both the number of features and the sample size grow quadratically with the data dimension. We show that when the target function depends on a low-dimensional projection of the data, such as generalized linear models, GET yields incorrect predictions. To capture the correct asymptotics, we introduce a Conditional Gaussian Equivalent (CGE) model, which can be viewed as appending a low-dimensional non-Gaussian component to an otherwise high-dimensional Gaussian model. This hybrid model retains the tractability of the Gaussian framework and accurately describes RF models in the quadratic scaling regime. We derive sharp asymptotics for the training and test errors in this setting, which continue to agree with numerical simulations even when GET fails. Our analysis combines general results on CLT for Wiener chaos expansions and a careful two-phase Lindeberg swapping argument. Beyond RF models and quadratic scaling, our work hints at a rich landscape of universality phenomena in high-dimensional ERM.",
      "authors": [
        "Garrett G. Wen",
        "Hong Hu",
        "Yue M. Lu",
        "Zhou Fan",
        "Theodor Misiakiewicz"
      ],
      "published_date": "2025-12-03",
      "full_text": "When does Gaussian equivalence fail and how to fix it: Non-universal behavior of random features with quadratic scaling\n\nA major effort in modern high-dimensional statistics has been devoted to the analysis of linear predictors trained on nonlinear feature embeddings via empirical risk minimization (ERM). Gaussian equivalence theory (GET) has emerged as a powerful universality principle in this context: it states that the behavior of high-dimensional, complex features can be captured by Gaussian surrogates, which are more amenable to analysis. Despite its remarkable successes, numerical experiments show that this equivalence can fail even for simple embeddings -- such as polynomial maps -- under general scaling regimes. We investigate this breakdown in the setting of random feature (RF) models in the quadratic scaling regime, where both the number of features and the sample size grow quadratically with the data dimension. We show that when the target function depends on a low-dimensional projection of the data, such as generalized linear models, GET yields incorrect predictions. To capture the correct asymptotics, we introduce a Conditional Gaussian Equivalent (CGE) model, which can be viewed as appending a low-dimensional non-Gaussian component to an otherwise high-dimensional Gaussian model. This hybrid model retains the tractability of the Gaussian framework and accurately describes RF models in the quadratic scaling regime. We derive sharp asymptotics for the training and test errors in this setting, which continue to agree with numerical simulations even when GET fails. Our analysis combines general results on CLT for Wiener chaos expansions and a careful two-phase Lindeberg swapping argument. Beyond RF models and quadratic scaling, our work hints at a rich landscape of universality phenomena in high-dimensional ERM."
    },
    {
      "id": "2512.03322v1",
      "title": "SSLfmm: An R Package for Semi-Supervised Learning with a Mixed-Missingness Mechanism in Finite Mixture Models",
      "abstract": "Semi-supervised learning (SSL) constructs classifiers from datasets in which only a subset of observations is labelled, a situation that naturally arises because obtaining labels often requires expert judgement or costly manual effort. This motivates methods that integrate labelled and unlabelled data within a learning framework. Most SSL approaches assume that label absence is harmless, typically treated as missing completely at random or ignored, but in practice, the missingness process can be informative, as the chances of an observation being unlabelled may depend on the ambiguity of its feature vector. In such cases, the missingness indicators themselves provide additional information that, if properly modelled, may improve estimation efficiency. The  package for R is designed to capture this behaviour by estimating the Bayes' classifier under a finite mixture model in which each component corresponding to a class follows a multivariate normal distribution. It incorporates a mixed-missingness mechanism that combines a missing completely at random (MCR) component with a (non-ignorable) missing at random (MAR) component, the latter modelling the probability of label missingness as a logistic function of the entropy based on the features. Parameters are estimated via an Expectation--Conditional Maximisation algorithm. In the two-class Gaussian setting with arbitrary covariance matrices, the resulting classifier trained on partially labelled data may, in some cases, achieve a lower misclassification rate than the supervised version in the case where all the labels are known. The package includes a practical tool for modelling and illustrates its performance through simulated examples.",
      "authors": [
        "Geoffrey J. McLachlan",
        "Jinran Wu"
      ],
      "published_date": "2025-12-03",
      "full_text": "SSLfmm: An R Package for Semi-Supervised Learning with a Mixed-Missingness Mechanism in Finite Mixture Models\n\nSemi-supervised learning (SSL) constructs classifiers from datasets in which only a subset of observations is labelled, a situation that naturally arises because obtaining labels often requires expert judgement or costly manual effort. This motivates methods that integrate labelled and unlabelled data within a learning framework. Most SSL approaches assume that label absence is harmless, typically treated as missing completely at random or ignored, but in practice, the missingness process can be informative, as the chances of an observation being unlabelled may depend on the ambiguity of its feature vector. In such cases, the missingness indicators themselves provide additional information that, if properly modelled, may improve estimation efficiency. The  package for R is designed to capture this behaviour by estimating the Bayes' classifier under a finite mixture model in which each component corresponding to a class follows a multivariate normal distribution. It incorporates a mixed-missingness mechanism that combines a missing completely at random (MCR) component with a (non-ignorable) missing at random (MAR) component, the latter modelling the probability of label missingness as a logistic function of the entropy based on the features. Parameters are estimated via an Expectation--Conditional Maximisation algorithm. In the two-class Gaussian setting with arbitrary covariance matrices, the resulting classifier trained on partially labelled data may, in some cases, achieve a lower misclassification rate than the supervised version in the case where all the labels are known. The package includes a practical tool for modelling and illustrates its performance through simulated examples."
    },
    {
      "id": "2512.03243v1",
      "title": "Novelty detection on path space",
      "abstract": "We frame novelty detection on path space as a hypothesis testing problem with signature-based test statistics. Using transportation-cost inequalities of Gasteratos and Jacquier (2023), we obtain tail bounds for false positive rates that extend beyond Gaussian measures to laws of RDE solutions with smooth bounded vector fields, yielding estimates of quantiles and p-values. Exploiting the shuffle product, we derive exact formulae for smooth surrogates of conditional value-at-risk (CVaR) in terms of expected signatures, leading to new one-class SVM algorithms optimising smooth CVaR objectives. We then establish lower bounds on type- error for alternatives with finite first moment, giving general power bounds when the reference measure and the alternative are absolutely continuous with respect to each other. Finally, we evaluate numerically the type- error and statistical power of signature-based test statistic, using synthetic anomalous diffusion data and real-world molecular biology data.",
      "authors": [
        "Ioannis Gasteratos",
        "Antoine Jacquier",
        "Maud Lemercier",
        "Terry Lyons",
        "Cristopher Salvi"
      ],
      "published_date": "2025-12-02",
      "full_text": "Novelty detection on path space\n\nWe frame novelty detection on path space as a hypothesis testing problem with signature-based test statistics. Using transportation-cost inequalities of Gasteratos and Jacquier (2023), we obtain tail bounds for false positive rates that extend beyond Gaussian measures to laws of RDE solutions with smooth bounded vector fields, yielding estimates of quantiles and p-values. Exploiting the shuffle product, we derive exact formulae for smooth surrogates of conditional value-at-risk (CVaR) in terms of expected signatures, leading to new one-class SVM algorithms optimising smooth CVaR objectives. We then establish lower bounds on type- error for alternatives with finite first moment, giving general power bounds when the reference measure and the alternative are absolutely continuous with respect to each other. Finally, we evaluate numerically the type- error and statistical power of signature-based test statistic, using synthetic anomalous diffusion data and real-world molecular biology data."
    },
    {
      "id": "2512.03238v1",
      "title": "How to DP-fy Your Data: A Practical Guide to Generating Synthetic Data With Differential Privacy",
      "abstract": "High quality data is needed to unlock the full potential of AI for end users. However finding new sources of such data is getting harder: most publicly-available human generated data will soon have been used. Additionally, publicly available data often is not representative of users of a particular system -- for example, a research speech dataset of contractors interacting with an AI assistant will likely be more homogeneous, well articulated and self-censored than real world commands that end users will issue. Therefore unlocking high-quality data grounded in real user interactions is of vital interest. However, the direct use of user data comes with significant privacy risks. Differential Privacy (DP) is a well established framework for reasoning about and limiting information leakage, and is a gold standard for protecting user privacy. The focus of this work, , refers to synthetic data that preserves the overall trends of source data,, while providing strong privacy guarantees to individuals that contributed to the source dataset. DP synthetic data can unlock the value of datasets that have previously been inaccessible due to privacy concerns and can replace the use of sensitive datasets that previously have only had rudimentary protections like ad-hoc rule-based anonymization. In this paper we explore the full suite of techniques surrounding DP synthetic data, the types of privacy protections they offer and the state-of-the-art for various modalities (image, tabular, text and decentralized). We outline all the components needed in a system that generates DP synthetic data, from sensitive data handling and preparation, to tracking the use and empirical privacy testing. We hope that work will result in increased adoption of DP synthetic data, spur additional research and increase trust in DP synthetic data approaches.",
      "authors": [
        "Natalia Ponomareva",
        "Zheng Xu",
        "H. Brendan McMahan",
        "Peter Kairouz",
        "Lucas Rosenblatt",
        "Vincent Cohen-Addad",
        "Cristóbal Guzmán",
        "Ryan McKenna",
        "Galen Andrew",
        "Alex Bie",
        "Da Yu",
        "Alex Kurakin",
        "Morteza Zadimoghaddam",
        "Sergei Vassilvitskii",
        "Andreas Terzis"
      ],
      "published_date": "2025-12-02",
      "full_text": "How to DP-fy Your Data: A Practical Guide to Generating Synthetic Data With Differential Privacy\n\nHigh quality data is needed to unlock the full potential of AI for end users. However finding new sources of such data is getting harder: most publicly-available human generated data will soon have been used. Additionally, publicly available data often is not representative of users of a particular system -- for example, a research speech dataset of contractors interacting with an AI assistant will likely be more homogeneous, well articulated and self-censored than real world commands that end users will issue. Therefore unlocking high-quality data grounded in real user interactions is of vital interest. However, the direct use of user data comes with significant privacy risks. Differential Privacy (DP) is a well established framework for reasoning about and limiting information leakage, and is a gold standard for protecting user privacy. The focus of this work, , refers to synthetic data that preserves the overall trends of source data,, while providing strong privacy guarantees to individuals that contributed to the source dataset. DP synthetic data can unlock the value of datasets that have previously been inaccessible due to privacy concerns and can replace the use of sensitive datasets that previously have only had rudimentary protections like ad-hoc rule-based anonymization. In this paper we explore the full suite of techniques surrounding DP synthetic data, the types of privacy protections they offer and the state-of-the-art for various modalities (image, tabular, text and decentralized). We outline all the components needed in a system that generates DP synthetic data, from sensitive data handling and preparation, to tracking the use and empirical privacy testing. We hope that work will result in increased adoption of DP synthetic data, spur additional research and increase trust in DP synthetic data approaches."
    },
    {
      "id": "2512.03234v1",
      "title": "Iterative Tilting for Diffusion Fine-Tuning",
      "abstract": "We introduce iterative tilting, a gradient-free method for fine-tuning diffusion models toward reward-tilted distributions. The method decomposes a large reward tilt  into  sequential smaller tilts, each admitting a tractable score update via first-order Taylor expansion. This requires only forward evaluations of the reward function and avoids backpropagating through sampling chains. We validate on a two-dimensional Gaussian mixture with linear reward, where the exact tilted distribution is available in closed form.",
      "authors": [
        "Jean Pachebat",
        "Giovanni Conforti",
        "Alain Durmus",
        "Yazid Janati"
      ],
      "published_date": "2025-12-02",
      "full_text": "Iterative Tilting for Diffusion Fine-Tuning\n\nWe introduce iterative tilting, a gradient-free method for fine-tuning diffusion models toward reward-tilted distributions. The method decomposes a large reward tilt  into  sequential smaller tilts, each admitting a tractable score update via first-order Taylor expansion. This requires only forward evaluations of the reward function and avoids backpropagating through sampling chains. We validate on a two-dimensional Gaussian mixture with linear reward, where the exact tilted distribution is available in closed form."
    },
    {
      "id": "2512.03225v1",
      "title": "Convergence of a class of gradient-free optimisation schemes when the objective function is noisy, irregular, or both",
      "abstract": "We investigate the convergence properties of a class of iterative algorithms designed to minimize a potentially non-smooth and noisy objective function, which may be algebraically intractable and whose values may be obtained as the output of a black box. The algorithms considered can be cast under the umbrella of a generalised gradient descent recursion, where the gradient is that of a smooth approximation of the objective function. The framework we develop includes as special cases model-based and mollification methods, two classical approaches to zero-th order optimisation. The convergence results are obtained under very weak assumptions on the regularity of the objective function and involve a trade-off between the degree of smoothing and size of the steps taken in the parameter updates. As expected, additional assumptions are required in the stochastic case. We illustrate the relevance of these algorithms and our convergence results through a challenging classification example from machine learning.",
      "authors": [
        "Christophe Andrieu",
        "Nicolas Chopin",
        "Ettore Fincato",
        "Mathieu Gerber"
      ],
      "published_date": "2025-12-02",
      "full_text": "Convergence of a class of gradient-free optimisation schemes when the objective function is noisy, irregular, or both\n\nWe investigate the convergence properties of a class of iterative algorithms designed to minimize a potentially non-smooth and noisy objective function, which may be algebraically intractable and whose values may be obtained as the output of a black box. The algorithms considered can be cast under the umbrella of a generalised gradient descent recursion, where the gradient is that of a smooth approximation of the objective function. The framework we develop includes as special cases model-based and mollification methods, two classical approaches to zero-th order optimisation. The convergence results are obtained under very weak assumptions on the regularity of the objective function and involve a trade-off between the degree of smoothing and size of the steps taken in the parameter updates. As expected, additional assumptions are required in the stochastic case. We illustrate the relevance of these algorithms and our convergence results through a challenging classification example from machine learning."
    },
    {
      "id": "2512.03208v1",
      "title": "Uncertainty Quantification for Large Language Model Reward Learning under Heterogeneous Human Feedback",
      "abstract": "We study estimation and statistical inference for reward models used in aligning large language models (LLMs). A key component of LLM alignment is reinforcement learning from human feedback (RLHF), where humans compare pairs of model-generated answers and their preferences are used to train a reward model. However, human feedback is inherently heterogeneous, creating significant challenges for reliable reward learning. To address this, we adopt a heterogeneous preference framework that jointly models the latent reward of answers and human rationality. This leads to a challenging biconvex optimization problem, which we solve via an alternating gradient descent algorithm. We establish theoretical guarantees for the resulting estimator, including its convergence and asymptotic distribution. These results enable the construction of confidence intervals for reward estimates. Leveraging these uncertainty quantification results, we conduct valid statistical comparisons between rewards and incorporate uncertainty into the best-of- (BoN) policy framework. Extensive simulations demonstrate the effectiveness of our method, and applications to real LLM data highlight the practical value of accounting for uncertainty in reward modeling for LLM alignment.",
      "authors": [
        "Pangpang Liu",
        "Junwei Lu",
        "Will Wei Sun"
      ],
      "published_date": "2025-12-02",
      "full_text": "Uncertainty Quantification for Large Language Model Reward Learning under Heterogeneous Human Feedback\n\nWe study estimation and statistical inference for reward models used in aligning large language models (LLMs). A key component of LLM alignment is reinforcement learning from human feedback (RLHF), where humans compare pairs of model-generated answers and their preferences are used to train a reward model. However, human feedback is inherently heterogeneous, creating significant challenges for reliable reward learning. To address this, we adopt a heterogeneous preference framework that jointly models the latent reward of answers and human rationality. This leads to a challenging biconvex optimization problem, which we solve via an alternating gradient descent algorithm. We establish theoretical guarantees for the resulting estimator, including its convergence and asymptotic distribution. These results enable the construction of confidence intervals for reward estimates. Leveraging these uncertainty quantification results, we conduct valid statistical comparisons between rewards and incorporate uncertainty into the best-of- (BoN) policy framework. Extensive simulations demonstrate the effectiveness of our method, and applications to real LLM data highlight the practical value of accounting for uncertainty in reward modeling for LLM alignment."
    },
    {
      "id": "2512.02970v1",
      "title": "Identification of Multivariate Measurement Error Models",
      "abstract": "This paper develops new identification results for multidimensional continuous measurement-error models where all observed measurements are contaminated by potentially correlated errors and none provides an injective mapping of the latent distribution. Using third order cross moments, the paper constructs a three way tensor whose unique decomposition, guaranteed by Kruskal theorem, identifies the factor loading matrices. Starting with a linear structure, the paper recovers the full distribution of latent factors by constructing suitable measurements and applying scalar or multivariate versions of Kotlarski identity. As a result, the joint distribution of the latent vector and measurement errors is fully identified without requiring injective measurements, showing that multivariate latent structure can be recovered in broader settings than previously believed. Under injectivity, the paper also provides user-friendly testable conditions for identification. Finally, this paper provides general identification results for nonlinear models using a newly-defined generalized Kruskal rank - signal rank - of intergral operators. These results have wide applicability in empirical work involving noisy or indirect measurements, including factor models, survey data with reporting errors, mismeasured regressors in econometrics, and multidimensional latent-trait models in psychology and marketing, potentially enabling more robust estimation and interpretation when clean measurements are unavailable.",
      "authors": [
        "Yingyao Hu"
      ],
      "published_date": "2025-12-02",
      "full_text": "Identification of Multivariate Measurement Error Models\n\nThis paper develops new identification results for multidimensional continuous measurement-error models where all observed measurements are contaminated by potentially correlated errors and none provides an injective mapping of the latent distribution. Using third order cross moments, the paper constructs a three way tensor whose unique decomposition, guaranteed by Kruskal theorem, identifies the factor loading matrices. Starting with a linear structure, the paper recovers the full distribution of latent factors by constructing suitable measurements and applying scalar or multivariate versions of Kotlarski identity. As a result, the joint distribution of the latent vector and measurement errors is fully identified without requiring injective measurements, showing that multivariate latent structure can be recovered in broader settings than previously believed. Under injectivity, the paper also provides user-friendly testable conditions for identification. Finally, this paper provides general identification results for nonlinear models using a newly-defined generalized Kruskal rank - signal rank - of intergral operators. These results have wide applicability in empirical work involving noisy or indirect measurements, including factor models, survey data with reporting errors, mismeasured regressors in econometrics, and multidimensional latent-trait models in psychology and marketing, potentially enabling more robust estimation and interpretation when clean measurements are unavailable."
    },
    {
      "id": "2512.02925v1",
      "title": "Fast Gaussian Process Approximations for Autocorrelated Data",
      "abstract": "This paper is concerned with the problem of how to speed up computation for Gaussian process models trained on autocorrelated data. The Gaussian process model is a powerful tool commonly used in nonlinear regression applications. Standard regression modeling assumes random samples and an independently, identically distributed noise. Various fast approximations that speed up Gaussian process regression work under this standard setting. But for autocorrelated data, failing to account for autocorrelation leads to a phenomenon known as temporal overfitting that deteriorates model performance on new test instances. To handle autocorrelated data, existing fast Gaussian process approximations have to be modified; one such approach is to segment the originally correlated data points into blocks in which the blocked data are de-correlated. This work explains how to make some of the existing Gaussian process approximations work with blocked data. Numerical experiments across diverse application datasets demonstrate that the proposed approaches can remarkably accelerate computation for Gaussian process regression on autocorrelated data without compromising model prediction performance.",
      "authors": [
        "Ahmadreza Chokhachian",
        "Matthias Katzfuss",
        "Yu Ding"
      ],
      "published_date": "2025-12-02",
      "full_text": "Fast Gaussian Process Approximations for Autocorrelated Data\n\nThis paper is concerned with the problem of how to speed up computation for Gaussian process models trained on autocorrelated data. The Gaussian process model is a powerful tool commonly used in nonlinear regression applications. Standard regression modeling assumes random samples and an independently, identically distributed noise. Various fast approximations that speed up Gaussian process regression work under this standard setting. But for autocorrelated data, failing to account for autocorrelation leads to a phenomenon known as temporal overfitting that deteriorates model performance on new test instances. To handle autocorrelated data, existing fast Gaussian process approximations have to be modified; one such approach is to segment the originally correlated data points into blocks in which the blocked data are de-correlated. This work explains how to make some of the existing Gaussian process approximations work with blocked data. Numerical experiments across diverse application datasets demonstrate that the proposed approaches can remarkably accelerate computation for Gaussian process regression on autocorrelated data without compromising model prediction performance."
    },
    {
      "id": "2512.02912v1",
      "title": "Hypothesis Testing for Generalized Thurstone Models",
      "abstract": "In this work, we develop a hypothesis testing framework to determine whether pairwise comparison data is generated by an underlying   for a given choice function . While prior work has predominantly focused on parameter estimation and uncertainty quantification for such models, we address the fundamental problem of minimax hypothesis testing for  models. We formulate this testing problem by introducing a notion of separation distance between general pairwise comparison models and the class of  models. We then derive upper and lower bounds on the critical threshold for testing that depend on the topology of the observation graph. For the special case of complete observation graphs, this threshold scales as , where  is the number of agents and  is the number of comparisons per pair. Furthermore, we propose a hypothesis test based on our separation distance, construct confidence intervals, establish time-uniform bounds on the probabilities of type I and II errors using reverse martingale techniques, and derive minimax lower bounds using information-theoretic methods. Finally, we validate our results through experiments on synthetic and real-world datasets.",
      "authors": [
        "Anuran Makur",
        "Japneet Singh"
      ],
      "published_date": "2025-12-02",
      "full_text": "Hypothesis Testing for Generalized Thurstone Models\n\nIn this work, we develop a hypothesis testing framework to determine whether pairwise comparison data is generated by an underlying   for a given choice function . While prior work has predominantly focused on parameter estimation and uncertainty quantification for such models, we address the fundamental problem of minimax hypothesis testing for  models. We formulate this testing problem by introducing a notion of separation distance between general pairwise comparison models and the class of  models. We then derive upper and lower bounds on the critical threshold for testing that depend on the topology of the observation graph. For the special case of complete observation graphs, this threshold scales as , where  is the number of agents and  is the number of comparisons per pair. Furthermore, we propose a hypothesis test based on our separation distance, construct confidence intervals, establish time-uniform bounds on the probabilities of type I and II errors using reverse martingale techniques, and derive minimax lower bounds using information-theoretic methods. Finally, we validate our results through experiments on synthetic and real-world datasets."
    },
    {
      "id": "2512.02866v1",
      "title": "HeteroJIVE: Joint Subspace Estimation for Heterogeneous Multi-View Data",
      "abstract": "Many modern datasets consist of multiple related matrices measured on a common set of units, where the goal is to recover the shared low-dimensional subspace. While the Angle-based Joint and Individual Variation Explained (AJIVE) framework provides a solution, it relies on equal-weight aggregation, which can be strictly suboptimal when views exhibit significant statistical heterogeneity (arising from varying SNR and dimensions) and structural heterogeneity (arising from individual components). In this paper, we propose HeteroJIVE, a weighted two-stage spectral algorithm tailored to such heterogeneity. Theoretically, we first revisit the ``non-diminishing\" error barrier with respect to the number of views  identified in recent literature for the equal-weight case. We demonstrate that this barrier is not universal: under generic geometric conditions, the bias term vanishes and our estimator achieves the  rate without the need for iterative refinement. Extending this to the general-weight case, we establish error bounds that explicitly disentangle the two layers of heterogeneity. Based on this, we derive an oracle-optimal weighting scheme implemented via a data-driven procedure. Extensive simulations corroborate our theoretical findings, and an application to TCGA-BRCA multi-omics data validates the superiority of HeteroJIVE in practice.",
      "authors": [
        "Jingyang Li",
        "Zhongyuan Lyu"
      ],
      "published_date": "2025-12-02",
      "full_text": "HeteroJIVE: Joint Subspace Estimation for Heterogeneous Multi-View Data\n\nMany modern datasets consist of multiple related matrices measured on a common set of units, where the goal is to recover the shared low-dimensional subspace. While the Angle-based Joint and Individual Variation Explained (AJIVE) framework provides a solution, it relies on equal-weight aggregation, which can be strictly suboptimal when views exhibit significant statistical heterogeneity (arising from varying SNR and dimensions) and structural heterogeneity (arising from individual components). In this paper, we propose HeteroJIVE, a weighted two-stage spectral algorithm tailored to such heterogeneity. Theoretically, we first revisit the ``non-diminishing\" error barrier with respect to the number of views  identified in recent literature for the equal-weight case. We demonstrate that this barrier is not universal: under generic geometric conditions, the bias term vanishes and our estimator achieves the  rate without the need for iterative refinement. Extending this to the general-weight case, we establish error bounds that explicitly disentangle the two layers of heterogeneity. Based on this, we derive an oracle-optimal weighting scheme implemented via a data-driven procedure. Extensive simulations corroborate our theoretical findings, and an application to TCGA-BRCA multi-omics data validates the superiority of HeteroJIVE in practice."
    },
    {
      "id": "2512.02831v1",
      "title": "Revisiting Theory of Contrastive Learning for Domain Generalization",
      "abstract": "Contrastive learning is among the most popular and powerful approaches for self-supervised representation learning, where the goal is to map semantically similar samples close together while separating dissimilar ones in the latent space. Existing theoretical methods assume that downstream task classes are drawn from the same latent class distribution used during the pretraining phase. However, in real-world settings, downstream tasks may not only exhibit distributional shifts within the same label space but also introduce new or broader label spaces, leading to domain generalization challenges. In this work, we introduce novel generalization bounds that explicitly account for both types of mismatch: domain shift and domain generalization. Specifically, we analyze scenarios where downstream tasks either (i) draw classes from the same latent class space but with shifted distributions, or (ii) involve new label spaces beyond those seen during pretraining. Our analysis reveals how the performance of contrastively learned representations depends on the statistical discrepancy between pretraining and downstream distributions. This extended perspective allows us to derive provable guarantees on the performance of learned representations on average classification tasks involving class distributions outside the pretraining latent class set.",
      "authors": [
        "Ali Alvandi",
        "Mina Rezaei"
      ],
      "published_date": "2025-12-02",
      "full_text": "Revisiting Theory of Contrastive Learning for Domain Generalization\n\nContrastive learning is among the most popular and powerful approaches for self-supervised representation learning, where the goal is to map semantically similar samples close together while separating dissimilar ones in the latent space. Existing theoretical methods assume that downstream task classes are drawn from the same latent class distribution used during the pretraining phase. However, in real-world settings, downstream tasks may not only exhibit distributional shifts within the same label space but also introduce new or broader label spaces, leading to domain generalization challenges. In this work, we introduce novel generalization bounds that explicitly account for both types of mismatch: domain shift and domain generalization. Specifically, we analyze scenarios where downstream tasks either (i) draw classes from the same latent class space but with shifted distributions, or (ii) involve new label spaces beyond those seen during pretraining. Our analysis reveals how the performance of contrastively learned representations depends on the statistical discrepancy between pretraining and downstream distributions. This extended perspective allows us to derive provable guarantees on the performance of learned representations on average classification tasks involving class distributions outside the pretraining latent class set."
    },
    {
      "id": "2512.04059v1",
      "title": "Inference for location and height of peaks of a standardized field after selection",
      "abstract": "Peak inference concerns the use of local maxima (\"peaks\") of a noisy random field to detect and localize regions where underlying signal is present. We propose a peak inference method that first subjects observed peaks to a significance test of the null hypothesis that no signal is present, and then uses the peaks that are declared significant to construct post-selectively valid confidence regions for the location and height of nearby true peaks. We analyze the performance of this method in a smooth signal plus constant variance noise model under a high-curvature asymptotic assumption, and prove that it asymptotically controls both the number of false discoveries, and the number of confidence regions that do not contain a true peak, relative to the number of points at which inference is conducted. An important intermediate theoretical result uses the Kac-Rice formula to derive a novel approximation to the intensity function of a point process that counts local maxima, which is second-order accurate under the alternative, nearby high-curvature true peaks.",
      "authors": [
        "Alden Green",
        "Jonathan Taylor"
      ],
      "published_date": "2025-12-03",
      "full_text": "Inference for location and height of peaks of a standardized field after selection\n\nPeak inference concerns the use of local maxima (\"peaks\") of a noisy random field to detect and localize regions where underlying signal is present. We propose a peak inference method that first subjects observed peaks to a significance test of the null hypothesis that no signal is present, and then uses the peaks that are declared significant to construct post-selectively valid confidence regions for the location and height of nearby true peaks. We analyze the performance of this method in a smooth signal plus constant variance noise model under a high-curvature asymptotic assumption, and prove that it asymptotically controls both the number of false discoveries, and the number of confidence regions that do not contain a true peak, relative to the number of points at which inference is conducted. An important intermediate theoretical result uses the Kac-Rice formula to derive a novel approximation to the intensity function of a point process that counts local maxima, which is second-order accurate under the alternative, nearby high-curvature true peaks."
    },
    {
      "id": "2512.04058v1",
      "title": "Closing the problem of which causal structures of up to six total nodes have a classical-quantum gap",
      "abstract": "The discovery of Bell that there exist quantum correlations that cannot be reproduced classically is one of the most important in the foundations of quantum mechanics, as well as having practical implications. Bell's result was originally proven in a simple bipartite causal structure, but analogous results have also been shown in further causal structures. Here we study the only causal structure with six or fewer nodes in which the question of whether or not there exist quantum correlations that cannot be achieved classically was open. In this causal structure we show that such quantum correlations exist using a method that involves imposing additional restrictions on the correlations. This hence completes the picture of which causal structures of up to six nodes support non-classical quantum correlations. We also provide further illustrations of our method using other causal structures.",
      "authors": [
        "Shashaank Khanna",
        "Matthew Pusey",
        "Roger Colbeck"
      ],
      "published_date": "2025-12-03",
      "full_text": "Closing the problem of which causal structures of up to six total nodes have a classical-quantum gap\n\nThe discovery of Bell that there exist quantum correlations that cannot be reproduced classically is one of the most important in the foundations of quantum mechanics, as well as having practical implications. Bell's result was originally proven in a simple bipartite causal structure, but analogous results have also been shown in further causal structures. Here we study the only causal structure with six or fewer nodes in which the question of whether or not there exist quantum correlations that cannot be achieved classically was open. In this causal structure we show that such quantum correlations exist using a method that involves imposing additional restrictions on the correlations. This hence completes the picture of which causal structures of up to six nodes support non-classical quantum correlations. We also provide further illustrations of our method using other causal structures."
    },
    {
      "id": "2512.03579v1",
      "title": "Optimal Transportation and Alignment Between Gaussian Measures",
      "abstract": "Optimal transport (OT) and Gromov-Wasserstein (GW) alignment provide interpretable geometric frameworks for comparing, transforming, and aggregating heterogeneous datasets -- tasks ubiquitous in data science and machine learning. Because these frameworks are computationally expensive, large-scale applications often rely on closed-form solutions for Gaussian distributions under quadratic cost. This work provides a comprehensive treatment of Gaussian, quadratic cost OT and inner product GW (IGW) alignment, closing several gaps in the literature to broaden applicability. First, we treat the open problem of IGW alignment between uncentered Gaussians on separable Hilbert spaces by giving a closed-form expression up to a quadratic optimization over unitary operators, for which we derive tight analytic upper and lower bounds. If at least one Gaussian measure is centered, the solution reduces to a fully closed-form expression, which we further extend to an analytic solution for the IGW barycenter between centered Gaussians. We also present a reduction of Gaussian multimarginal OT with pairwise quadratic costs to a tractable optimization problem and provide an efficient algorithm to solve it using a rank-deficiency constraint. To demonstrate utility, we apply our results to knowledge distillation and heterogeneous clustering on synthetic and real-world datasets.",
      "authors": [
        "Sanjit Dandapanthula",
        "Aleksandr Podkopaev",
        "Shiva Prasad Kasiviswanathan",
        "Aaditya Ramdas",
        "Ziv Goldfeld"
      ],
      "published_date": "2025-12-03",
      "full_text": "Optimal Transportation and Alignment Between Gaussian Measures\n\nOptimal transport (OT) and Gromov-Wasserstein (GW) alignment provide interpretable geometric frameworks for comparing, transforming, and aggregating heterogeneous datasets -- tasks ubiquitous in data science and machine learning. Because these frameworks are computationally expensive, large-scale applications often rely on closed-form solutions for Gaussian distributions under quadratic cost. This work provides a comprehensive treatment of Gaussian, quadratic cost OT and inner product GW (IGW) alignment, closing several gaps in the literature to broaden applicability. First, we treat the open problem of IGW alignment between uncentered Gaussians on separable Hilbert spaces by giving a closed-form expression up to a quadratic optimization over unitary operators, for which we derive tight analytic upper and lower bounds. If at least one Gaussian measure is centered, the solution reduces to a fully closed-form expression, which we further extend to an analytic solution for the IGW barycenter between centered Gaussians. We also present a reduction of Gaussian multimarginal OT with pairwise quadratic costs to a tractable optimization problem and provide an efficient algorithm to solve it using a rank-deficiency constraint. To demonstrate utility, we apply our results to knowledge distillation and heterogeneous clustering on synthetic and real-world datasets."
    },
    {
      "id": "2512.03333v1",
      "title": "Sketch Tomography: Hybridizing Classical Shadow and Matrix Product State",
      "abstract": "We introduce Sketch Tomography, an efficient procedure for quantum state tomography based on the classical shadow protocol used for quantum observable estimations. The procedure applies to the case where the ground truth quantum state is a matrix product state (MPS). The density matrix of the ground truth state admits a tensor train ansatz as a result of the MPS assumption, and we estimate the tensor components of the ansatz through a series of observable estimations, thus outputting an approximation of the density matrix. The procedure is provably convergent with a sample complexity that scales quadratically in the system size. We conduct extensive numerical experiments to show that the procedure outputs an accurate approximation to the quantum state. For observable estimation tasks involving moderately large subsystems, we show that our procedure gives rise to a more accurate estimation than the classical shadow protocol. We also show that sketch tomography is more accurate in observable estimation than quantum states trained from the maximum likelihood estimation formulation.",
      "authors": [
        "Xun Tang",
        "Haoxuan Chen",
        "Yuehaw Khoo",
        "Lexing Ying"
      ],
      "published_date": "2025-12-03",
      "full_text": "Sketch Tomography: Hybridizing Classical Shadow and Matrix Product State\n\nWe introduce Sketch Tomography, an efficient procedure for quantum state tomography based on the classical shadow protocol used for quantum observable estimations. The procedure applies to the case where the ground truth quantum state is a matrix product state (MPS). The density matrix of the ground truth state admits a tensor train ansatz as a result of the MPS assumption, and we estimate the tensor components of the ansatz through a series of observable estimations, thus outputting an approximation of the density matrix. The procedure is provably convergent with a sample complexity that scales quadratically in the system size. We conduct extensive numerical experiments to show that the procedure outputs an accurate approximation to the quantum state. For observable estimation tasks involving moderately large subsystems, we show that our procedure gives rise to a more accurate estimation than the classical shadow protocol. We also show that sketch tomography is more accurate in observable estimation than quantum states trained from the maximum likelihood estimation formulation."
    },
    {
      "id": "2512.03325v1",
      "title": "When does Gaussian equivalence fail and how to fix it: Non-universal behavior of random features with quadratic scaling",
      "abstract": "A major effort in modern high-dimensional statistics has been devoted to the analysis of linear predictors trained on nonlinear feature embeddings via empirical risk minimization (ERM). Gaussian equivalence theory (GET) has emerged as a powerful universality principle in this context: it states that the behavior of high-dimensional, complex features can be captured by Gaussian surrogates, which are more amenable to analysis. Despite its remarkable successes, numerical experiments show that this equivalence can fail even for simple embeddings -- such as polynomial maps -- under general scaling regimes. We investigate this breakdown in the setting of random feature (RF) models in the quadratic scaling regime, where both the number of features and the sample size grow quadratically with the data dimension. We show that when the target function depends on a low-dimensional projection of the data, such as generalized linear models, GET yields incorrect predictions. To capture the correct asymptotics, we introduce a Conditional Gaussian Equivalent (CGE) model, which can be viewed as appending a low-dimensional non-Gaussian component to an otherwise high-dimensional Gaussian model. This hybrid model retains the tractability of the Gaussian framework and accurately describes RF models in the quadratic scaling regime. We derive sharp asymptotics for the training and test errors in this setting, which continue to agree with numerical simulations even when GET fails. Our analysis combines general results on CLT for Wiener chaos expansions and a careful two-phase Lindeberg swapping argument. Beyond RF models and quadratic scaling, our work hints at a rich landscape of universality phenomena in high-dimensional ERM.",
      "authors": [
        "Garrett G. Wen",
        "Hong Hu",
        "Yue M. Lu",
        "Zhou Fan",
        "Theodor Misiakiewicz"
      ],
      "published_date": "2025-12-03",
      "full_text": "When does Gaussian equivalence fail and how to fix it: Non-universal behavior of random features with quadratic scaling\n\nA major effort in modern high-dimensional statistics has been devoted to the analysis of linear predictors trained on nonlinear feature embeddings via empirical risk minimization (ERM). Gaussian equivalence theory (GET) has emerged as a powerful universality principle in this context: it states that the behavior of high-dimensional, complex features can be captured by Gaussian surrogates, which are more amenable to analysis. Despite its remarkable successes, numerical experiments show that this equivalence can fail even for simple embeddings -- such as polynomial maps -- under general scaling regimes. We investigate this breakdown in the setting of random feature (RF) models in the quadratic scaling regime, where both the number of features and the sample size grow quadratically with the data dimension. We show that when the target function depends on a low-dimensional projection of the data, such as generalized linear models, GET yields incorrect predictions. To capture the correct asymptotics, we introduce a Conditional Gaussian Equivalent (CGE) model, which can be viewed as appending a low-dimensional non-Gaussian component to an otherwise high-dimensional Gaussian model. This hybrid model retains the tractability of the Gaussian framework and accurately describes RF models in the quadratic scaling regime. We derive sharp asymptotics for the training and test errors in this setting, which continue to agree with numerical simulations even when GET fails. Our analysis combines general results on CLT for Wiener chaos expansions and a careful two-phase Lindeberg swapping argument. Beyond RF models and quadratic scaling, our work hints at a rich landscape of universality phenomena in high-dimensional ERM."
    },
    {
      "id": "2512.03266v1",
      "title": "Invited Discussion of \"Model Uncertainty and Missing Data: An Objective Bayesian Perspective\" by Gonzalo García-Donato , María Eugenia Castellanos , Stefano Cabras Alicia Quirós , and Anabel Forte",
      "abstract": "The article by Garc{í}a-Donato and co-authors addresses the dual challenges of accounting for model uncertainty and missing data within the Gaussian regression frameworks from an objective Bayesian perspective. Thru the use of an imputation -prior that replaces  for model  in the covariance of  with , the authors develop a coherent approach to addressing the missing data problem and model uncertainty simultaneously with random  in the missing at random (MAR) or missing completely at random (MCAR) settings, while still being computationally tractable. I discuss the connection of the imputation -prior to the -prior with imputed , and to model selection for graphical models that provide an alternative justification for the -prior for random s.",
      "authors": [
        "Merlise A Clyde"
      ],
      "published_date": "2025-12-02",
      "full_text": "Invited Discussion of \"Model Uncertainty and Missing Data: An Objective Bayesian Perspective\" by Gonzalo García-Donato , María Eugenia Castellanos , Stefano Cabras Alicia Quirós , and Anabel Forte\n\nThe article by Garc{í}a-Donato and co-authors addresses the dual challenges of accounting for model uncertainty and missing data within the Gaussian regression frameworks from an objective Bayesian perspective. Thru the use of an imputation -prior that replaces  for model  in the covariance of  with , the authors develop a coherent approach to addressing the missing data problem and model uncertainty simultaneously with random  in the missing at random (MAR) or missing completely at random (MCAR) settings, while still being computationally tractable. I discuss the connection of the imputation -prior to the -prior with imputed , and to model selection for graphical models that provide an alternative justification for the -prior for random s."
    },
    {
      "id": "2512.03243v1",
      "title": "Novelty detection on path space",
      "abstract": "We frame novelty detection on path space as a hypothesis testing problem with signature-based test statistics. Using transportation-cost inequalities of Gasteratos and Jacquier (2023), we obtain tail bounds for false positive rates that extend beyond Gaussian measures to laws of RDE solutions with smooth bounded vector fields, yielding estimates of quantiles and p-values. Exploiting the shuffle product, we derive exact formulae for smooth surrogates of conditional value-at-risk (CVaR) in terms of expected signatures, leading to new one-class SVM algorithms optimising smooth CVaR objectives. We then establish lower bounds on type- error for alternatives with finite first moment, giving general power bounds when the reference measure and the alternative are absolutely continuous with respect to each other. Finally, we evaluate numerically the type- error and statistical power of signature-based test statistic, using synthetic anomalous diffusion data and real-world molecular biology data.",
      "authors": [
        "Ioannis Gasteratos",
        "Antoine Jacquier",
        "Maud Lemercier",
        "Terry Lyons",
        "Cristopher Salvi"
      ],
      "published_date": "2025-12-02",
      "full_text": "Novelty detection on path space\n\nWe frame novelty detection on path space as a hypothesis testing problem with signature-based test statistics. Using transportation-cost inequalities of Gasteratos and Jacquier (2023), we obtain tail bounds for false positive rates that extend beyond Gaussian measures to laws of RDE solutions with smooth bounded vector fields, yielding estimates of quantiles and p-values. Exploiting the shuffle product, we derive exact formulae for smooth surrogates of conditional value-at-risk (CVaR) in terms of expected signatures, leading to new one-class SVM algorithms optimising smooth CVaR objectives. We then establish lower bounds on type- error for alternatives with finite first moment, giving general power bounds when the reference measure and the alternative are absolutely continuous with respect to each other. Finally, we evaluate numerically the type- error and statistical power of signature-based test statistic, using synthetic anomalous diffusion data and real-world molecular biology data."
    },
    {
      "id": "2512.02922v1",
      "title": "Asymptotics for additive functionals of particle systems via Stein's method",
      "abstract": "We consider additive functionals of systems of random measures whose initial configuration is given by a Poisson point process, and whose individual components evolve according to arbitrary Markovian or non-Markovian measure valued dynamics, with no structural assumptions beyond basic moment bounds. In this setting and under adequate conditions, we establish a general third moment theorem for the normalized functionals. Building on this result, we obtain the first quantitative bounds in the Wasserstein distance for a variety of moving-measure models initialized by Poisson-driven clouds of points, turning qualitative central limit theorems into explicit rates of convergence. The scope of the approach is then demonstrated through several examples, including systems driven by fractional Brownian motion, -stable processes, uniformly elliptic diffusions, and spectral empirical measures arising from Dyson Brownian motion, all under broad assumptions on the control measure of the initial Poisson configuration. The analysis relies on a combination of Stein's method with Mecke's formula, in the spirit of the Poisson Malliavin-Stein methodology.",
      "authors": [
        "Arturo Jaramillo",
        "Antonio Murillo-Salas"
      ],
      "published_date": "2025-12-02",
      "full_text": "Asymptotics for additive functionals of particle systems via Stein's method\n\nWe consider additive functionals of systems of random measures whose initial configuration is given by a Poisson point process, and whose individual components evolve according to arbitrary Markovian or non-Markovian measure valued dynamics, with no structural assumptions beyond basic moment bounds. In this setting and under adequate conditions, we establish a general third moment theorem for the normalized functionals. Building on this result, we obtain the first quantitative bounds in the Wasserstein distance for a variety of moving-measure models initialized by Poisson-driven clouds of points, turning qualitative central limit theorems into explicit rates of convergence. The scope of the approach is then demonstrated through several examples, including systems driven by fractional Brownian motion, -stable processes, uniformly elliptic diffusions, and spectral empirical measures arising from Dyson Brownian motion, all under broad assumptions on the control measure of the initial Poisson configuration. The analysis relies on a combination of Stein's method with Mecke's formula, in the spirit of the Poisson Malliavin-Stein methodology."
    },
    {
      "id": "2512.02912v1",
      "title": "Hypothesis Testing for Generalized Thurstone Models",
      "abstract": "In this work, we develop a hypothesis testing framework to determine whether pairwise comparison data is generated by an underlying   for a given choice function . While prior work has predominantly focused on parameter estimation and uncertainty quantification for such models, we address the fundamental problem of minimax hypothesis testing for  models. We formulate this testing problem by introducing a notion of separation distance between general pairwise comparison models and the class of  models. We then derive upper and lower bounds on the critical threshold for testing that depend on the topology of the observation graph. For the special case of complete observation graphs, this threshold scales as , where  is the number of agents and  is the number of comparisons per pair. Furthermore, we propose a hypothesis test based on our separation distance, construct confidence intervals, establish time-uniform bounds on the probabilities of type I and II errors using reverse martingale techniques, and derive minimax lower bounds using information-theoretic methods. Finally, we validate our results through experiments on synthetic and real-world datasets.",
      "authors": [
        "Anuran Makur",
        "Japneet Singh"
      ],
      "published_date": "2025-12-02",
      "full_text": "Hypothesis Testing for Generalized Thurstone Models\n\nIn this work, we develop a hypothesis testing framework to determine whether pairwise comparison data is generated by an underlying   for a given choice function . While prior work has predominantly focused on parameter estimation and uncertainty quantification for such models, we address the fundamental problem of minimax hypothesis testing for  models. We formulate this testing problem by introducing a notion of separation distance between general pairwise comparison models and the class of  models. We then derive upper and lower bounds on the critical threshold for testing that depend on the topology of the observation graph. For the special case of complete observation graphs, this threshold scales as , where  is the number of agents and  is the number of comparisons per pair. Furthermore, we propose a hypothesis test based on our separation distance, construct confidence intervals, establish time-uniform bounds on the probabilities of type I and II errors using reverse martingale techniques, and derive minimax lower bounds using information-theoretic methods. Finally, we validate our results through experiments on synthetic and real-world datasets."
    },
    {
      "id": "2512.02866v1",
      "title": "HeteroJIVE: Joint Subspace Estimation for Heterogeneous Multi-View Data",
      "abstract": "Many modern datasets consist of multiple related matrices measured on a common set of units, where the goal is to recover the shared low-dimensional subspace. While the Angle-based Joint and Individual Variation Explained (AJIVE) framework provides a solution, it relies on equal-weight aggregation, which can be strictly suboptimal when views exhibit significant statistical heterogeneity (arising from varying SNR and dimensions) and structural heterogeneity (arising from individual components). In this paper, we propose HeteroJIVE, a weighted two-stage spectral algorithm tailored to such heterogeneity. Theoretically, we first revisit the ``non-diminishing\" error barrier with respect to the number of views  identified in recent literature for the equal-weight case. We demonstrate that this barrier is not universal: under generic geometric conditions, the bias term vanishes and our estimator achieves the  rate without the need for iterative refinement. Extending this to the general-weight case, we establish error bounds that explicitly disentangle the two layers of heterogeneity. Based on this, we derive an oracle-optimal weighting scheme implemented via a data-driven procedure. Extensive simulations corroborate our theoretical findings, and an application to TCGA-BRCA multi-omics data validates the superiority of HeteroJIVE in practice.",
      "authors": [
        "Jingyang Li",
        "Zhongyuan Lyu"
      ],
      "published_date": "2025-12-02",
      "full_text": "HeteroJIVE: Joint Subspace Estimation for Heterogeneous Multi-View Data\n\nMany modern datasets consist of multiple related matrices measured on a common set of units, where the goal is to recover the shared low-dimensional subspace. While the Angle-based Joint and Individual Variation Explained (AJIVE) framework provides a solution, it relies on equal-weight aggregation, which can be strictly suboptimal when views exhibit significant statistical heterogeneity (arising from varying SNR and dimensions) and structural heterogeneity (arising from individual components). In this paper, we propose HeteroJIVE, a weighted two-stage spectral algorithm tailored to such heterogeneity. Theoretically, we first revisit the ``non-diminishing\" error barrier with respect to the number of views  identified in recent literature for the equal-weight case. We demonstrate that this barrier is not universal: under generic geometric conditions, the bias term vanishes and our estimator achieves the  rate without the need for iterative refinement. Extending this to the general-weight case, we establish error bounds that explicitly disentangle the two layers of heterogeneity. Based on this, we derive an oracle-optimal weighting scheme implemented via a data-driven procedure. Extensive simulations corroborate our theoretical findings, and an application to TCGA-BRCA multi-omics data validates the superiority of HeteroJIVE in practice."
    },
    {
      "id": "2512.02831v1",
      "title": "Revisiting Theory of Contrastive Learning for Domain Generalization",
      "abstract": "Contrastive learning is among the most popular and powerful approaches for self-supervised representation learning, where the goal is to map semantically similar samples close together while separating dissimilar ones in the latent space. Existing theoretical methods assume that downstream task classes are drawn from the same latent class distribution used during the pretraining phase. However, in real-world settings, downstream tasks may not only exhibit distributional shifts within the same label space but also introduce new or broader label spaces, leading to domain generalization challenges. In this work, we introduce novel generalization bounds that explicitly account for both types of mismatch: domain shift and domain generalization. Specifically, we analyze scenarios where downstream tasks either (i) draw classes from the same latent class space but with shifted distributions, or (ii) involve new label spaces beyond those seen during pretraining. Our analysis reveals how the performance of contrastively learned representations depends on the statistical discrepancy between pretraining and downstream distributions. This extended perspective allows us to derive provable guarantees on the performance of learned representations on average classification tasks involving class distributions outside the pretraining latent class set.",
      "authors": [
        "Ali Alvandi",
        "Mina Rezaei"
      ],
      "published_date": "2025-12-02",
      "full_text": "Revisiting Theory of Contrastive Learning for Domain Generalization\n\nContrastive learning is among the most popular and powerful approaches for self-supervised representation learning, where the goal is to map semantically similar samples close together while separating dissimilar ones in the latent space. Existing theoretical methods assume that downstream task classes are drawn from the same latent class distribution used during the pretraining phase. However, in real-world settings, downstream tasks may not only exhibit distributional shifts within the same label space but also introduce new or broader label spaces, leading to domain generalization challenges. In this work, we introduce novel generalization bounds that explicitly account for both types of mismatch: domain shift and domain generalization. Specifically, we analyze scenarios where downstream tasks either (i) draw classes from the same latent class space but with shifted distributions, or (ii) involve new label spaces beyond those seen during pretraining. Our analysis reveals how the performance of contrastively learned representations depends on the statistical discrepancy between pretraining and downstream distributions. This extended perspective allows us to derive provable guarantees on the performance of learned representations on average classification tasks involving class distributions outside the pretraining latent class set."
    },
    {
      "id": "2512.02519v1",
      "title": "Mean First Passage Time of the Symmetric Noisy Voter Model with Arbitrary Initial and Boundary Conditions",
      "abstract": "Models of imitation and herding behavior often underestimate the role of individualistic actions and assume symmetric boundary conditions. However, real-world systems (e.g., electoral processes) frequently involve asymmetric boundaries. In this study, we explore how arbitrarily placed boundary conditions influence the mean first passage time in the symmetric noisy voter model, and how individualistic behavior amplifies this asymmetry. We derive exact analytical expressions for mean first passage time that accommodate any initial condition and two types of boundary configurations: (i) both boundaries absorbing, and (ii) one absorbing and one reflective. In both scenarios, mean first passage time exhibits a clear asymmetry with respect to the initial condition, shaped by the boundary placement and the rate of independent transitions. Symmetry in mean first passage time emerges only when absorbing boundaries are equidistant from the midpoint. Additionally, we show that Kramers' law holds in both configurations when the rate of independent transitions is large. Our analytical results are in excellent agreement with numerical simulations, reinforcing the robustness of our findings.",
      "authors": [
        "Rytis Kazakevičius",
        "Aleksejus Kononovicius"
      ],
      "published_date": "2025-12-02",
      "full_text": "Mean First Passage Time of the Symmetric Noisy Voter Model with Arbitrary Initial and Boundary Conditions\n\nModels of imitation and herding behavior often underestimate the role of individualistic actions and assume symmetric boundary conditions. However, real-world systems (e.g., electoral processes) frequently involve asymmetric boundaries. In this study, we explore how arbitrarily placed boundary conditions influence the mean first passage time in the symmetric noisy voter model, and how individualistic behavior amplifies this asymmetry. We derive exact analytical expressions for mean first passage time that accommodate any initial condition and two types of boundary configurations: (i) both boundaries absorbing, and (ii) one absorbing and one reflective. In both scenarios, mean first passage time exhibits a clear asymmetry with respect to the initial condition, shaped by the boundary placement and the rate of independent transitions. Symmetry in mean first passage time emerges only when absorbing boundaries are equidistant from the midpoint. Additionally, we show that Kramers' law holds in both configurations when the rate of independent transitions is large. Our analytical results are in excellent agreement with numerical simulations, reinforcing the robustness of our findings."
    },
    {
      "id": "2512.02249v1",
      "title": "Discrete Sequential Barycenter Arrays: Representation, Approximation, and Modeling of Probability Measures",
      "abstract": "Constructing flexible probability models that respect constraints on key functionals -- such as the mean -- is a fundamental problem in nonparametric statistics. Existing approaches lack systematic tools for enforcing such constraints while retaining full modeling flexibility. This paper introduces a new representation for univariate probability measures based on discrete sequential barycenter arrays (SBA). We study structural properties of SBA representations and establish new approximation results. In particular, we show that for any target distribution, its SBA-based discrete approximations converge in both the weak topology and in Wasserstein distances, and that the representation is exact for all distributions with finite discrete support. We further characterize a broad class of measures whose SBA partitions exhibit regularity and induce increasingly fine meshes, and we prove that this class is dense in standard probabilistic topologies. These theoretical results enable the construction of probability models that preserve prescribed values -- or full distributions -- of the mean while maintaining large support. As an application, we derive a mixture model for density estimation whose induced mixing distribution has a fixed or user-specified mean. The resulting framework provides a principled mechanism for incorporating mean constraints in nonparametric modeling while preserving strong approximation properties. The approach is illustrated using both simulated and real data.",
      "authors": [
        "Alejandro Jara",
        "Carlos Sing-Long"
      ],
      "published_date": "2025-12-01",
      "full_text": "Discrete Sequential Barycenter Arrays: Representation, Approximation, and Modeling of Probability Measures\n\nConstructing flexible probability models that respect constraints on key functionals -- such as the mean -- is a fundamental problem in nonparametric statistics. Existing approaches lack systematic tools for enforcing such constraints while retaining full modeling flexibility. This paper introduces a new representation for univariate probability measures based on discrete sequential barycenter arrays (SBA). We study structural properties of SBA representations and establish new approximation results. In particular, we show that for any target distribution, its SBA-based discrete approximations converge in both the weak topology and in Wasserstein distances, and that the representation is exact for all distributions with finite discrete support. We further characterize a broad class of measures whose SBA partitions exhibit regularity and induce increasingly fine meshes, and we prove that this class is dense in standard probabilistic topologies. These theoretical results enable the construction of probability models that preserve prescribed values -- or full distributions -- of the mean while maintaining large support. As an application, we derive a mixture model for density estimation whose induced mixing distribution has a fixed or user-specified mean. The resulting framework provides a principled mechanism for incorporating mean constraints in nonparametric modeling while preserving strong approximation properties. The approach is illustrated using both simulated and real data."
    },
    {
      "id": "2512.01963v1",
      "title": "Basis Choices for Frequency Domain Statistical Independence Tests and Algorithms for Algebraic Relation Extraction",
      "abstract": "In this paper, we explore how different selections of basis functions impact the efficacy of frequency domain techniques in statistical independence tests, and study different algorithms for extracting low-dimensional algebraic relations from dependent data. We examine a range of complete orthonormal bases functions including the Legendre polynomials, Fourier series, Walsh functions, and standard and nonstandard Haar wavelet bases. We utilize fast transformation algorithms to efficiently transform physical domain data to frequency domain coefficients. The main focuses of this paper are the effectiveness of different basis selections in detecting data dependency using frequency domain data, e.g., whether varying basis choices significantly influence statistical power loss for small data with large noise; and on the stability of different optimization formulations for finding proper algebraic relations when data are dependent. We present numerical results to demonstrate the effectiveness of frequency domain-based statistical analysis methods and provide guidance for selecting the proper basis and algorithm to detect a particular type of relations.",
      "authors": [
        "Juan Shi",
        "Wenbo Wang",
        "Wan Zhang",
        "Han Bao",
        "Sergio Chavez",
        "Jingfang Huang",
        "Yichao Wu",
        "Kai Zhang"
      ],
      "published_date": "2025-12-01",
      "full_text": "Basis Choices for Frequency Domain Statistical Independence Tests and Algorithms for Algebraic Relation Extraction\n\nIn this paper, we explore how different selections of basis functions impact the efficacy of frequency domain techniques in statistical independence tests, and study different algorithms for extracting low-dimensional algebraic relations from dependent data. We examine a range of complete orthonormal bases functions including the Legendre polynomials, Fourier series, Walsh functions, and standard and nonstandard Haar wavelet bases. We utilize fast transformation algorithms to efficiently transform physical domain data to frequency domain coefficients. The main focuses of this paper are the effectiveness of different basis selections in detecting data dependency using frequency domain data, e.g., whether varying basis choices significantly influence statistical power loss for small data with large noise; and on the stability of different optimization formulations for finding proper algebraic relations when data are dependent. We present numerical results to demonstrate the effectiveness of frequency domain-based statistical analysis methods and provide guidance for selecting the proper basis and algorithm to detect a particular type of relations."
    },
    {
      "id": "2512.01838v1",
      "title": "Goodness-of-fit testing from observations with multiplicative measurement error",
      "abstract": "Given observations from a positive random variable contaminated by multiplicative measurement error, we consider a nonparametric goodness-of-fit testing task for its unknown density in a non-asymptotic framework. We propose a testing procedure based on estimating a quadratic functional of the Mellin transform of the unknown density and the null. We derive non-asymptotic testing radii and testing rates over Mellin-Sobolev spaces, which naturally characterize regularity and ill-posedness in this model. By employing a multiple testing procedure with Bonferroni correction, we obtain data-driven procedures and analyze their performance. Compared with the non-adaptive tests, their testing radii deteriorate by at most a logarithmic factor. We illustrate the testing procedures with a simulation study using various choices of densities.",
      "authors": [
        "Jan Johannes",
        "Bianca Neubert"
      ],
      "published_date": "2025-12-01",
      "full_text": "Goodness-of-fit testing from observations with multiplicative measurement error\n\nGiven observations from a positive random variable contaminated by multiplicative measurement error, we consider a nonparametric goodness-of-fit testing task for its unknown density in a non-asymptotic framework. We propose a testing procedure based on estimating a quadratic functional of the Mellin transform of the unknown density and the null. We derive non-asymptotic testing radii and testing rates over Mellin-Sobolev spaces, which naturally characterize regularity and ill-posedness in this model. By employing a multiple testing procedure with Bonferroni correction, we obtain data-driven procedures and analyze their performance. Compared with the non-adaptive tests, their testing radii deteriorate by at most a logarithmic factor. We illustrate the testing procedures with a simulation study using various choices of densities."
    },
    {
      "id": "2512.01820v1",
      "title": "Dimension-free error estimate for diffusion model and optimal scheduling",
      "abstract": "Diffusion generative models have emerged as powerful tools for producing synthetic data from an empirically observed distribution. A common approach involves simulating the time-reversal of an Ornstein-Uhlenbeck (OU) process initialized at the true data distribution. Since the score function associated with the OU process is typically unknown, it is approximated using a trained neural network. This approximation, along with finite time simulation, time discretization and statistical approximation, introduce several sources of error whose impact on the generated samples must be carefully understood. Previous analyses have quantified the error between the generated and the true data distributions in terms of Wasserstein distance or Kullback-Leibler (KL) divergence. However, both metrics present limitations: KL divergence requires absolute continuity between distributions, while Wasserstein distance, though more general, leads to error bounds that scale poorly with dimension, rendering them impractical in high-dimensional settings. In this work, we derive an explicit, dimension-free bound on the discrepancy between the generated and the true data distributions. The bound is expressed in terms of a smooth test functional with bounded first and second derivatives. The key novelty lies in the use of this weaker, functional metric to obtain dimension-independent guarantees, at the cost of higher regularity on the test functions. As an application, we formulate and solve a variational problem to minimize the time-discretization error, leading to the derivation of an optimal time-scheduling strategy for the reverse-time diffusion. Interestingly, this scheduler has appeared previously in the literature in a different context; our analysis provides a new justification for its optimality, now grounded in minimizing the discretization bias in generative sampling.",
      "authors": [
        "Valentin de Bortoli",
        "Romuald Elie",
        "Anna Kazeykina",
        "Zhenjie Ren",
        "Jiacheng Zhang"
      ],
      "published_date": "2025-12-01",
      "full_text": "Dimension-free error estimate for diffusion model and optimal scheduling\n\nDiffusion generative models have emerged as powerful tools for producing synthetic data from an empirically observed distribution. A common approach involves simulating the time-reversal of an Ornstein-Uhlenbeck (OU) process initialized at the true data distribution. Since the score function associated with the OU process is typically unknown, it is approximated using a trained neural network. This approximation, along with finite time simulation, time discretization and statistical approximation, introduce several sources of error whose impact on the generated samples must be carefully understood. Previous analyses have quantified the error between the generated and the true data distributions in terms of Wasserstein distance or Kullback-Leibler (KL) divergence. However, both metrics present limitations: KL divergence requires absolute continuity between distributions, while Wasserstein distance, though more general, leads to error bounds that scale poorly with dimension, rendering them impractical in high-dimensional settings. In this work, we derive an explicit, dimension-free bound on the discrepancy between the generated and the true data distributions. The bound is expressed in terms of a smooth test functional with bounded first and second derivatives. The key novelty lies in the use of this weaker, functional metric to obtain dimension-independent guarantees, at the cost of higher regularity on the test functions. As an application, we formulate and solve a variational problem to minimize the time-discretization error, leading to the derivation of an optimal time-scheduling strategy for the reverse-time diffusion. Interestingly, this scheduler has appeared previously in the literature in a different context; our analysis provides a new justification for its optimality, now grounded in minimizing the discretization bias in generative sampling."
    },
    {
      "id": "2512.01817v3",
      "title": "Sharp Self-Normalized Concentration Inequalities of Marginal Mean with Sample Variance Only",
      "abstract": "(This is the first version of a working paper. A more detailed follow-up with applications is in preparation.) We develop a family of self-normalized concentration inequalities for marginal mean under martingale-difference structure and -mixing conditions, where the latter includes many processes that are not strongly mixing. The variance term is fully data-observable: naive sample variance in the martingale case and an empirical block long-run variance under mixing conditions. Thus, no predictable variance proxy is required. No specific assumption on the decay of the mixing coefficients (e.g. summability) is needed for the validity. The constants are explicit and the bounds are ready to use.",
      "authors": [
        "Zihao Yuan"
      ],
      "published_date": "2025-12-01",
      "full_text": "Sharp Self-Normalized Concentration Inequalities of Marginal Mean with Sample Variance Only\n\n(This is the first version of a working paper. A more detailed follow-up with applications is in preparation.) We develop a family of self-normalized concentration inequalities for marginal mean under martingale-difference structure and -mixing conditions, where the latter includes many processes that are not strongly mixing. The variance term is fully data-observable: naive sample variance in the martingale case and an empirical block long-run variance under mixing conditions. Thus, no predictable variance proxy is required. No specific assumption on the decay of the mixing coefficients (e.g. summability) is needed for the validity. The constants are explicit and the bounds are ready to use."
    },
    {
      "id": "2512.01790v1",
      "title": "An hybrid stochastic Newton algorithm for logistic regression",
      "abstract": "In this paper, we investigate a second-order stochastic algorithm for solving large-scale binary classification problems. We propose to make use of a new hybrid stochastic Newton algorithm that includes two weighted components in the Hessian matrix estimation: the first one coming from the natural Hessian estimate and the second associated with the stochastic gradient information. Our motivation comes from the fact that both parts evaluated at the true parameter of logistic regression, are equal to the Hessian matrix. This new formulation has several advantages and it enables us to prove the almost sure convergence of our stochastic algorithm to the true parameter. Moreover, we significantly improve the almost sure rate of convergence to the Hessian matrix. Furthermore, we establish the central limit theorem for our hybrid stochastic Newton algorithm. Finally, we show a surprising result on the almost sure convergence of the cumulative excess risk.",
      "authors": [
        "Bernard Bercu",
        "Luis Fredes",
        "Eméric Gbaguidi"
      ],
      "published_date": "2025-12-01",
      "full_text": "An hybrid stochastic Newton algorithm for logistic regression\n\nIn this paper, we investigate a second-order stochastic algorithm for solving large-scale binary classification problems. We propose to make use of a new hybrid stochastic Newton algorithm that includes two weighted components in the Hessian matrix estimation: the first one coming from the natural Hessian estimate and the second associated with the stochastic gradient information. Our motivation comes from the fact that both parts evaluated at the true parameter of logistic regression, are equal to the Hessian matrix. This new formulation has several advantages and it enables us to prove the almost sure convergence of our stochastic algorithm to the true parameter. Moreover, we significantly improve the almost sure rate of convergence to the Hessian matrix. Furthermore, we establish the central limit theorem for our hybrid stochastic Newton algorithm. Finally, we show a surprising result on the almost sure convergence of the cumulative excess risk."
    },
    {
      "id": "2512.01761v1",
      "title": "A novel sequential method for building upper and lower bounds of moments of distributions",
      "abstract": "Approximating integrals is a fundamental task in probability theory and statistical inference, and their applied fields of signal processing, and Bayesian learning, as soon as expectations over probability distributions must be computed efficiently and accurately. When these integrals lack closed-form expressions, numerical methods must be used, from the Newton-Cotes formulas and Gaussian quadrature, to Monte Carlo and variational approximation techniques. Despite these numerous tools, few are guaranteed to preserve majoration/minoration inequalities, while this feature is fundamental in certain applications in statistics. In this paper, we focus on the integration problem arising in the estimation of moments of scalar, unnormalized, distributions. We introduce a sequential method for constructing upper and lower bounds on the sought integral. Our approach leverages the majorization-minimization framework to iteratively refine these bounds, in an enveloped principle. The method has proven convergence, and controlled accuracy, under mild conditions. We demonstrate its effectiveness through a detailed numerical example of the estimation of a Monte-Carlo sampler variance in a Bayesian inference problem.",
      "authors": [
        "Solal Martin",
        "Emilie Chouzenoux",
        "Victor Elvira"
      ],
      "published_date": "2025-12-01",
      "full_text": "A novel sequential method for building upper and lower bounds of moments of distributions\n\nApproximating integrals is a fundamental task in probability theory and statistical inference, and their applied fields of signal processing, and Bayesian learning, as soon as expectations over probability distributions must be computed efficiently and accurately. When these integrals lack closed-form expressions, numerical methods must be used, from the Newton-Cotes formulas and Gaussian quadrature, to Monte Carlo and variational approximation techniques. Despite these numerous tools, few are guaranteed to preserve majoration/minoration inequalities, while this feature is fundamental in certain applications in statistics. In this paper, we focus on the integration problem arising in the estimation of moments of scalar, unnormalized, distributions. We introduce a sequential method for constructing upper and lower bounds on the sought integral. Our approach leverages the majorization-minimization framework to iteratively refine these bounds, in an enveloped principle. The method has proven convergence, and controlled accuracy, under mild conditions. We demonstrate its effectiveness through a detailed numerical example of the estimation of a Monte-Carlo sampler variance in a Bayesian inference problem."
    },
    {
      "id": "2512.01689v1",
      "title": "The Klebanov theorem for the group ",
      "abstract": "L. Klebanov proved the following theorem. Let  be independent random variables. Consider linear forms     where the coefficients  are real numbers. If the random vectors  and  are identically distributed, then all  for which  for all  are Gaussian random variables. The present article is devoted to an analogue of the Klebanov theorem in the case when random variables take values in the group  and the coefficients of the linear forms are topological endomorphisms of this group.",
      "authors": [
        "Margaryta Myronyuk"
      ],
      "published_date": "2025-12-01",
      "full_text": "The Klebanov theorem for the group \n\nL. Klebanov proved the following theorem. Let  be independent random variables. Consider linear forms     where the coefficients  are real numbers. If the random vectors  and  are identically distributed, then all  for which  for all  are Gaussian random variables. The present article is devoted to an analogue of the Klebanov theorem in the case when random variables take values in the group  and the coefficients of the linear forms are topological endomorphisms of this group."
    },
    {
      "id": "2512.01447v1",
      "title": "Hawkes process with a diffusion-driven baseline: long-run behavior, inference, statistical tests",
      "abstract": "Event-driven systems in fields such as neuroscience, social networks, and finance often exhibit dynamics influenced by continuously evolving external covariates. Motivated by these applications, we introduce a new class of multivariate Hawkes processes, in which the spontaneous rate of events is modulated by a diffusion process. This framework allows the point process to adapt dynamically to continuously evolving covariates, capturing both intrinsic self-excitation and external influences. In this article, we establish the probabilistic properties of the coupled process, proving stability and ergodicity under moderate assumptions. Classical functional results, including law of large numbers and mixing properties, are extended to this diffusion-driven setting. Building on these results, we study parametric inference for the Hawkes component: we derive consistency and asymptotic normality of the maximum likelihood estimator in the long-time regime, and derive stronger convergence results under additional assumptions on the covariate process. We further propose hypothesis testing procedures to assess the statistical relevance of the covariate. Simulation studies illustrate the validity of the asymptotic results and the effectiveness of the proposed inference methods. Overall, this work provides theoretical and practical foundations for diffusion-driven Hawkes models.",
      "authors": [
        "Maya Sadeler Perrin",
        "Anna Bonnet",
        "Charlotte Dion-Blanc",
        "Adeline Samson"
      ],
      "published_date": "2025-12-01",
      "full_text": "Hawkes process with a diffusion-driven baseline: long-run behavior, inference, statistical tests\n\nEvent-driven systems in fields such as neuroscience, social networks, and finance often exhibit dynamics influenced by continuously evolving external covariates. Motivated by these applications, we introduce a new class of multivariate Hawkes processes, in which the spontaneous rate of events is modulated by a diffusion process. This framework allows the point process to adapt dynamically to continuously evolving covariates, capturing both intrinsic self-excitation and external influences. In this article, we establish the probabilistic properties of the coupled process, proving stability and ergodicity under moderate assumptions. Classical functional results, including law of large numbers and mixing properties, are extended to this diffusion-driven setting. Building on these results, we study parametric inference for the Hawkes component: we derive consistency and asymptotic normality of the maximum likelihood estimator in the long-time regime, and derive stronger convergence results under additional assumptions on the covariate process. We further propose hypothesis testing procedures to assess the statistical relevance of the covariate. Simulation studies illustrate the validity of the asymptotic results and the effectiveness of the proposed inference methods. Overall, this work provides theoretical and practical foundations for diffusion-driven Hawkes models."
    },
    {
      "id": "2512.01408v1",
      "title": "Bayesian Distributionally Robust Merton Problem with Nonlinear Wasserstein Projections",
      "abstract": "We revisit Merton's continuous-time portfolio selection through a data-driven, distributionally robust lens. Our aim is to tap the benefits of frequent trading over short horizons while acknowledging that drift is hard to pin down, whereas volatility can be screened using realized or implied measures for appropriately selected assets. Rather than time-rectangular distributional robust control -- which replenishes adversarial power at every instant and induces over-pessimism -- we place a single ambiguity set on the drift prior within a Bayesian Merton model. This prior-level ambiguity preserves learning and tractability: a minimax swap reduces the robust control to optimizing a nonlinear functional of the prior, enabling Karatzas and Zhao -type's closed-form evaluation for each candidate prior. We then characterize small-radius worst-case priors under Wasserstein uncertainty via an explicit asymptotically optimal pushforward of the nominal prior, and we calibrate the ambiguity radius through a nonlinear Wasserstein projection tailored to the Merton functional. Synthetic and real-data studies demonstrate reduced pessimism relative to DRC and improved performance over myopic DRO-Markowitz under frequent rebalancing.",
      "authors": [
        "Jose Blanchet",
        "Jiayi Cheng",
        "Hao Liu",
        "Yang Liu"
      ],
      "published_date": "2025-12-01",
      "full_text": "Bayesian Distributionally Robust Merton Problem with Nonlinear Wasserstein Projections\n\nWe revisit Merton's continuous-time portfolio selection through a data-driven, distributionally robust lens. Our aim is to tap the benefits of frequent trading over short horizons while acknowledging that drift is hard to pin down, whereas volatility can be screened using realized or implied measures for appropriately selected assets. Rather than time-rectangular distributional robust control -- which replenishes adversarial power at every instant and induces over-pessimism -- we place a single ambiguity set on the drift prior within a Bayesian Merton model. This prior-level ambiguity preserves learning and tractability: a minimax swap reduces the robust control to optimizing a nonlinear functional of the prior, enabling Karatzas and Zhao -type's closed-form evaluation for each candidate prior. We then characterize small-radius worst-case priors under Wasserstein uncertainty via an explicit asymptotically optimal pushforward of the nominal prior, and we calibrate the ambiguity radius through a nonlinear Wasserstein projection tailored to the Merton functional. Synthetic and real-data studies demonstrate reduced pessimism relative to DRC and improved performance over myopic DRO-Markowitz under frequent rebalancing."
    },
    {
      "id": "2512.01277v1",
      "title": "Volatility change point detection for linear parabolic SPDEs",
      "abstract": "We consider change point detection for the volatility in second order linear parabolic stochastic partial differential equations based on high frequency spatio-temporal data. We give a test statistic to detect changes in the volatility based on change point analysis for diffusion processes and derive the asymptotic null distribution of the test statistic. We also show that the test is consistent. Moreover, we provide some examples and then perform numerical simulations of the proposed test statistic.",
      "authors": [
        "Yozo Tonaki",
        "Yusuke Kaino",
        "Masayuki Uchida"
      ],
      "published_date": "2025-12-01",
      "full_text": "Volatility change point detection for linear parabolic SPDEs\n\nWe consider change point detection for the volatility in second order linear parabolic stochastic partial differential equations based on high frequency spatio-temporal data. We give a test statistic to detect changes in the volatility based on change point analysis for diffusion processes and derive the asymptotic null distribution of the test statistic. We also show that the test is consistent. Moreover, we provide some examples and then perform numerical simulations of the proposed test statistic."
    },
    {
      "id": "2512.01026v1",
      "title": "Asymptotic inference in a stationary quantum time series",
      "abstract": "We consider a statistical model of a n-mode quantum Gaussian state which is shift invariant and also gauge invariant. Such models can be considered analogs of classical Gaussian stationary time series, parametrized by their spectral density. Defining an appropriate quantum spectral density as the parameter, we establish that the quantum Gaussian time series model is asymptotically equivalent to a classical nonlinear regression model given as a collection of independent geometric random variables. The asymptotic equivalence is established in the sense of the quantum Le Cam distance between statistical models (experiments). The geometric regression model has a further classical approximation as a certain Gaussian white noise model with a transformed quantum spectral density as signal. In this sense, the result is a quantum analog of the asymptotic equivalence of classical spectral density estimation and Gaussian white noise, which is known for Gaussian stationary time series. In a forthcoming version of this preprint, we will also identify a quantum analog of the periodogram and provide optimal parametric and nonparametric estimates of the quantum spectral density.",
      "authors": [
        "Michael Nussbaum",
        "Arleta Szkoła"
      ],
      "published_date": "2025-11-30",
      "full_text": "Asymptotic inference in a stationary quantum time series\n\nWe consider a statistical model of a n-mode quantum Gaussian state which is shift invariant and also gauge invariant. Such models can be considered analogs of classical Gaussian stationary time series, parametrized by their spectral density. Defining an appropriate quantum spectral density as the parameter, we establish that the quantum Gaussian time series model is asymptotically equivalent to a classical nonlinear regression model given as a collection of independent geometric random variables. The asymptotic equivalence is established in the sense of the quantum Le Cam distance between statistical models (experiments). The geometric regression model has a further classical approximation as a certain Gaussian white noise model with a transformed quantum spectral density as signal. In this sense, the result is a quantum analog of the asymptotic equivalence of classical spectral density estimation and Gaussian white noise, which is known for Gaussian stationary time series. In a forthcoming version of this preprint, we will also identify a quantum analog of the periodogram and provide optimal parametric and nonparametric estimates of the quantum spectral density."
    },
    {
      "id": "2512.00901v1",
      "title": "Grouped Competition Test with Unified False Discovery Rate Control",
      "abstract": "This paper discusses several p-value-free multiple hypothesis testing methods proposed in recent years and organizes them by introducing a unified framework termed competition test. Although existing competition tests are effective in controlling the False Discovery Rate (FDR), they struggle with handling data with strong heterogeneity or dependency structures. Based on this framework, the paper proposes a novel approach that applies a corrected competition procedure to group data with certain structure, and then integrates the results from each group. Using the favorable properties of competition test, the paper proposes a theorem demonstrating that this approach controls the global FDR. We further show that although the correction parameters may lead to a slight loss in power, such loss is typically minimal. Through simulation experiments and mass spectrometry data analysis, we illustrate the flexibility and efficacy of our approach.",
      "authors": [
        "Mingzhou Deng",
        "Yan Fu"
      ],
      "published_date": "2025-11-30",
      "full_text": "Grouped Competition Test with Unified False Discovery Rate Control\n\nThis paper discusses several p-value-free multiple hypothesis testing methods proposed in recent years and organizes them by introducing a unified framework termed competition test. Although existing competition tests are effective in controlling the False Discovery Rate (FDR), they struggle with handling data with strong heterogeneity or dependency structures. Based on this framework, the paper proposes a novel approach that applies a corrected competition procedure to group data with certain structure, and then integrates the results from each group. Using the favorable properties of competition test, the paper proposes a theorem demonstrating that this approach controls the global FDR. We further show that although the correction parameters may lead to a slight loss in power, such loss is typically minimal. Through simulation experiments and mass spectrometry data analysis, we illustrate the flexibility and efficacy of our approach."
    },
    {
      "id": "2512.04024v1",
      "title": "Predicting parameters of a model cuprate superconductor using machine learning",
      "abstract": "The computational complexity of calculating phase diagrams for multi-parameter models significantly limits the ability to select parameters that correspond to experimental data. This work presents a machine learning method for solving the inverse problem - forecasting the parameters of a model Hamiltonian for a cuprate superconductor based on its phase diagram. A comparative study of three deep learning architectures was conducted: VGG, ResNet, and U-Net. The latter was adapted for regression tasks and demonstrated the best performance. Training the U-Net model was performed on an extensive dataset of phase diagrams calculated within the mean-field approximation, followed by validation on data obtained using a semi-classical heat bath algorithm for Monte Carlo simulations. It is shown that the model accurately predicts all considered Hamiltonian parameters, and areas of low prediction accuracy correspond to regions of parametric insensitivity in the phase diagrams. This allows for the extraction of physically interpretable patterns and validation of the significance of parameters for the system. The results confirm the promising potential of applying machine learning to analyze complex physical models in condensed matter physics.",
      "authors": [
        "V. A. Ulitko",
        "D. N. Yasinskaya",
        "S. A. Bezzubin",
        "A. A. Koshelev",
        "Y. D. Panov"
      ],
      "published_date": "2025-12-03",
      "full_text": "Predicting parameters of a model cuprate superconductor using machine learning\n\nThe computational complexity of calculating phase diagrams for multi-parameter models significantly limits the ability to select parameters that correspond to experimental data. This work presents a machine learning method for solving the inverse problem - forecasting the parameters of a model Hamiltonian for a cuprate superconductor based on its phase diagram. A comparative study of three deep learning architectures was conducted: VGG, ResNet, and U-Net. The latter was adapted for regression tasks and demonstrated the best performance. Training the U-Net model was performed on an extensive dataset of phase diagrams calculated within the mean-field approximation, followed by validation on data obtained using a semi-classical heat bath algorithm for Monte Carlo simulations. It is shown that the model accurately predicts all considered Hamiltonian parameters, and areas of low prediction accuracy correspond to regions of parametric insensitivity in the phase diagrams. This allows for the extraction of physically interpretable patterns and validation of the significance of parameters for the system. The results confirm the promising potential of applying machine learning to analyze complex physical models in condensed matter physics."
    },
    {
      "id": "2512.03974v1",
      "title": "Refining Machine Learning Potentials through Thermodynamic Theory of Phase Transitions",
      "abstract": "Foundational Machine Learning Potentials can resolve the accuracy and transferability limitations of classical force fields. They enable microscopic insights into material behavior through Molecular Dynamics simulations, which can crucially expedite material design and discovery. However, insufficiently broad and systematically biased reference data affect the predictive quality of the learned models. Often, these models exhibit significant deviations from experimentally observed phase transition temperatures, in the order of several hundred kelvins. Thus, fine-tuning is necessary to achieve adequate accuracy in many practical problems. This work proposes a fine-tuning strategy via top-down learning, directly correcting the wrongly predicted transition temperatures to match the experimental reference data. Our approach leverages the Differentiable Trajectory Reweighting algorithm to minimize the free energy differences between phases at the experimental target pressures and temperatures. We demonstrate that our approach can accurately correct the phase diagram of pure Titanium in a pressure range of up to 5 GPa, matching the experimental reference within tenths of kelvins and improving the liquid-state diffusion constant. Our approach is model-agnostic, applicable to multi-component systems with solid-solid and solid-liquid transitions, and compliant with top-down training on other experimental properties. Therefore, our approach can serve as an essential step towards highly accurate application-specific and foundational machine learning potentials.",
      "authors": [
        "Paul Fuchs",
        "Julija Zavadlav"
      ],
      "published_date": "2025-12-03",
      "full_text": "Refining Machine Learning Potentials through Thermodynamic Theory of Phase Transitions\n\nFoundational Machine Learning Potentials can resolve the accuracy and transferability limitations of classical force fields. They enable microscopic insights into material behavior through Molecular Dynamics simulations, which can crucially expedite material design and discovery. However, insufficiently broad and systematically biased reference data affect the predictive quality of the learned models. Often, these models exhibit significant deviations from experimentally observed phase transition temperatures, in the order of several hundred kelvins. Thus, fine-tuning is necessary to achieve adequate accuracy in many practical problems. This work proposes a fine-tuning strategy via top-down learning, directly correcting the wrongly predicted transition temperatures to match the experimental reference data. Our approach leverages the Differentiable Trajectory Reweighting algorithm to minimize the free energy differences between phases at the experimental target pressures and temperatures. We demonstrate that our approach can accurately correct the phase diagram of pure Titanium in a pressure range of up to 5 GPa, matching the experimental reference within tenths of kelvins and improving the liquid-state diffusion constant. Our approach is model-agnostic, applicable to multi-component systems with solid-solid and solid-liquid transitions, and compliant with top-down training on other experimental properties. Therefore, our approach can serve as an essential step towards highly accurate application-specific and foundational machine learning potentials."
    },
    {
      "id": "2512.03923v1",
      "title": "Quantum-Classical Physics-Informed Neural Networks for Solving Reservoir Seepage Equations",
      "abstract": "Solving partial differential equations (PDEs) for reservoir seepage is critical for optimizing oil and gas field development and predicting production performance. Traditional numerical methods suffer from mesh-dependent errors and high computational costs, while classical Physics-Informed Neural Networks (PINNs) face bottlenecks in parameter efficiency, high-dimensional expression, and strong nonlinear fitting. To address these limitations, we propose a Discrete Variable (DV)-Circuit Quantum-Classical Physics-Informed Neural Network (QCPINN) and apply it to three typical reservoir seepage models for the first time: the pressure diffusion equation for heterogeneous single-phase flow, the nonlinear Buckley-Leverett (BL) equation for two-phase waterflooding, and the convection-diffusion equation for compositional flow considering adsorption. The QCPINN integrates classical preprocessing/postprocessing networks with a DV quantum core, leveraging quantum superposition and entanglement to enhance high-dimensional feature mapping while embedding physical constraints to ensure solution consistency. We test three quantum circuit topologies (Cascade, Cross-mesh, Alternate) and demonstrate through numerical experiments that QCPINNs achieve high prediction accuracy with fewer parameters than classical PINNs. Specifically, the Alternate topology outperforms others in heterogeneous single-phase flow and two-phase BL equation simulations, while the Cascade topology excels in compositional flow with convection-dispersion-adsorption coupling. Our work verifies the feasibility of QCPINN for reservoir engineering applications, bridging the gap between quantum computing research and industrial practice in oil and gas engineering.",
      "authors": [
        "Xiang Rao",
        "Yina Liu",
        "Yuxuan Shen"
      ],
      "published_date": "2025-12-03",
      "full_text": "Quantum-Classical Physics-Informed Neural Networks for Solving Reservoir Seepage Equations\n\nSolving partial differential equations (PDEs) for reservoir seepage is critical for optimizing oil and gas field development and predicting production performance. Traditional numerical methods suffer from mesh-dependent errors and high computational costs, while classical Physics-Informed Neural Networks (PINNs) face bottlenecks in parameter efficiency, high-dimensional expression, and strong nonlinear fitting. To address these limitations, we propose a Discrete Variable (DV)-Circuit Quantum-Classical Physics-Informed Neural Network (QCPINN) and apply it to three typical reservoir seepage models for the first time: the pressure diffusion equation for heterogeneous single-phase flow, the nonlinear Buckley-Leverett (BL) equation for two-phase waterflooding, and the convection-diffusion equation for compositional flow considering adsorption. The QCPINN integrates classical preprocessing/postprocessing networks with a DV quantum core, leveraging quantum superposition and entanglement to enhance high-dimensional feature mapping while embedding physical constraints to ensure solution consistency. We test three quantum circuit topologies (Cascade, Cross-mesh, Alternate) and demonstrate through numerical experiments that QCPINNs achieve high prediction accuracy with fewer parameters than classical PINNs. Specifically, the Alternate topology outperforms others in heterogeneous single-phase flow and two-phase BL equation simulations, while the Cascade topology excels in compositional flow with convection-dispersion-adsorption coupling. Our work verifies the feasibility of QCPINN for reservoir engineering applications, bridging the gap between quantum computing research and industrial practice in oil and gas engineering."
    },
    {
      "id": "2512.03858v1",
      "title": "Comparing time and frequency domain numerical methods with Born-Rytov approximations for far-field electromagnetic scattering from single biological cells",
      "abstract": "The Born-Rytov approximation estimates effective refractive index of biological cells from measurements of scattered light intensity, polarization and phase. Effective refractive index is useful for estimating a biological cell's dry mass, volume, and internal morphology directly from its elastic light scattering pattern. This work compares the Born-Rytov approximation with analytical, Yee-lattice finite-difference time-domain, and discrete-dipole approximations to Maxwell's equations in the cases of electromagnetic scattering from a sphere and a tomographic reconstruction of Saccharomyces cerevisiae. Practical advantages and limitations of each numerical method are compared for modeling electromagnetic scattering of both near-field intensity and the far-field projected intensity, in terms of accuracy, memory, and compute time. When compared with a commercial software implementation of the Yee-lattice finite-difference time domain method, the Born-Rytov scattering approximation and discrete dipole approximation show better agreement with the far-field light scattering pattern from Saccharomyces cerevisiae.",
      "authors": [
        "Cael Warner"
      ],
      "published_date": "2025-12-03",
      "full_text": "Comparing time and frequency domain numerical methods with Born-Rytov approximations for far-field electromagnetic scattering from single biological cells\n\nThe Born-Rytov approximation estimates effective refractive index of biological cells from measurements of scattered light intensity, polarization and phase. Effective refractive index is useful for estimating a biological cell's dry mass, volume, and internal morphology directly from its elastic light scattering pattern. This work compares the Born-Rytov approximation with analytical, Yee-lattice finite-difference time-domain, and discrete-dipole approximations to Maxwell's equations in the cases of electromagnetic scattering from a sphere and a tomographic reconstruction of Saccharomyces cerevisiae. Practical advantages and limitations of each numerical method are compared for modeling electromagnetic scattering of both near-field intensity and the far-field projected intensity, in terms of accuracy, memory, and compute time. When compared with a commercial software implementation of the Yee-lattice finite-difference time domain method, the Born-Rytov scattering approximation and discrete dipole approximation show better agreement with the far-field light scattering pattern from Saccharomyces cerevisiae."
    },
    {
      "id": "2512.03706v1",
      "title": "Consistent Projection of Langevin Dynamics: Preserving Thermodynamics and Kinetics in Coarse-Grained Models",
      "abstract": "Coarse graining (CG) is an important task for efficient modeling and simulation of complex multi-scale systems, such as the conformational dynamics of biomolecules. This work presents a projection-based coarse-graining formalism for general underdamped Langevin dynamics. Following the Zwanzig projection approach, we derive a closed-form expression for the coarse grained dynamics. In addition, we show how the generator Extended Dynamic Mode Decomposition (gEDMD) method, which was developed in the context of Koopman operator methods, can be used to model the CG dynamics and evaluate its kinetic properties, such as transition timescales. Finally, we combine our approach with thermodynamic interpolation (TI), a generative approach to transform samples between thermodynamic conditions, to extend the scope of the approach across thermodynamic states without repeated numerical simulations. Using a two-dimensional model system, we demonstrate that the proposed method allows to accurately capture the thermodynamic and kinetic properties of the full-space model.",
      "authors": [
        "Vahid Nateghi",
        "Lara Neureither",
        "Selma Moqvist",
        "Carsten Hartmann",
        "Simon Olsson",
        "Feliks Nüske"
      ],
      "published_date": "2025-12-03",
      "full_text": "Consistent Projection of Langevin Dynamics: Preserving Thermodynamics and Kinetics in Coarse-Grained Models\n\nCoarse graining (CG) is an important task for efficient modeling and simulation of complex multi-scale systems, such as the conformational dynamics of biomolecules. This work presents a projection-based coarse-graining formalism for general underdamped Langevin dynamics. Following the Zwanzig projection approach, we derive a closed-form expression for the coarse grained dynamics. In addition, we show how the generator Extended Dynamic Mode Decomposition (gEDMD) method, which was developed in the context of Koopman operator methods, can be used to model the CG dynamics and evaluate its kinetic properties, such as transition timescales. Finally, we combine our approach with thermodynamic interpolation (TI), a generative approach to transform samples between thermodynamic conditions, to extend the scope of the approach across thermodynamic states without repeated numerical simulations. Using a two-dimensional model system, we demonstrate that the proposed method allows to accurately capture the thermodynamic and kinetic properties of the full-space model."
    },
    {
      "id": "2512.03496v1",
      "title": "Constraint-Preserving High-Order Compact OEDG Method for Spherically Symmetric Einstein-Euler System",
      "abstract": "Numerical simulation of the spherically symmetric Einstein--Euler (EE) system faces severe challenges due to the stringent physical admissibility constraints of relativistic fluids and the geometric singularities inherent in metric evolution. This paper proposes a high-order Constraint-Preserving (CP) compact Oscillation-Eliminating Discontinuous Galerkin (cOEDG) method specifically tailored to address these difficulties. The method integrates a scale-invariant oscillation-eliminating mechanism [M. Peng, Z. Sun, K. Wu, Math. Comp., 94: 1147--1198, 2025] into a compact Runge--Kutta DG framework. By characterizing the convex invariant region of the hydrodynamic subsystem with general barotropic equations of state, we prove that the proposed scheme preserves physical realizability (specifically, positive density and subluminal velocity) directly in terms of conservative variables, thereby eliminating the need for complex primitive-variable checks. To ensure the geometric validity of the spacetime, we introduce a bijective transformation of the metric potentials. Rather than evolving the constrained metric components directly, the scheme advances unconstrained auxiliary variables whose inverse mapping automatically enforces strict positivity and asymptotic bounds without any limiters. Combined with a compatible high-order boundary treatment, the resulting CPcOEDG method exhibits robust stability and design-order accuracy in capturing strong gravity-fluid interactions, as demonstrated by simulations of black hole accretion and relativistic shock waves.",
      "authors": [
        "Yuchen Huang",
        "Manting Peng",
        "Kailiang Wu"
      ],
      "published_date": "2025-12-03",
      "full_text": "Constraint-Preserving High-Order Compact OEDG Method for Spherically Symmetric Einstein-Euler System\n\nNumerical simulation of the spherically symmetric Einstein--Euler (EE) system faces severe challenges due to the stringent physical admissibility constraints of relativistic fluids and the geometric singularities inherent in metric evolution. This paper proposes a high-order Constraint-Preserving (CP) compact Oscillation-Eliminating Discontinuous Galerkin (cOEDG) method specifically tailored to address these difficulties. The method integrates a scale-invariant oscillation-eliminating mechanism [M. Peng, Z. Sun, K. Wu, Math. Comp., 94: 1147--1198, 2025] into a compact Runge--Kutta DG framework. By characterizing the convex invariant region of the hydrodynamic subsystem with general barotropic equations of state, we prove that the proposed scheme preserves physical realizability (specifically, positive density and subluminal velocity) directly in terms of conservative variables, thereby eliminating the need for complex primitive-variable checks. To ensure the geometric validity of the spacetime, we introduce a bijective transformation of the metric potentials. Rather than evolving the constrained metric components directly, the scheme advances unconstrained auxiliary variables whose inverse mapping automatically enforces strict positivity and asymptotic bounds without any limiters. Combined with a compatible high-order boundary treatment, the resulting CPcOEDG method exhibits robust stability and design-order accuracy in capturing strong gravity-fluid interactions, as demonstrated by simulations of black hole accretion and relativistic shock waves."
    },
    {
      "id": "2512.03476v1",
      "title": "ATHENA: Agentic Team for Hierarchical Evolutionary Numerical Algorithms",
      "abstract": "Bridging the gap between theoretical conceptualization and computational implementation is a major bottleneck in Scientific Computing (SciC) and Scientific Machine Learning (SciML). We introduce ATHENA (Agentic Team for Hierarchical Evolutionary Numerical Algorithms), an agentic framework designed as an Autonomous Lab to manage the end-to-end computational research lifecycle. Its core is the HENA loop, a knowledge-driven diagnostic process framed as a Contextual Bandit problem. Acting as an online learner, the system analyzes prior trials to select structural `actions' () from combinatorial spaces guided by expert blueprints (e.g., Universal Approximation, Physics-Informed constraints). These actions are translated into executable code () to generate scientific rewards (). ATHENA transcends standard automation: in SciC, it autonomously identifies mathematical symmetries for exact analytical solutions or derives stable numerical solvers where foundation models fail. In SciML, it performs deep diagnosis to tackle ill-posed formulations and combines hybrid symbolic-numeric workflows (e.g., coupling PINNs with FEM) to resolve multiphysics problems. The framework achieves super-human performance, reaching validation errors of . Furthermore, collaborative ``human-in-the-loop\" intervention allows the system to bridge stability gaps, improving results by an order of magnitude. This paradigm shift focuses from implementation mechanics to methodological innovation, accelerating scientific discovery.",
      "authors": [
        "Juan Diego Toscano",
        "Daniel T. Chen",
        "George Em Karniadakis"
      ],
      "published_date": "2025-12-03",
      "full_text": "ATHENA: Agentic Team for Hierarchical Evolutionary Numerical Algorithms\n\nBridging the gap between theoretical conceptualization and computational implementation is a major bottleneck in Scientific Computing (SciC) and Scientific Machine Learning (SciML). We introduce ATHENA (Agentic Team for Hierarchical Evolutionary Numerical Algorithms), an agentic framework designed as an Autonomous Lab to manage the end-to-end computational research lifecycle. Its core is the HENA loop, a knowledge-driven diagnostic process framed as a Contextual Bandit problem. Acting as an online learner, the system analyzes prior trials to select structural `actions' () from combinatorial spaces guided by expert blueprints (e.g., Universal Approximation, Physics-Informed constraints). These actions are translated into executable code () to generate scientific rewards (). ATHENA transcends standard automation: in SciC, it autonomously identifies mathematical symmetries for exact analytical solutions or derives stable numerical solvers where foundation models fail. In SciML, it performs deep diagnosis to tackle ill-posed formulations and combines hybrid symbolic-numeric workflows (e.g., coupling PINNs with FEM) to resolve multiphysics problems. The framework achieves super-human performance, reaching validation errors of . Furthermore, collaborative ``human-in-the-loop\" intervention allows the system to bridge stability gaps, improving results by an order of magnitude. This paradigm shift focuses from implementation mechanics to methodological innovation, accelerating scientific discovery."
    },
    {
      "id": "2512.03365v1",
      "title": "Generative Refinement:A New Paradigm for Determining Single Crystal Structures Directly from HKL Data",
      "abstract": "Single-crystal X-ray diffraction (SC-XRD) is the gold standard technique to characterize crystal structures in solid state. Despite significant advances in automation for structure solution, the refinement stage still depends heavily on expert intervention and subjective judgment, limiting accessibility and scalability. Herein, we introduce RefrActor, an end-to-end deep learning framework that enables crystal structure determination directly from HKL data. By coupling a physics-informed reciprocal-space encoder (ReciEncoder) with a symmetry-aware diffusion-based generator (StruDiffuser), RefrActor produces fully refined atomic models without requiring initial structural guesses or manual input. Comprehensive evaluations on the GenRef-10k benchmark demonstrates that RefrActor achieves low R1-factors across diverse systems, including low-symmetry, light-atom, and heavy-atom crystals. Case studies further confirm that RefrActor can correctly resolve hydrogen positions, elemental assignments, and moderate disorder. This work establishes a new data-driven paradigm for autonomous crystallographic analysis, offering a foundation for fully automated, high-throughput crystal structure determination.",
      "authors": [
        "Wen-Lin Luo",
        "Yi Yuan",
        "Cheng-Hui Li",
        "Yue Zhao",
        "Jing-Lin Zuo"
      ],
      "published_date": "2025-12-03",
      "full_text": "Generative Refinement:A New Paradigm for Determining Single Crystal Structures Directly from HKL Data\n\nSingle-crystal X-ray diffraction (SC-XRD) is the gold standard technique to characterize crystal structures in solid state. Despite significant advances in automation for structure solution, the refinement stage still depends heavily on expert intervention and subjective judgment, limiting accessibility and scalability. Herein, we introduce RefrActor, an end-to-end deep learning framework that enables crystal structure determination directly from HKL data. By coupling a physics-informed reciprocal-space encoder (ReciEncoder) with a symmetry-aware diffusion-based generator (StruDiffuser), RefrActor produces fully refined atomic models without requiring initial structural guesses or manual input. Comprehensive evaluations on the GenRef-10k benchmark demonstrates that RefrActor achieves low R1-factors across diverse systems, including low-symmetry, light-atom, and heavy-atom crystals. Case studies further confirm that RefrActor can correctly resolve hydrogen positions, elemental assignments, and moderate disorder. This work establishes a new data-driven paradigm for autonomous crystallographic analysis, offering a foundation for fully automated, high-throughput crystal structure determination."
    },
    {
      "id": "2512.03333v1",
      "title": "Sketch Tomography: Hybridizing Classical Shadow and Matrix Product State",
      "abstract": "We introduce Sketch Tomography, an efficient procedure for quantum state tomography based on the classical shadow protocol used for quantum observable estimations. The procedure applies to the case where the ground truth quantum state is a matrix product state (MPS). The density matrix of the ground truth state admits a tensor train ansatz as a result of the MPS assumption, and we estimate the tensor components of the ansatz through a series of observable estimations, thus outputting an approximation of the density matrix. The procedure is provably convergent with a sample complexity that scales quadratically in the system size. We conduct extensive numerical experiments to show that the procedure outputs an accurate approximation to the quantum state. For observable estimation tasks involving moderately large subsystems, we show that our procedure gives rise to a more accurate estimation than the classical shadow protocol. We also show that sketch tomography is more accurate in observable estimation than quantum states trained from the maximum likelihood estimation formulation.",
      "authors": [
        "Xun Tang",
        "Haoxuan Chen",
        "Yuehaw Khoo",
        "Lexing Ying"
      ],
      "published_date": "2025-12-03",
      "full_text": "Sketch Tomography: Hybridizing Classical Shadow and Matrix Product State\n\nWe introduce Sketch Tomography, an efficient procedure for quantum state tomography based on the classical shadow protocol used for quantum observable estimations. The procedure applies to the case where the ground truth quantum state is a matrix product state (MPS). The density matrix of the ground truth state admits a tensor train ansatz as a result of the MPS assumption, and we estimate the tensor components of the ansatz through a series of observable estimations, thus outputting an approximation of the density matrix. The procedure is provably convergent with a sample complexity that scales quadratically in the system size. We conduct extensive numerical experiments to show that the procedure outputs an accurate approximation to the quantum state. For observable estimation tasks involving moderately large subsystems, we show that our procedure gives rise to a more accurate estimation than the classical shadow protocol. We also show that sketch tomography is more accurate in observable estimation than quantum states trained from the maximum likelihood estimation formulation."
    },
    {
      "id": "2512.03330v1",
      "title": "Simpson variational integrator for nonlinear systems: a tutorial on the Lagrange top",
      "abstract": "This contribution presents an integration method based on the Simpson quadrature. The integrator is designed for finite-dimensional nonlinear mechanical systems that derive from variational principles. The action is discretized using quadratic finite elements interpolation of the state and Simpson's quadrature, leading to discrete motion equations. The scheme is implicit, symplectic, and fourth-order accurate. The proposed integrator is compared with the implicit midpoint variational integrator on two examples of systems with inseparable Hamiltonians. First, the example of the nonlinear double pendulum illustrates how the method can be applied to multibody systems. The analytical solution of the Lagrange top is then used as a reference to analyze accuracy, convergence, and precision of the numerical method. A reduced Lagrange top system is also proposed and solved with a classical fourth-order method. Its solution is compared with the Simpson solution of the complete system, and the convergence order of the difference between both is consistent with the order of the classical method.",
      "authors": [
        "Juan Antonio Rojas-Quintero",
        "François Dubois",
        "Frédéric Jourdan"
      ],
      "published_date": "2025-12-03",
      "full_text": "Simpson variational integrator for nonlinear systems: a tutorial on the Lagrange top\n\nThis contribution presents an integration method based on the Simpson quadrature. The integrator is designed for finite-dimensional nonlinear mechanical systems that derive from variational principles. The action is discretized using quadratic finite elements interpolation of the state and Simpson's quadrature, leading to discrete motion equations. The scheme is implicit, symplectic, and fourth-order accurate. The proposed integrator is compared with the implicit midpoint variational integrator on two examples of systems with inseparable Hamiltonians. First, the example of the nonlinear double pendulum illustrates how the method can be applied to multibody systems. The analytical solution of the Lagrange top is then used as a reference to analyze accuracy, convergence, and precision of the numerical method. A reduced Lagrange top system is also proposed and solved with a classical fourth-order method. Its solution is compared with the Simpson solution of the complete system, and the convergence order of the difference between both is consistent with the order of the classical method."
    },
    {
      "id": "2512.03269v1",
      "title": "Understanding cold electron impact on parallel-propagating whistler chorus waves via moment-based quasilinear theory",
      "abstract": "Earth's magnetosphere hosts a wide range of collisionless particle populations that interact through various wave-particle processes. Among these, cold electrons, with energies below 100eV, often dominate the plasma density but remain poorly characterized due to measurement challenges such as spacecraft charging and photoelectron contamination. Understanding the contribution of these cold populations to wave-particle interaction is of significant interest. Recent kinetic simulations identified a secondary drift-driven instability in which parallel-propagating whistler-mode chorus waves excite oblique electrostatic whistler waves near the resonance cone and Bernstein-mode turbulence. These secondary modes enable a new channel of energy transfer from the parallel-propagating whistler wave to the cold electrons. In this work, we develop a moment-based quasilinear theory of the secondary instabilities to quantify such energy exchange. Our results show that these secondary instabilities persist for a wide range of parameters and, in many cases, lead to nearly complete damping of the primary wave. Such secondary instability might limit the amplitude of parallel-propagating whistler waves in Earth's magnetosphere and might explain why high-amplitude oblique whistler or electron Bernstein waves are rarely observed simultaneously with high-amplitude field-aligned whistler waves in the inner magnetosphere.",
      "authors": [
        "Opal Issan",
        "Vadim Roytershteyn",
        "Gian Luca Delzanno",
        "Salomon Janhunen"
      ],
      "published_date": "2025-12-02",
      "full_text": "Understanding cold electron impact on parallel-propagating whistler chorus waves via moment-based quasilinear theory\n\nEarth's magnetosphere hosts a wide range of collisionless particle populations that interact through various wave-particle processes. Among these, cold electrons, with energies below 100eV, often dominate the plasma density but remain poorly characterized due to measurement challenges such as spacecraft charging and photoelectron contamination. Understanding the contribution of these cold populations to wave-particle interaction is of significant interest. Recent kinetic simulations identified a secondary drift-driven instability in which parallel-propagating whistler-mode chorus waves excite oblique electrostatic whistler waves near the resonance cone and Bernstein-mode turbulence. These secondary modes enable a new channel of energy transfer from the parallel-propagating whistler wave to the cold electrons. In this work, we develop a moment-based quasilinear theory of the secondary instabilities to quantify such energy exchange. Our results show that these secondary instabilities persist for a wide range of parameters and, in many cases, lead to nearly complete damping of the primary wave. Such secondary instability might limit the amplitude of parallel-propagating whistler waves in Earth's magnetosphere and might explain why high-amplitude oblique whistler or electron Bernstein waves are rarely observed simultaneously with high-amplitude field-aligned whistler waves in the inner magnetosphere."
    },
    {
      "id": "2512.03149v1",
      "title": "A high-order regularized delta-Chebyshev method for computing spectral densities",
      "abstract": "We introduce a numerical method for computing spectral densities, and apply it to the evaluation of the local density of states (LDOS) of sparse Hamiltonians derived from tight-binding models. The approach, which we call the high-order delta-Chebyshev method, can be viewed as a variant of the popular regularized Chebyshev kernel polynomial method (KPM), but it uses a high-order accurate approximation of the -function to achieve rapid convergence to the thermodynamic limit for smooth spectral densities. The costly computational steps are identical to those for KPM, with high-order accuracy achieved by an inexpensive post-processing procedure. We apply the algorithm to tight-binding models of graphene and twisted bilayer graphene, demonstrating high-order convergence to the LDOS at non-singular points.",
      "authors": [
        "Jinjing Yi",
        "Daniel Massatt",
        "Andrew Horning",
        "Mitchell Luskin",
        "J. H. Pixley",
        "Jason Kaye"
      ],
      "published_date": "2025-12-02",
      "full_text": "A high-order regularized delta-Chebyshev method for computing spectral densities\n\nWe introduce a numerical method for computing spectral densities, and apply it to the evaluation of the local density of states (LDOS) of sparse Hamiltonians derived from tight-binding models. The approach, which we call the high-order delta-Chebyshev method, can be viewed as a variant of the popular regularized Chebyshev kernel polynomial method (KPM), but it uses a high-order accurate approximation of the -function to achieve rapid convergence to the thermodynamic limit for smooth spectral densities. The costly computational steps are identical to those for KPM, with high-order accuracy achieved by an inexpensive post-processing procedure. We apply the algorithm to tight-binding models of graphene and twisted bilayer graphene, demonstrating high-order convergence to the LDOS at non-singular points."
    },
    {
      "id": "2512.02904v1",
      "title": "Towards a fully differentiable digital twin for solar cells",
      "abstract": "Maximizing energy yield (EY) - the total electric energy generated by a solar cell within a year at a specific location - is crucial in photovoltaics (PV), especially for emerging technologies. Computational methods provide the necessary insights and guidance for future research. However, existing simulations typically focus on only isolated aspects of solar cells. This lack of consistency highlights the need for a framework unifying all computational levels, from material to cell properties, for accurate prediction and optimization of EY prediction. To address this challenge, a differentiable digital twin, Sol(Di)T, is introduced to enable comprehensive end-to-end optimization of solar cells. The workflow starts with material properties and morphological processing parameters, followed by optical and electrical simulations. Finally, climatic conditions and geographic location are incorporated to predict the EY. Each step is either intrinsically differentiable or replaced with a machine-learned surrogate model, enabling not only accurate EY prediction but also gradient-based optimization with respect to input parameters. Consequently, Sol(Di)T extends EY predictions to previously unexplored conditions. Demonstrated for an organic solar cell, the proposed framework marks a significant step towards tailoring solar cells for specific applications while ensuring maximal performance.",
      "authors": [
        "Marie Louise Schubert",
        "Houssam Metni",
        "Jan David Fischbach",
        "Benedikt Zerulla",
        "Marjan Krstić",
        "Ulrich W. Paetzold",
        "Seyedamir Orooji",
        "Olivier J. J. Ronsin",
        "Yasin Ameslon",
        "Jens Harting",
        "Thomas Kirchartz",
        "Sandheep Ravishankar",
        "Chris Dreessen",
        "Eunchi Kim",
        "Christian Sprau",
        "Mohamed Hussein",
        "Alexander Colsmann",
        "Karen Forberich",
        "Klaus Jäger",
        "Pascal Friederich",
        "Carsten Rockstuhl"
      ],
      "published_date": "2025-12-02",
      "full_text": "Towards a fully differentiable digital twin for solar cells\n\nMaximizing energy yield (EY) - the total electric energy generated by a solar cell within a year at a specific location - is crucial in photovoltaics (PV), especially for emerging technologies. Computational methods provide the necessary insights and guidance for future research. However, existing simulations typically focus on only isolated aspects of solar cells. This lack of consistency highlights the need for a framework unifying all computational levels, from material to cell properties, for accurate prediction and optimization of EY prediction. To address this challenge, a differentiable digital twin, Sol(Di)T, is introduced to enable comprehensive end-to-end optimization of solar cells. The workflow starts with material properties and morphological processing parameters, followed by optical and electrical simulations. Finally, climatic conditions and geographic location are incorporated to predict the EY. Each step is either intrinsically differentiable or replaced with a machine-learned surrogate model, enabling not only accurate EY prediction but also gradient-based optimization with respect to input parameters. Consequently, Sol(Di)T extends EY predictions to previously unexplored conditions. Demonstrated for an organic solar cell, the proposed framework marks a significant step towards tailoring solar cells for specific applications while ensuring maximal performance."
    },
    {
      "id": "2512.02864v1",
      "title": "Modulation of DNA rheology by a transcription factor that forms aging microgels",
      "abstract": "Proteins and nucleic acids form non-Newtonian liquids with complex rheological properties that contribute to their function in vivo. Here we investigate the rheology of the transcription factor NANOG, a key protein in sustaining embryonic stem cell self-renewal. We discover that at high concentrations NANOG forms macroscopic aging gels through its intrinsically disordered tryptophan-rich domain. By combining molecular dynamics simulations, mass photometry and Cryo-EM, we also discover that NANOG forms self-limiting micelle-like clusters which expose their DNA-binding domains. In dense solutions of DNA, NANOG micelle-like structures stabilize intermolecular entanglements and crosslinks, forming microgel-like structures. Our findings suggest that NANOG may contribute to regulate gene expression in a unconventional way: by restricting and stabilizing genome dynamics at key transcriptional sites through the formation of an aging microgel-like structure, potentially enabling mechanical memory in the gene network.",
      "authors": [
        "Amandine Hong-Minh",
        "Yair Augusto Gutiérrez Fosado",
        "Abbie Guild",
        "Nicholas Mullin",
        "Laura Spagnolo",
        "Ian Chambers",
        "Davide Michieletto"
      ],
      "published_date": "2025-12-02",
      "full_text": "Modulation of DNA rheology by a transcription factor that forms aging microgels\n\nProteins and nucleic acids form non-Newtonian liquids with complex rheological properties that contribute to their function in vivo. Here we investigate the rheology of the transcription factor NANOG, a key protein in sustaining embryonic stem cell self-renewal. We discover that at high concentrations NANOG forms macroscopic aging gels through its intrinsically disordered tryptophan-rich domain. By combining molecular dynamics simulations, mass photometry and Cryo-EM, we also discover that NANOG forms self-limiting micelle-like clusters which expose their DNA-binding domains. In dense solutions of DNA, NANOG micelle-like structures stabilize intermolecular entanglements and crosslinks, forming microgel-like structures. Our findings suggest that NANOG may contribute to regulate gene expression in a unconventional way: by restricting and stabilizing genome dynamics at key transcriptional sites through the formation of an aging microgel-like structure, potentially enabling mechanical memory in the gene network."
    },
    {
      "id": "2512.02674v1",
      "title": "Rational regulation strategies of interstitial localized electrons in electride: A density functional theory study",
      "abstract": "As a class of electron-rich materials, electrides demonstrate promising applications in many fields. However, the required high pressure restricts the practical applications to some extent. This study reveals that the unique feature of electride, i.e., the localization of interstitial electrons, can be greatly enhanced and tuned by self-defective doping, applying tensile/compressive stress, or shear stress. Moreover, the requirement of orbital orthogonality between the valence and core electron wave functions, as well as the Pauli exclusion principle, should be the driven force for the electron interstitial localization; and the exertion of external pressure modifies the available space to accommodate the electronic wave functions, thus enhances the interstitial localization. These discoveries lay down the ground for searching for promising electrides that are practicable at ambient conditions.",
      "authors": [
        "L. Zhang",
        "D. Wang",
        "H. Wang",
        "J. Li",
        "Y. F. Wang",
        "Q. Wu",
        "Hua Y. Geng"
      ],
      "published_date": "2025-12-02",
      "full_text": "Rational regulation strategies of interstitial localized electrons in electride: A density functional theory study\n\nAs a class of electron-rich materials, electrides demonstrate promising applications in many fields. However, the required high pressure restricts the practical applications to some extent. This study reveals that the unique feature of electride, i.e., the localization of interstitial electrons, can be greatly enhanced and tuned by self-defective doping, applying tensile/compressive stress, or shear stress. Moreover, the requirement of orbital orthogonality between the valence and core electron wave functions, as well as the Pauli exclusion principle, should be the driven force for the electron interstitial localization; and the exertion of external pressure modifies the available space to accommodate the electronic wave functions, thus enhances the interstitial localization. These discoveries lay down the ground for searching for promising electrides that are practicable at ambient conditions."
    },
    {
      "id": "2512.02666v1",
      "title": "Efficient Simulation of the 2D Hubbard Model via Hilbert Space-Filling Curve Mapping",
      "abstract": "We investigate tensor network simulations of the two-dimensional Hubbard model by mapping the lattice onto a one-dimensional chain using space-filling curves. In particular, we focus on the Hilbert curve, whose locality-preserving structure minimizes the range of effective interactions in the mapped model. This enables a more compact matrix product state (MPS) representation compared to conventional snake mapping. Through systematic benchmarks, we show that the Hilbert curve consistently yields lower ground-state energies at fixed bond dimension, with the advantage increasing for larger system sizes and in physically relevant interaction regimes. Our implementation reaches clusters up to  sites with open and periodic boundary conditions, delivering reliable ground-state energies and correlation functions in agreement with established results, but at significantly reduced computational cost. These findings establish space-filling curve mappings, particularly the Hilbert curve, as a powerful tool for extending tensor-network studies of strongly correlated two-dimensional quantum systems beyond the limits accessible with standard approaches.",
      "authors": [
        "Ashkan Abedi",
        "Vittorio Giovannetti",
        "Dario De Santis"
      ],
      "published_date": "2025-12-02",
      "full_text": "Efficient Simulation of the 2D Hubbard Model via Hilbert Space-Filling Curve Mapping\n\nWe investigate tensor network simulations of the two-dimensional Hubbard model by mapping the lattice onto a one-dimensional chain using space-filling curves. In particular, we focus on the Hilbert curve, whose locality-preserving structure minimizes the range of effective interactions in the mapped model. This enables a more compact matrix product state (MPS) representation compared to conventional snake mapping. Through systematic benchmarks, we show that the Hilbert curve consistently yields lower ground-state energies at fixed bond dimension, with the advantage increasing for larger system sizes and in physically relevant interaction regimes. Our implementation reaches clusters up to  sites with open and periodic boundary conditions, delivering reliable ground-state energies and correlation functions in agreement with established results, but at significantly reduced computational cost. These findings establish space-filling curve mappings, particularly the Hilbert curve, as a powerful tool for extending tensor-network studies of strongly correlated two-dimensional quantum systems beyond the limits accessible with standard approaches."
    },
    {
      "id": "2512.02516v1",
      "title": "Improved Ising Meson Spectroscopy Simulation on a Noisy Digital Quantum Device",
      "abstract": "The transverse-field Ising model serves as a paradigm for studying confinement and excitation spectra, particularly the emergence of  symmetry near criticality. However, experimentally resolving the Ising meson spectroscopy required to verify these symmetries is challenging on near-term quantum hardware due to the depth of circuits required for real-time evolution. Here, we demonstrate improved spectroscopy of confined excitations using two distinct error-resilient circuit construction techniques on the IBM Torino device: first-order Trotter decomposition utilizing native fractional gates, and a tensor-network-based circuit compression via Riemannian optimization. By analyzing the Fourier spectrum of error-mitigated time-series data, we successfully identify key signatures of  symmetry despite hardware noise. These results validate the viability of both circuit compression and hardware-efficient compilation for probing complex topological phenomena on NISQ devices.",
      "authors": [
        "Hao-Ti Hung",
        "Isabel Nha Minh Le",
        "Johannes Knolle",
        "Ying-Jer Kao"
      ],
      "published_date": "2025-12-02",
      "full_text": "Improved Ising Meson Spectroscopy Simulation on a Noisy Digital Quantum Device\n\nThe transverse-field Ising model serves as a paradigm for studying confinement and excitation spectra, particularly the emergence of  symmetry near criticality. However, experimentally resolving the Ising meson spectroscopy required to verify these symmetries is challenging on near-term quantum hardware due to the depth of circuits required for real-time evolution. Here, we demonstrate improved spectroscopy of confined excitations using two distinct error-resilient circuit construction techniques on the IBM Torino device: first-order Trotter decomposition utilizing native fractional gates, and a tensor-network-based circuit compression via Riemannian optimization. By analyzing the Fourier spectrum of error-mitigated time-series data, we successfully identify key signatures of  symmetry despite hardware noise. These results validate the viability of both circuit compression and hardware-efficient compilation for probing complex topological phenomena on NISQ devices."
    },
    {
      "id": "2512.02308v1",
      "title": "Geant4 Modeling of Energy and Charge Deposition in Satellites Solar Cells",
      "abstract": "Understanding radiation effects in spacecraft components is critical for predicting long-term performance degradation. In this work, a Geant4 Monte Carlo model is developed to compute charge and energy deposition in satellite solar-cell materials exposed to electrons, protons, and X-ray environments. The model is validated against published experimental and computational benchmarks and shows strong agreement across multiple energy ranges. Power-density deposition profiles are then evaluated for a multi-layer solar-cell structure under blackbody soft X-rays, mono-energetic X-rays, and high-energy electrons. The results show that low-energy X-rays dominate damage in surface layers, while high-energy particles penetrate deeper into semiconductor and substrate layers, posing risk to electronic components. These findings highlight the importance of radiation-specific shielding strategies for space solar-cell design.",
      "authors": [
        "Youssef Abouhussien",
        "Gennady Miloshevsky"
      ],
      "published_date": "2025-12-02",
      "full_text": "Geant4 Modeling of Energy and Charge Deposition in Satellites Solar Cells\n\nUnderstanding radiation effects in spacecraft components is critical for predicting long-term performance degradation. In this work, a Geant4 Monte Carlo model is developed to compute charge and energy deposition in satellite solar-cell materials exposed to electrons, protons, and X-ray environments. The model is validated against published experimental and computational benchmarks and shows strong agreement across multiple energy ranges. Power-density deposition profiles are then evaluated for a multi-layer solar-cell structure under blackbody soft X-rays, mono-energetic X-rays, and high-energy electrons. The results show that low-energy X-rays dominate damage in surface layers, while high-energy particles penetrate deeper into semiconductor and substrate layers, posing risk to electronic components. These findings highlight the importance of radiation-specific shielding strategies for space solar-cell design."
    },
    {
      "id": "2512.02158v1",
      "title": "Out-of-equilibrium modeling of lyotropic liquid crystals: from binary simulations to multi-component theory",
      "abstract": "We present a thermodynamically consistent theoretical framework for lyotropic liquid crystals (LCs) based on the GENERIC (General Equation for the Non-Equilibrium Reversible-Irreversible Coupling) formalism. This formalism ensures conservation of energy and production of entropy, while coupling concentration, momentum balance, and liquid crystalline order. Starting from a binary nematic-isotropic mixture, we derive a theory for these key variables, which is then extended to multi-component systems. The binary equations are solved numerically using a Julia-based solver that relies on an upwind finite-difference scheme, enabling stable and efficient simulations capable of handling multiple time scales while satisfying fundamental mathematical constraints. The results of simulations are consistent with experimental observations of topological core defects in chromonic LCs, as well as flow-driven droplet shape transitions under Couette and Poiseuille flows. This work provides a platform for simulations of multi-component lyotropic LCs that can be extended to systems with multiple interfaces, active materials, and materials subject to external fields.",
      "authors": [
        "Jonathan Salmerón-Hernández",
        "Pablo Zubieta-Rico",
        "Juan de Pablo"
      ],
      "published_date": "2025-12-01",
      "full_text": "Out-of-equilibrium modeling of lyotropic liquid crystals: from binary simulations to multi-component theory\n\nWe present a thermodynamically consistent theoretical framework for lyotropic liquid crystals (LCs) based on the GENERIC (General Equation for the Non-Equilibrium Reversible-Irreversible Coupling) formalism. This formalism ensures conservation of energy and production of entropy, while coupling concentration, momentum balance, and liquid crystalline order. Starting from a binary nematic-isotropic mixture, we derive a theory for these key variables, which is then extended to multi-component systems. The binary equations are solved numerically using a Julia-based solver that relies on an upwind finite-difference scheme, enabling stable and efficient simulations capable of handling multiple time scales while satisfying fundamental mathematical constraints. The results of simulations are consistent with experimental observations of topological core defects in chromonic LCs, as well as flow-driven droplet shape transitions under Couette and Poiseuille flows. This work provides a platform for simulations of multi-component lyotropic LCs that can be extended to systems with multiple interfaces, active materials, and materials subject to external fields."
    },
    {
      "id": "2512.01940v1",
      "title": "Comparison of shock compaction models for granular materials: P -α model and mesoscale simulation",
      "abstract": "This study examines particle velocity measurements in granular sugar to evaluate the predictive accuracy of two computational models for weak shock under flyer-plate impact: the continuum-based P-alpha Menko model and mesoscale simulations with explicit particle and pore representations. Using flyer-plate impact experiments as a benchmark, we show that both two-dimensional (2D) models can reproduce the measured particle velocity histories, but through fundamentally different mechanisms. In the P-alpha framework, applying a pressure-dependent yield strength is essential to capture the particle velocity evolution, though calibration of other constitutive parameters, such as crush-out pressure, still strongly influences the response. In contrast, mesoscale simulations are less sensitive to parameter tuning and rely critically on the physical state variable of porosity, represented in 2D as an equivalent measure of the 3D specimen. Together, these results establish that their mechanical interpretations differ: continuum parameters act as effective surrogates for grain-scale physics, whereas mesoscale modeling reveals porosity as the dominant control of macroscopic wave onset.",
      "authors": [
        "Dawa Seo",
        "Darby J. Luscher",
        "Nitin Daphalapurkar"
      ],
      "published_date": "2025-12-01",
      "full_text": "Comparison of shock compaction models for granular materials: P -α model and mesoscale simulation\n\nThis study examines particle velocity measurements in granular sugar to evaluate the predictive accuracy of two computational models for weak shock under flyer-plate impact: the continuum-based P-alpha Menko model and mesoscale simulations with explicit particle and pore representations. Using flyer-plate impact experiments as a benchmark, we show that both two-dimensional (2D) models can reproduce the measured particle velocity histories, but through fundamentally different mechanisms. In the P-alpha framework, applying a pressure-dependent yield strength is essential to capture the particle velocity evolution, though calibration of other constitutive parameters, such as crush-out pressure, still strongly influences the response. In contrast, mesoscale simulations are less sensitive to parameter tuning and rely critically on the physical state variable of porosity, represented in 2D as an equivalent measure of the 3D specimen. Together, these results establish that their mechanical interpretations differ: continuum parameters act as effective surrogates for grain-scale physics, whereas mesoscale modeling reveals porosity as the dominant control of macroscopic wave onset."
    },
    {
      "id": "2512.01888v1",
      "title": "Domain-Decomposed Graph Neural Network Surrogate Modeling for Ice Sheets",
      "abstract": "Accurate yet efficient surrogate models are essential for large-scale simulations of partial differential equations (PDEs), particularly for uncertainty quantification (UQ) tasks that demand hundreds or thousands of evaluations. We develop a physics-inspired graph neural network (GNN) surrogate that operates directly on unstructured meshes and leverages the flexibility of graph attention. To improve both training efficiency and generalization properties of the model, we introduce a domain decomposition (DD) strategy that partitions the mesh into subdomains, trains local GNN surrogates in parallel, and aggregates their predictions. We then employ transfer learning to fine-tune models across subdomains, accelerating training and improving accuracy in data-limited settings. Applied to ice sheet simulations, our approach accurately predicts full-field velocities on high-resolution meshes, substantially reduces training time relative to training a single global surrogate model, and provides a ripe foundation for UQ objectives. Our results demonstrate that graph-based DD, combined with transfer learning, provides a scalable and reliable pathway for training GNN surrogates on massive PDE-governed systems, with broad potential for application beyond ice sheet dynamics.",
      "authors": [
        "Adrienne M. Propp",
        "Mauro Perego",
        "Eric C. Cyr",
        "Anthony Gruber",
        "Amanda A. Howard",
        "Alexander Heinlein",
        "Panos Stinis",
        "Daniel M. Tartakovsky"
      ],
      "published_date": "2025-12-01",
      "full_text": "Domain-Decomposed Graph Neural Network Surrogate Modeling for Ice Sheets\n\nAccurate yet efficient surrogate models are essential for large-scale simulations of partial differential equations (PDEs), particularly for uncertainty quantification (UQ) tasks that demand hundreds or thousands of evaluations. We develop a physics-inspired graph neural network (GNN) surrogate that operates directly on unstructured meshes and leverages the flexibility of graph attention. To improve both training efficiency and generalization properties of the model, we introduce a domain decomposition (DD) strategy that partitions the mesh into subdomains, trains local GNN surrogates in parallel, and aggregates their predictions. We then employ transfer learning to fine-tune models across subdomains, accelerating training and improving accuracy in data-limited settings. Applied to ice sheet simulations, our approach accurately predicts full-field velocities on high-resolution meshes, substantially reduces training time relative to training a single global surrogate model, and provides a ripe foundation for UQ objectives. Our results demonstrate that graph-based DD, combined with transfer learning, provides a scalable and reliable pathway for training GNN surrogates on massive PDE-governed systems, with broad potential for application beyond ice sheet dynamics."
    },
    {
      "id": "2512.01765v1",
      "title": "Granite sliding on granite: friction, wear rates, surface topography, and the scale-dependence of rate-state effects",
      "abstract": "We study tribological granite-granite contacts as a model for tectonic faulting, combining experiments, theory, and molecular dynamics simulations. The high friction in this system is not dominated by particulate wear or plowing, as frequently assumed, but by cold welding within plastically deformed asperity junctions. We base this conclusion on the observation that wear is repeatedly high after cleaning contacts but decreases as gouge accumulates, while friction shows the opposite trend. Moreover, adding water reduces wear by a factor of ten but barely decreases friction. Thermal and rate-dependent effects - central to most earthquake models-are negligible: friction remains unchanged between -40°C and 20°C, across abrupt velocity steps, and after hours of stationary contact. The absence of rate-state effects in our macroscopic samples is rationalized by the scale-dependence of pre-slip. The evolution of surface topography shows that quartz grains become locally smooth, with height spectra isotropic for wavelength below 10 microns but anisotropic at longer wavelengths, similar to natural faults. The resulting gouge particles have the usual characteristic sizes near 100 nm. Molecular dynamics simulations of a rigid, amorphous silica tip sliding on α-quartz reproduce not only similar friction coefficients near unity but also other experimentally observed features, including stress-introduced transitions to phases observed in post-mortem faults, as well as theoretical estimates of local flash temperatures. Additionally, they reveal a marked decrease of interfacial shear strength above 600°C. The overall correspondence between experiments, simulations, theory, and field observations indicates that our model system captures essential aspects of rock friction.",
      "authors": [
        "Sergey V. Sukhomlinov",
        "Martin H. Müser",
        "B. N. J. Persson"
      ],
      "published_date": "2025-12-01",
      "full_text": "Granite sliding on granite: friction, wear rates, surface topography, and the scale-dependence of rate-state effects\n\nWe study tribological granite-granite contacts as a model for tectonic faulting, combining experiments, theory, and molecular dynamics simulations. The high friction in this system is not dominated by particulate wear or plowing, as frequently assumed, but by cold welding within plastically deformed asperity junctions. We base this conclusion on the observation that wear is repeatedly high after cleaning contacts but decreases as gouge accumulates, while friction shows the opposite trend. Moreover, adding water reduces wear by a factor of ten but barely decreases friction. Thermal and rate-dependent effects - central to most earthquake models-are negligible: friction remains unchanged between -40°C and 20°C, across abrupt velocity steps, and after hours of stationary contact. The absence of rate-state effects in our macroscopic samples is rationalized by the scale-dependence of pre-slip. The evolution of surface topography shows that quartz grains become locally smooth, with height spectra isotropic for wavelength below 10 microns but anisotropic at longer wavelengths, similar to natural faults. The resulting gouge particles have the usual characteristic sizes near 100 nm. Molecular dynamics simulations of a rigid, amorphous silica tip sliding on α-quartz reproduce not only similar friction coefficients near unity but also other experimentally observed features, including stress-introduced transitions to phases observed in post-mortem faults, as well as theoretical estimates of local flash temperatures. Additionally, they reveal a marked decrease of interfacial shear strength above 600°C. The overall correspondence between experiments, simulations, theory, and field observations indicates that our model system captures essential aspects of rock friction."
    },
    {
      "id": "2512.01631v1",
      "title": "First-principles screening of materials with extreme effective masses",
      "abstract": "The effective mass of charge carriers is a fundamental descriptor of the electronic structure of materials, and can be used to assess performance in electronics applications, or to screen for thermoelectrics and transparent conductors. Here, we perform a high-throughput computational screening of approximately 20,000 experimentally known three-dimensional stoichiometric inorganics obtained from the Materials Cloud 3D structure database. By combining density-functional theory calculations and maximally localized Wannier functions, we are able to compute the full conductivity effective mass tensor for electrons and holes from the Boltzmann transport equation in the constant relaxation-time approximation. This approach captures the effects of band non-parabolicity, anisotropy, and valley multiplicity that would be neglected by standard parabolic fittings. The screening identifies a curated set of candidates exhibiting extreme electronic properties, from ultra-low to ultra-large effective masses, these latter associated with flat-band physics. We validate the workflow by recovering established high-mobility semiconductors and highlight promising novel candidates. Furthermore, we classify materials by their mass anisotropy and discuss the physical limits of defining a conductivity effective mass in narrow-gap regimes at room temperature. The resulting dataset provides a systematic roadmap to search for high-performance materials in novel chemical spaces.",
      "authors": [
        "Szymon Błazucki",
        "Junfeng Qiao",
        "Nicola Marzari"
      ],
      "published_date": "2025-12-01",
      "full_text": "First-principles screening of materials with extreme effective masses\n\nThe effective mass of charge carriers is a fundamental descriptor of the electronic structure of materials, and can be used to assess performance in electronics applications, or to screen for thermoelectrics and transparent conductors. Here, we perform a high-throughput computational screening of approximately 20,000 experimentally known three-dimensional stoichiometric inorganics obtained from the Materials Cloud 3D structure database. By combining density-functional theory calculations and maximally localized Wannier functions, we are able to compute the full conductivity effective mass tensor for electrons and holes from the Boltzmann transport equation in the constant relaxation-time approximation. This approach captures the effects of band non-parabolicity, anisotropy, and valley multiplicity that would be neglected by standard parabolic fittings. The screening identifies a curated set of candidates exhibiting extreme electronic properties, from ultra-low to ultra-large effective masses, these latter associated with flat-band physics. We validate the workflow by recovering established high-mobility semiconductors and highlight promising novel candidates. Furthermore, we classify materials by their mass anisotropy and discuss the physical limits of defining a conductivity effective mass in narrow-gap regimes at room temperature. The resulting dataset provides a systematic roadmap to search for high-performance materials in novel chemical spaces."
    },
    {
      "id": "2512.01627v1",
      "title": "Accelerated Machine Learning Force Field for Predicting Thermal Conductivity of Organic Liquids",
      "abstract": "The thermal conductivity of organic liquids is a vital parameter influencing various industrial and environmental applications, including energy conversion, electronics cooling, and chemical processing. However, atomistic simulation of thermal conductivity of organic liquids has been hindered by the limited accuracy of classical force fields and the huge computational demand of ab initio methods. In this work, we present a machine learning force field (MLFF)-based molecular dynamics simulation workflow to predict the thermal conductivity of 20 organic liquids. Here, we introduce the concept of differential attention into the MLFF architecture for enhanced learning ability, and we use density of the liquids to align the MLFF with experiments. As a result, this workflow achieves a mean absolute percentage error of 14% for the thermal conductivity of various organic liquids, significantly lower than that of the current off-the-shelf classical force field (78%). Furthermore, the MLFF is rewritten using Triton language to maximize simulation speed, enabling rapid prediction of thermal conductivity.",
      "authors": [
        "Wei Feng",
        "Siyuan Liu",
        "Hongyi Wang",
        "Zhenliang Mu",
        "Zhichen Pu",
        "Xu Han",
        "Tianze Zheng",
        "Zhenze Yang",
        "Zhi Wang",
        "Weihao Gao",
        "Yidan Cao",
        "Kuang Yu",
        "Sheng Gong",
        "Wen Yan"
      ],
      "published_date": "2025-12-01",
      "full_text": "Accelerated Machine Learning Force Field for Predicting Thermal Conductivity of Organic Liquids\n\nThe thermal conductivity of organic liquids is a vital parameter influencing various industrial and environmental applications, including energy conversion, electronics cooling, and chemical processing. However, atomistic simulation of thermal conductivity of organic liquids has been hindered by the limited accuracy of classical force fields and the huge computational demand of ab initio methods. In this work, we present a machine learning force field (MLFF)-based molecular dynamics simulation workflow to predict the thermal conductivity of 20 organic liquids. Here, we introduce the concept of differential attention into the MLFF architecture for enhanced learning ability, and we use density of the liquids to align the MLFF with experiments. As a result, this workflow achieves a mean absolute percentage error of 14% for the thermal conductivity of various organic liquids, significantly lower than that of the current off-the-shelf classical force field (78%). Furthermore, the MLFF is rewritten using Triton language to maximize simulation speed, enabling rapid prediction of thermal conductivity."
    },
    {
      "id": "2512.01579v1",
      "title": "The Spin-MInt Algorithm: an Accurate and Symplectic Propagator for the Spin-Mapping Representation of Nonadiabatic Dynamics",
      "abstract": "Mapping methods, including the Meyer-Miller-Stock-Thoss (MMST) mapping and spin-mapping, are commonly utilised to simulate nonadiabatic dynamics by propagating classical mapping variable trajectories. Recent work confirmed the Momentum Integral (MInt) algorithm is the only known symplectic algorithm for the MMST Hamiltonian. To our knowledge, no symplectic algorithm has been published for the spin-mapping representation without converting to MMST variables and utilising the MInt algorithm. Here, we present the Spin-MInt algorithm which directly propagates the spin-mapping variables. First, we consider a two-level system which maps onto a spin-vector on a Bloch sphere and determine that the Spin-MInt is a symplectic, symmetrical, second-order, time-reversible, angle invariant and geometric structure preserving algorithm. Despite spin-variables resulting in a non-invertible structure matrix, we rigorously prove the Spin-MInt is symplectic using a canonical variable transformation. Computationally, we find that the Spin-MInt and MInt algorithms are symplectic, satisfy Liouville's theorem, provide second-order energy conservation and are more accurate than a previously-published angle-based algorithm. The Spin-MInt is significantly faster than the MInt algorithm for two electronic states. Secondly, we extend this methodology to more than two electronic states and present accurate population results for a three-state morse potential. We believe this to be the first known symplectic algorithm for propagating the nonadiabatic spin-mapping Hamiltonian and one of the first rigorously symplectic algorithms in the case of non-trivial coupling between canonical and spin systems. These results should guide and improve future simulations.",
      "authors": [
        "Lauren E. Cook",
        "James R. Rampton",
        "Timothy J. H. Hele"
      ],
      "published_date": "2025-12-01",
      "full_text": "The Spin-MInt Algorithm: an Accurate and Symplectic Propagator for the Spin-Mapping Representation of Nonadiabatic Dynamics\n\nMapping methods, including the Meyer-Miller-Stock-Thoss (MMST) mapping and spin-mapping, are commonly utilised to simulate nonadiabatic dynamics by propagating classical mapping variable trajectories. Recent work confirmed the Momentum Integral (MInt) algorithm is the only known symplectic algorithm for the MMST Hamiltonian. To our knowledge, no symplectic algorithm has been published for the spin-mapping representation without converting to MMST variables and utilising the MInt algorithm. Here, we present the Spin-MInt algorithm which directly propagates the spin-mapping variables. First, we consider a two-level system which maps onto a spin-vector on a Bloch sphere and determine that the Spin-MInt is a symplectic, symmetrical, second-order, time-reversible, angle invariant and geometric structure preserving algorithm. Despite spin-variables resulting in a non-invertible structure matrix, we rigorously prove the Spin-MInt is symplectic using a canonical variable transformation. Computationally, we find that the Spin-MInt and MInt algorithms are symplectic, satisfy Liouville's theorem, provide second-order energy conservation and are more accurate than a previously-published angle-based algorithm. The Spin-MInt is significantly faster than the MInt algorithm for two electronic states. Secondly, we extend this methodology to more than two electronic states and present accurate population results for a three-state morse potential. We believe this to be the first known symplectic algorithm for propagating the nonadiabatic spin-mapping Hamiltonian and one of the first rigorously symplectic algorithms in the case of non-trivial coupling between canonical and spin systems. These results should guide and improve future simulations."
    },
    {
      "id": "2512.04066v1",
      "title": "Instantaneous Sobolev Regularization for Dissipative Bosonic Dynamics",
      "abstract": "We investigate quantum Markov semigroups on bosonic Fock space and identify a broad class of infinite-dimensional dissipative evolutions that exhibit instantaneous Sobolev-regularization. Motivated by stability problems in quantum computation, we show that for certain Lindblad operators that are polynomials of creation and annihilation operators, the resulting dynamics immediately transform any initial state into one with finite expectation in all powers of the number operator. A key application is in the bosonic cat code, where we obtain explicit estimates in the trace norm for the speed of convergence. These estimates sharpen existing perturbative bounds at both short and long times, offering new analytic tools for assessing stability and error suppression in bosonic quantum information processing. For example, we improve the strong exponential convergence of the (shifted) -photon dissipation to its fixed point to the uniform topology.",
      "authors": [
        "Pablo Costa Rico",
        "Paul Gondolf",
        "Tim Möbus"
      ],
      "published_date": "2025-12-03",
      "full_text": "Instantaneous Sobolev Regularization for Dissipative Bosonic Dynamics\n\nWe investigate quantum Markov semigroups on bosonic Fock space and identify a broad class of infinite-dimensional dissipative evolutions that exhibit instantaneous Sobolev-regularization. Motivated by stability problems in quantum computation, we show that for certain Lindblad operators that are polynomials of creation and annihilation operators, the resulting dynamics immediately transform any initial state into one with finite expectation in all powers of the number operator. A key application is in the bosonic cat code, where we obtain explicit estimates in the trace norm for the speed of convergence. These estimates sharpen existing perturbative bounds at both short and long times, offering new analytic tools for assessing stability and error suppression in bosonic quantum information processing. For example, we improve the strong exponential convergence of the (shifted) -photon dissipation to its fixed point to the uniform topology."
    },
    {
      "id": "2512.04058v1",
      "title": "Closing the problem of which causal structures of up to six total nodes have a classical-quantum gap",
      "abstract": "The discovery of Bell that there exist quantum correlations that cannot be reproduced classically is one of the most important in the foundations of quantum mechanics, as well as having practical implications. Bell's result was originally proven in a simple bipartite causal structure, but analogous results have also been shown in further causal structures. Here we study the only causal structure with six or fewer nodes in which the question of whether or not there exist quantum correlations that cannot be achieved classically was open. In this causal structure we show that such quantum correlations exist using a method that involves imposing additional restrictions on the correlations. This hence completes the picture of which causal structures of up to six nodes support non-classical quantum correlations. We also provide further illustrations of our method using other causal structures.",
      "authors": [
        "Shashaank Khanna",
        "Matthew Pusey",
        "Roger Colbeck"
      ],
      "published_date": "2025-12-03",
      "full_text": "Closing the problem of which causal structures of up to six total nodes have a classical-quantum gap\n\nThe discovery of Bell that there exist quantum correlations that cannot be reproduced classically is one of the most important in the foundations of quantum mechanics, as well as having practical implications. Bell's result was originally proven in a simple bipartite causal structure, but analogous results have also been shown in further causal structures. Here we study the only causal structure with six or fewer nodes in which the question of whether or not there exist quantum correlations that cannot be achieved classically was open. In this causal structure we show that such quantum correlations exist using a method that involves imposing additional restrictions on the correlations. This hence completes the picture of which causal structures of up to six nodes support non-classical quantum correlations. We also provide further illustrations of our method using other causal structures."
    },
    {
      "id": "2512.04028v1",
      "title": "Thermalization from quenching in coupled oscillators",
      "abstract": "We introduce a finite-time protocol that thermalizes a quantum harmonic oscillator, initially in its ground state, without requiring a macroscopic bath. The method uses a second oscillator as an effective environment and implements sudden quenches of the oscillator frequencies and coupling. Owing to the Gaussian nature of the dynamics, the thermalization condition reduces to three solvable equations, yielding exact analytic solutions for a dense discrete set of temperatures and numerical solutions in all other cases. Any target temperature can be approximated with arbitrary precision, with a trade-off between speed and accuracy. The simplicity of the protocol makes it a promising tool for rapid, controlled thermalization in quantum thermodynamics experiments and state preparation.",
      "authors": [
        "M. Harinarayanan",
        "Karthik Rajeev"
      ],
      "published_date": "2025-12-03",
      "full_text": "Thermalization from quenching in coupled oscillators\n\nWe introduce a finite-time protocol that thermalizes a quantum harmonic oscillator, initially in its ground state, without requiring a macroscopic bath. The method uses a second oscillator as an effective environment and implements sudden quenches of the oscillator frequencies and coupling. Owing to the Gaussian nature of the dynamics, the thermalization condition reduces to three solvable equations, yielding exact analytic solutions for a dense discrete set of temperatures and numerical solutions in all other cases. Any target temperature can be approximated with arbitrary precision, with a trade-off between speed and accuracy. The simplicity of the protocol makes it a promising tool for rapid, controlled thermalization in quantum thermodynamics experiments and state preparation."
    },
    {
      "id": "2512.04016v1",
      "title": "TARA Test-by-Adaptive-Ranks for Quantum Anomaly Detection with Conformal Prediction Guarantees",
      "abstract": "Quantum key distribution (QKD) security fundamentally relies on the ability to distinguish genuine quantum correlations from classical eavesdropper simulations, yet existing certification methods lack rigorous statistical guarantees under finite-sample conditions and adversarial scenarios. We introduce TARA (Test by Adaptive Ranks), a novel framework combining conformal prediction with sequential martingale testing for quantum anomaly detection that provides distribution-free validity guarantees. TARA offers two complementary approaches. TARA k, based on Kolmogorov Smirnov calibration against local hidden variable (LHV) null distributions, achieving ROC AUC = 0.96 for quantum-classical discrimination. And TARA-m, employing betting martingales for streaming detection with anytime valid type I error control that enables real time monitoring of quantum channels. We establish theoretical guarantees proving that under (context conditional) exchangeability, conformal p-values remain uniformly distributed even for strongly contextual quantum data, confirming that quantum contextuality does not break conformal prediction validity a result with implications beyond quantum certification to any application of distribution-free methods to nonclassical data. Extensive validation on both IBM Torino (superconducting, CHSH = 2.725) and IonQ Forte Enterprise (trapped ion, CHSH = 2.716) quantum processors demonstrates cross-platform robustness, achieving 36% security margins above the classical CHSH bound of 2. Critically, our framework reveals a methodological concern affecting quantum certification more broadly: same-distribution calibration can inflate detection performance by up to 44 percentage points compared to proper cross-distribution calibration, suggesting that prior quantum certification studies using standard train test splits may have systematically overestimated adversarial robustness.",
      "authors": [
        "Davut Emre Tasar",
        "Ceren Ocal Tasar"
      ],
      "published_date": "2025-12-03",
      "full_text": "TARA Test-by-Adaptive-Ranks for Quantum Anomaly Detection with Conformal Prediction Guarantees\n\nQuantum key distribution (QKD) security fundamentally relies on the ability to distinguish genuine quantum correlations from classical eavesdropper simulations, yet existing certification methods lack rigorous statistical guarantees under finite-sample conditions and adversarial scenarios. We introduce TARA (Test by Adaptive Ranks), a novel framework combining conformal prediction with sequential martingale testing for quantum anomaly detection that provides distribution-free validity guarantees. TARA offers two complementary approaches. TARA k, based on Kolmogorov Smirnov calibration against local hidden variable (LHV) null distributions, achieving ROC AUC = 0.96 for quantum-classical discrimination. And TARA-m, employing betting martingales for streaming detection with anytime valid type I error control that enables real time monitoring of quantum channels. We establish theoretical guarantees proving that under (context conditional) exchangeability, conformal p-values remain uniformly distributed even for strongly contextual quantum data, confirming that quantum contextuality does not break conformal prediction validity a result with implications beyond quantum certification to any application of distribution-free methods to nonclassical data. Extensive validation on both IBM Torino (superconducting, CHSH = 2.725) and IonQ Forte Enterprise (trapped ion, CHSH = 2.716) quantum processors demonstrates cross-platform robustness, achieving 36% security margins above the classical CHSH bound of 2. Critically, our framework reveals a methodological concern affecting quantum certification more broadly: same-distribution calibration can inflate detection performance by up to 44 percentage points compared to proper cross-distribution calibration, suggesting that prior quantum certification studies using standard train test splits may have systematically overestimated adversarial robustness."
    },
    {
      "id": "2512.03987v1",
      "title": "Fully quantum theory of strong-field driven tunable entangled multi-photon states in HHG",
      "abstract": "Quantum high-harmonic generation (HHG) is a growing field of research with capabilities of providing high photon-number entangled states of light. However, there is an open debate regarding the theory level required for correctly describing the quantum aspects of HHG emission, such as squeezing or entanglement. Previous approaches have employed non-interacting classical ensembles of trajectories, or perturbation theory utilizing the classical trajectories as a starting point, missing out key entanglement features. In this Letter, we develop a full quantum theory for entanglement measures in HHG solving exactly the light-matter interaction Hamiltonian and employ it for evaluating the entanglement between emitted photons of different harmonics. For the first time, we reach qualitative agreement of theory with recent experiments showing that the R entanglement parameter decreases with increasing laser power for below-threshold harmonics. Our results indicate that fine-tuning the laser power could enhance HHG entanglement features, which are observed to oscillate with the driving power and exhibit local non-classical maxima structures. Similarly, our theory predicts that the oscillatory behavior of entanglement observed for below-threshold harmonics also appears for entanglement involving above-threshold harmonics. We also show that the long-range behavior of driven electronic trajectories can qualitatively change the resulting entanglement. Lastly, we show that focal averaging over classical degrees of freedom, which has thus far been ignored in quantum HHG theories, plays a key role in entanglement measures and can change the qualitative behavior of observables. Our work establishes the state-of-the art in exploring entanglement features in HHG, and paves way for analysis and engineering of 'truly-quantum' multi-photon states in the XUV and ultrafast regime for more complex matter systems.",
      "authors": [
        "Sebastián de-la-Peña",
        "Heiko Appel",
        "Angel Rubio",
        "Ofer Neufeld"
      ],
      "published_date": "2025-12-03",
      "full_text": "Fully quantum theory of strong-field driven tunable entangled multi-photon states in HHG\n\nQuantum high-harmonic generation (HHG) is a growing field of research with capabilities of providing high photon-number entangled states of light. However, there is an open debate regarding the theory level required for correctly describing the quantum aspects of HHG emission, such as squeezing or entanglement. Previous approaches have employed non-interacting classical ensembles of trajectories, or perturbation theory utilizing the classical trajectories as a starting point, missing out key entanglement features. In this Letter, we develop a full quantum theory for entanglement measures in HHG solving exactly the light-matter interaction Hamiltonian and employ it for evaluating the entanglement between emitted photons of different harmonics. For the first time, we reach qualitative agreement of theory with recent experiments showing that the R entanglement parameter decreases with increasing laser power for below-threshold harmonics. Our results indicate that fine-tuning the laser power could enhance HHG entanglement features, which are observed to oscillate with the driving power and exhibit local non-classical maxima structures. Similarly, our theory predicts that the oscillatory behavior of entanglement observed for below-threshold harmonics also appears for entanglement involving above-threshold harmonics. We also show that the long-range behavior of driven electronic trajectories can qualitatively change the resulting entanglement. Lastly, we show that focal averaging over classical degrees of freedom, which has thus far been ignored in quantum HHG theories, plays a key role in entanglement measures and can change the qualitative behavior of observables. Our work establishes the state-of-the art in exploring entanglement features in HHG, and paves way for analysis and engineering of 'truly-quantum' multi-photon states in the XUV and ultrafast regime for more complex matter systems."
    },
    {
      "id": "2512.03984v1",
      "title": "Entanglement Detection with Rotationally Covariant Measurements - From Compton Scattering to Lemonade",
      "abstract": "The accurate and efficient detection of quantum entanglement remains a central challenge in quantum information science. In this work, we study the detection of entanglement of polarized photons for measurement devices that are solely specified by rotational symmetry. We derive explicit positive operator valued measures (POVMs) showing that from a quantum information perspective any such setting is classified by one real measurable parameter r. In Particular, we give a POVM formulation of the Klein--Nishina formula for Compton scattering of polarized photons. We provide an SDP-based entanglement certification method that operates on the full measured statistics and gives tight bounds, also considering semi-device independent scenarios. Furthermore, we show that, while Bell violations are impossible with rotationally covariant measurements, EPR steering can still be certified under one-sided symmetry constraints. Finally, we present a rotationally covariant showcase experiment, analyzing the scattering of polarized optical light in a selection of soft drinks. Our results suggest that lemonade-based detectors are suitable for entanglement detection.",
      "authors": [
        "Marlene Funck",
        "Ilija Funk",
        "Tizian Schmidt",
        "René Schwonnek"
      ],
      "published_date": "2025-12-03",
      "full_text": "Entanglement Detection with Rotationally Covariant Measurements - From Compton Scattering to Lemonade\n\nThe accurate and efficient detection of quantum entanglement remains a central challenge in quantum information science. In this work, we study the detection of entanglement of polarized photons for measurement devices that are solely specified by rotational symmetry. We derive explicit positive operator valued measures (POVMs) showing that from a quantum information perspective any such setting is classified by one real measurable parameter r. In Particular, we give a POVM formulation of the Klein--Nishina formula for Compton scattering of polarized photons. We provide an SDP-based entanglement certification method that operates on the full measured statistics and gives tight bounds, also considering semi-device independent scenarios. Furthermore, we show that, while Bell violations are impossible with rotationally covariant measurements, EPR steering can still be certified under one-sided symmetry constraints. Finally, we present a rotationally covariant showcase experiment, analyzing the scattering of polarized optical light in a selection of soft drinks. Our results suggest that lemonade-based detectors are suitable for entanglement detection."
    },
    {
      "id": "2512.03980v1",
      "title": "Quantum Diplomacy within the Southeast Asia Quantum Ecosystem",
      "abstract": "Amid the International Year of Quantum Science and Technology 2025 (IYQ 2025), a significant portion of global funding has been dedicated to various quantum initiatives, with over 30 countries announcing their respective quantum strategies. Within the Southeast Asia context, Singapore, Thailand, and the Philippines have launched their respective quantum strategies and roadmaps. Meanwhile, six out of eleven Southeast Asia countries have expressed interest in formulating a regional quantum ecosystem to pursue a set of common goals. Quantum technologies, though still in their infancy within the second quantum revolution, have advanced rapidly in recent years. Due to their dual-use nature, quantum technologies are considered emerging and disruptive, often raising concerns from the cybersecurity perspective. While several discussions regarding Malaysia's quantum initiative and strategy are ongoing, it is vital to broaden the conversation and position Malaysia within the regional ecosystem. This paper provides an overview of Malaysia's quantum landscape and a summary of the regional initiatives since the establishment of Southeast Asia Quantum Network. We then analyse Malaysia's strengths in quantum research and provide four recommendations to strengthen the regional ecosystem.",
      "authors": [
        "Pak Shen Choong",
        "Nurisya Mohd Shah",
        "Yung Szen Yap"
      ],
      "published_date": "2025-12-03",
      "full_text": "Quantum Diplomacy within the Southeast Asia Quantum Ecosystem\n\nAmid the International Year of Quantum Science and Technology 2025 (IYQ 2025), a significant portion of global funding has been dedicated to various quantum initiatives, with over 30 countries announcing their respective quantum strategies. Within the Southeast Asia context, Singapore, Thailand, and the Philippines have launched their respective quantum strategies and roadmaps. Meanwhile, six out of eleven Southeast Asia countries have expressed interest in formulating a regional quantum ecosystem to pursue a set of common goals. Quantum technologies, though still in their infancy within the second quantum revolution, have advanced rapidly in recent years. Due to their dual-use nature, quantum technologies are considered emerging and disruptive, often raising concerns from the cybersecurity perspective. While several discussions regarding Malaysia's quantum initiative and strategy are ongoing, it is vital to broaden the conversation and position Malaysia within the regional ecosystem. This paper provides an overview of Malaysia's quantum landscape and a summary of the regional initiatives since the establishment of Southeast Asia Quantum Network. We then analyse Malaysia's strengths in quantum research and provide four recommendations to strengthen the regional ecosystem."
    },
    {
      "id": "2512.03970v1",
      "title": "Non-radiative energy transfer between boron vacancies in hexagonal boron nitride and other 2D materials",
      "abstract": "Boron vacancies () in hexagonal boron nitride (hBN) have emerged as a promising platform for two-dimensional quantum sensors capable of operating at atomic-scale proximity. However, the mechanisms responsible for photoluminescence quenching in thin hBN sensing layers when placed in contact with absorptive materials remain largely unexplored. In this Letter, we investigate non-radiative Förster resonance energy transfer (FRET) between  centers and either monolayer graphene or 2D semiconductors. Strikingly, we find that the FRET rate is negligible for hBN sensing layers thicker than 3 nm, highlighting the potential of  centers for integration into ultra-thin quantum sensors within van der Waals heterostructures. Furthermore, we experimentally extract the intrinsic radiative decay rate of  defects.",
      "authors": [
        "Fraunié Jules",
        "Mikhail M. Glazov",
        "Sébastien Roux",
        "Abraao Cefas Torres-Dias",
        "Cora Crunteanu-Stanescu",
        "Tom Fournier",
        "Maryam S. Dehaghani",
        "Tristan Clua-Provost",
        "Delphine Lagarde",
        "Laurent Lombez",
        "Xavier Marie",
        "Benjamin Lassagne",
        "Thomas Poirier",
        "James H. Edgar",
        "Vincent Jacques",
        "Cedric Robert"
      ],
      "published_date": "2025-12-03",
      "full_text": "Non-radiative energy transfer between boron vacancies in hexagonal boron nitride and other 2D materials\n\nBoron vacancies () in hexagonal boron nitride (hBN) have emerged as a promising platform for two-dimensional quantum sensors capable of operating at atomic-scale proximity. However, the mechanisms responsible for photoluminescence quenching in thin hBN sensing layers when placed in contact with absorptive materials remain largely unexplored. In this Letter, we investigate non-radiative Förster resonance energy transfer (FRET) between  centers and either monolayer graphene or 2D semiconductors. Strikingly, we find that the FRET rate is negligible for hBN sensing layers thicker than 3 nm, highlighting the potential of  centers for integration into ultra-thin quantum sensors within van der Waals heterostructures. Furthermore, we experimentally extract the intrinsic radiative decay rate of  defects."
    },
    {
      "id": "2512.03953v1",
      "title": "Image Theory for the Single Bounce Quantum Gravimeter",
      "abstract": "We develop an image theory for the recently proposed single-bounce quantum gravimeter. Free fall and quantum bounce of a matter wave-packet are described through decompositions over a basis of continuous energies. This leads to a much clearer interpretation of the origin of quantum interferences, associated to semi-classical estimations. We then give new tools to explore the space of parameters, and discuss the expected accuracy of the free-fall acceleration measurement.",
      "authors": [
        "Joachim Guyomard",
        "Serge Reynaud",
        "Pierre Cladé"
      ],
      "published_date": "2025-12-03",
      "full_text": "Image Theory for the Single Bounce Quantum Gravimeter\n\nWe develop an image theory for the recently proposed single-bounce quantum gravimeter. Free fall and quantum bounce of a matter wave-packet are described through decompositions over a basis of continuous energies. This leads to a much clearer interpretation of the origin of quantum interferences, associated to semi-classical estimations. We then give new tools to explore the space of parameters, and discuss the expected accuracy of the free-fall acceleration measurement."
    },
    {
      "id": "2512.03935v1",
      "title": "Thermodynamics of an Open Symmetric Quantum System",
      "abstract": "For a subclass of a general symmetric Hamiltonian obeying anti-commutation relation with its conjugate, a Hermitian basis is found that spans the bi-orthonormal energy eigenvectors. Using the modified projectors constructed from these eigenvectors, the generalized density matrix of the symmetric evolution is calculated, and subsequently, ergotropy for a closed system is obtained. The symmetric system, in an open system scenario, is studied to understand ergotropy under different regimes of non-Hermiticity of the Hamiltonian. The consistency of the three laws of thermodynamics for the symmetric system in an open system scenario is also analyzed.",
      "authors": [
        "Baibhab Bose",
        "Devvrat Tiwari",
        "Subhashish Banerjee"
      ],
      "published_date": "2025-12-03",
      "full_text": "Thermodynamics of an Open Symmetric Quantum System\n\nFor a subclass of a general symmetric Hamiltonian obeying anti-commutation relation with its conjugate, a Hermitian basis is found that spans the bi-orthonormal energy eigenvectors. Using the modified projectors constructed from these eigenvectors, the generalized density matrix of the symmetric evolution is calculated, and subsequently, ergotropy for a closed system is obtained. The symmetric system, in an open system scenario, is studied to understand ergotropy under different regimes of non-Hermiticity of the Hamiltonian. The consistency of the three laws of thermodynamics for the symmetric system in an open system scenario is also analyzed."
    },
    {
      "id": "2512.03933v1",
      "title": "Phase-space open-systems dynamics of second-order nonlinear interactions with pulsed quantum light",
      "abstract": "The theoretical description of broadband, multimode quantum pulses undergoing a second-order -nonlinear interaction can be quite intricate, due to the large dimensionality of the underlying phase space. However, in many cases only a few broadband (temporal) modes are relevant before and after the nonlinear interaction. Here we present an efficient framework to calculate the relation between the quantum states at the input and output of a nonlinear element in their respective relevant modes. Since the number of relevant input and output modes may differ, resulting in an open quantum system, we introduce the generalized Bloch-Messiah decomposition (GBMD), reducing the description to an equal number of input and output modes. The GBMD enables us to calculate the multimode Wigner function of the output state by convolving the rescaled Wigner function of the reduced input quantum pulse with a multivariate Gaussian phase-space function. We expand on this result by considering two examples input states: A Fock state in a single broadband mode and a two-mode squeezed vacuum, both in the THz-frequency regime, up-converted to a single output broadband mode of optical frequencies. We investigate the effect, the convolution and thermalization due to entanglement breakage have on the output Wigner function by calculating the von Neumann entropy of the output Wigner function. The methods presented here can be used to optimize the amplification or frequency conversion of broadband quantum states, opening an avenue to the generation and characterization of optical quantum states on ultrafast time scales.",
      "authors": [
        "Emanuel Hubenschmid",
        "Victor Rueskov Christiansen"
      ],
      "published_date": "2025-12-03",
      "full_text": "Phase-space open-systems dynamics of second-order nonlinear interactions with pulsed quantum light\n\nThe theoretical description of broadband, multimode quantum pulses undergoing a second-order -nonlinear interaction can be quite intricate, due to the large dimensionality of the underlying phase space. However, in many cases only a few broadband (temporal) modes are relevant before and after the nonlinear interaction. Here we present an efficient framework to calculate the relation between the quantum states at the input and output of a nonlinear element in their respective relevant modes. Since the number of relevant input and output modes may differ, resulting in an open quantum system, we introduce the generalized Bloch-Messiah decomposition (GBMD), reducing the description to an equal number of input and output modes. The GBMD enables us to calculate the multimode Wigner function of the output state by convolving the rescaled Wigner function of the reduced input quantum pulse with a multivariate Gaussian phase-space function. We expand on this result by considering two examples input states: A Fock state in a single broadband mode and a two-mode squeezed vacuum, both in the THz-frequency regime, up-converted to a single output broadband mode of optical frequencies. We investigate the effect, the convolution and thermalization due to entanglement breakage have on the output Wigner function by calculating the von Neumann entropy of the output Wigner function. The methods presented here can be used to optimize the amplification or frequency conversion of broadband quantum states, opening an avenue to the generation and characterization of optical quantum states on ultrafast time scales."
    },
    {
      "id": "2512.03929v1",
      "title": "Rethinking Collapse: Coupling Quantum States to Classical Bits with quasi-probabilities",
      "abstract": "We propose a formulation of quantum measurement within a modified framework of frames, in which a quantum system - a single qubit - is directly coupled to a classical measurement bit. The qubit is represented as a positive probability distribution over two classical bits, a and a', denoted by p(aa'). The measurement apparatus is described by a classical bit , initialized in the pure distribution . The measurement interaction is modeled by a quasi-bistochastic process  - a bistochastic map that may include negative transition probabilities, while acting on an entirely positive state space. When this process acts on the joint initial state , it produces a collapsed state , yielding the measurement outcome  with the correct quantum-mechanical probability . This approach bypasses the von Neumann chain of infinite couplings by treating the measurement register classically, while capturing the nonclassical nature of measurement through the quasi-bistochastic structure of the interaction.",
      "authors": [
        "Dagomir Kaszlikowski",
        "Pawel Kurzynski"
      ],
      "published_date": "2025-12-03",
      "full_text": "Rethinking Collapse: Coupling Quantum States to Classical Bits with quasi-probabilities\n\nWe propose a formulation of quantum measurement within a modified framework of frames, in which a quantum system - a single qubit - is directly coupled to a classical measurement bit. The qubit is represented as a positive probability distribution over two classical bits, a and a', denoted by p(aa'). The measurement apparatus is described by a classical bit , initialized in the pure distribution . The measurement interaction is modeled by a quasi-bistochastic process  - a bistochastic map that may include negative transition probabilities, while acting on an entirely positive state space. When this process acts on the joint initial state , it produces a collapsed state , yielding the measurement outcome  with the correct quantum-mechanical probability . This approach bypasses the von Neumann chain of infinite couplings by treating the measurement register classically, while capturing the nonclassical nature of measurement through the quasi-bistochastic structure of the interaction."
    },
    {
      "id": "2512.03925v1",
      "title": "Towards Quantum Stochastic Optimization for Energy Systems under Uncertainty: Joint Chance Constraints with Quantum Annealing",
      "abstract": "Uncertainty is fundamental in modern power systems, where renewable generation and fluctuating demand make stochastic optimization indispensable. The chance constrained unit commitment problem (UCP) captures this uncertainty but rapidly becomes computationally challenging as the number of scenarios grows. Quantum computing has been proposed as a potential route to overcome such scaling barriers. In this work, we evaluate the applicability of quantum annealing platforms to the chance constrained UCP. Focusing on a scenario approximation, we reformulated the problem as a mixed integer linear program and solved it using DWave hybrid quantum classical solver alongside Gurobi. The hybrid solver proved competitive under strict runtime limits for large scenario sets (15,000 in our experiments), while Gurobi remained superior on smaller cases. QUBO reformulations were also tested, but current annealers cannot accommodate stochastic UCPs due to hardware limits, and deterministic cases suffered from embedding overhead. Our study delineates where chance constrained UCPs can already be addressed with hybrid quantum classical methods, and where current quantum annealers remain fundamentally limited.",
      "authors": [
        "David Ribes",
        "Tatiana Gonzalez Grandon"
      ],
      "published_date": "2025-12-03",
      "full_text": "Towards Quantum Stochastic Optimization for Energy Systems under Uncertainty: Joint Chance Constraints with Quantum Annealing\n\nUncertainty is fundamental in modern power systems, where renewable generation and fluctuating demand make stochastic optimization indispensable. The chance constrained unit commitment problem (UCP) captures this uncertainty but rapidly becomes computationally challenging as the number of scenarios grows. Quantum computing has been proposed as a potential route to overcome such scaling barriers. In this work, we evaluate the applicability of quantum annealing platforms to the chance constrained UCP. Focusing on a scenario approximation, we reformulated the problem as a mixed integer linear program and solved it using DWave hybrid quantum classical solver alongside Gurobi. The hybrid solver proved competitive under strict runtime limits for large scenario sets (15,000 in our experiments), while Gurobi remained superior on smaller cases. QUBO reformulations were also tested, but current annealers cannot accommodate stochastic UCPs due to hardware limits, and deterministic cases suffered from embedding overhead. Our study delineates where chance constrained UCPs can already be addressed with hybrid quantum classical methods, and where current quantum annealers remain fundamentally limited."
    },
    {
      "id": "2512.03924v1",
      "title": "Experimental Quantum Electronic Voting",
      "abstract": "Quantum information protocols offer significant advantages in properties such as security, anonymity, and privacy for communication and computing tasks. An application where guaranteeing the highest possible security and privacy is critical for democratic societies is electronic voting. As computational power continues to evolve, classical voting schemes may become increasingly vulnerable to information leakage. In this work, we present the experimental demonstration of an information-theoretically secure and efficient electronic voting protocol that, crucially, does not rely on election authorities, leveraging the unique properties of quantum states. Our experiment is based on a high-performance source of Greenberger-Horne-Zeilinger (GHZ) states and realizes a proof-of-principle implementation of the protocol in two scenarios: a configuration with four voters and two candidates employing privacy enhancement techniques and an election scenario supporting up to eight voters and sixteen candidates. The latter is particularly well-suited for secure board-level elections within organizations or small-scale governmental contexts.",
      "authors": [
        "Nicolas Laurent-Puig",
        "Matilde Baroni",
        "Federico Centrone",
        "Eleni Diamanti"
      ],
      "published_date": "2025-12-03",
      "full_text": "Experimental Quantum Electronic Voting\n\nQuantum information protocols offer significant advantages in properties such as security, anonymity, and privacy for communication and computing tasks. An application where guaranteeing the highest possible security and privacy is critical for democratic societies is electronic voting. As computational power continues to evolve, classical voting schemes may become increasingly vulnerable to information leakage. In this work, we present the experimental demonstration of an information-theoretically secure and efficient electronic voting protocol that, crucially, does not rely on election authorities, leveraging the unique properties of quantum states. Our experiment is based on a high-performance source of Greenberger-Horne-Zeilinger (GHZ) states and realizes a proof-of-principle implementation of the protocol in two scenarios: a configuration with four voters and two candidates employing privacy enhancement techniques and an election scenario supporting up to eight voters and sixteen candidates. The latter is particularly well-suited for secure board-level elections within organizations or small-scale governmental contexts."
    },
    {
      "id": "2512.03898v1",
      "title": "Polylogarithmic-Depth Quantum Algorithm for Simulating the Extended Hubbard Model on a Two-Dimensional Lattice Using the Fast Multipole Method",
      "abstract": "The extended Hubbard model on a two-dimensional lattice captures key physical phenomena, but is challenging to simulate due to the presence of long-range interactions. In this work, we present an efficient quantum algorithm for simulating the time evolution of this model. Our approach, inspired by the fast multipole method, approximates pairwise interactions by interactions between hierarchical levels of coarse-graining boxes. We discuss how to leverage recent advances in two-dimensional neutral atom quantum computing, supporting non-local operations such as long-range gates and shuttling. The resulting circuit depth for a single Trotter step scales polylogarithmically with system size.",
      "authors": [
        "Yu Wang",
        "Martina Nibbi",
        "Maxine Luo",
        "Isabel Nha Minh Le",
        "Yanbin Chen",
        "J. Ignacio Cirac",
        "Christian Mendl"
      ],
      "published_date": "2025-12-03",
      "full_text": "Polylogarithmic-Depth Quantum Algorithm for Simulating the Extended Hubbard Model on a Two-Dimensional Lattice Using the Fast Multipole Method\n\nThe extended Hubbard model on a two-dimensional lattice captures key physical phenomena, but is challenging to simulate due to the presence of long-range interactions. In this work, we present an efficient quantum algorithm for simulating the time evolution of this model. Our approach, inspired by the fast multipole method, approximates pairwise interactions by interactions between hierarchical levels of coarse-graining boxes. We discuss how to leverage recent advances in two-dimensional neutral atom quantum computing, supporting non-local operations such as long-range gates and shuttling. The resulting circuit depth for a single Trotter step scales polylogarithmically with system size."
    },
    {
      "id": "2512.03863v1",
      "title": "Laser-induced modulation of conductance in graphene with magnetic barriers",
      "abstract": "We study how electrons move across a graphene sheet when it encounters two magnetic barriers with a region in between that is continuously driven by laser light. Rather than acting as a static obstacle, this illuminated middle section becomes a Floquet cavity that opens new transport channels through controlled photon absorption and emission. By combining Floquet theory with the transfer matrix method, we track electron transmission through both the main energy band and the emerging photon-assisted sidebands. We find that the laser does more than modify the potential--it reshapes how electrons interact between the magnetic barriers, enabling a switch from ordinary transmission to transport dominated by photon exchange. Because the magnetic field and the optical drive are applied to separate sections of the device, the system supports interference between cyclotron-filtered motion and discrete photon-pumping channels, producing Fano resonances and angle-dependent transmission zeros that cannot appear in double magnetic or double laser barrier systems alone. Under well-defined conditions, the distance between the magnetic barriers controls the coupling between Floquet channels, allowing highly tunable resonances and even perfect transmission, despite strong magnetic confinement. We also observe that low-energy carriers are efficiently blocked by the magnetic regions, while conductance steadily rises with energy until it reaches a clear saturation plateau. This hybrid design provides a versatile way to steer graphene electrons by balancing optical pumping and magnetic momentum filtering.",
      "authors": [
        "Rachid El Aitouni",
        "Miloud Mekkaoui",
        "Pablo Díaz",
        "David Laroze",
        "Ahmed Jellal"
      ],
      "published_date": "2025-12-03",
      "full_text": "Laser-induced modulation of conductance in graphene with magnetic barriers\n\nWe study how electrons move across a graphene sheet when it encounters two magnetic barriers with a region in between that is continuously driven by laser light. Rather than acting as a static obstacle, this illuminated middle section becomes a Floquet cavity that opens new transport channels through controlled photon absorption and emission. By combining Floquet theory with the transfer matrix method, we track electron transmission through both the main energy band and the emerging photon-assisted sidebands. We find that the laser does more than modify the potential--it reshapes how electrons interact between the magnetic barriers, enabling a switch from ordinary transmission to transport dominated by photon exchange. Because the magnetic field and the optical drive are applied to separate sections of the device, the system supports interference between cyclotron-filtered motion and discrete photon-pumping channels, producing Fano resonances and angle-dependent transmission zeros that cannot appear in double magnetic or double laser barrier systems alone. Under well-defined conditions, the distance between the magnetic barriers controls the coupling between Floquet channels, allowing highly tunable resonances and even perfect transmission, despite strong magnetic confinement. We also observe that low-energy carriers are efficiently blocked by the magnetic regions, while conductance steadily rises with energy until it reaches a clear saturation plateau. This hybrid design provides a versatile way to steer graphene electrons by balancing optical pumping and magnetic momentum filtering."
    },
    {
      "id": "2512.03853v1",
      "title": "Modelling the Impact of Device Imperfections on Electron Shuttling in SiMOS devices",
      "abstract": "Extensive theoretical and experimental work has established high-fidelity electron shuttling in Si/SiGe systems, whereas demonstrations in Si/SiO2 (SiMOS) remain at an early stage. To help address this, we perform full 3D simulations of conveyor-belt charge shuttling in a realistic SiMOS device, building on earlier 2D modelling. We solve the Poisson and time-dependent Schrodinger equations for varying shuttling speeds and gate voltages, focusing on potential pitfalls of typical SiMOS devices such as oxide-interface roughness, gate fabrication imperfections, and charge defects along the transport path. The simulations reveal that for low clavier-gate voltages, the additional oxide screening in multi-layer gate architectures causes conveyor-belt shuttling to collapse to the bucket-brigade mode, inducing considerable orbital excitation in the process. Increasing the confinement restores conveyor-belt operation, which we find to be robust against interface roughness, gate misalignment, and charge defects buried in the oxide. However, our results indicate that defects located at the Si/SiO2-interface can induce considerable orbital excitation. For lower conveyor gate biases, positive defects in the transport channel can even capture passing electrons. Hence we identify key challenges and find operating regimes for reliable charge transport in SiMOS architectures.",
      "authors": [
        "Jack J. Turner",
        "Christian W. Binder",
        "Guido Burkard",
        "Andrew J. Fisher"
      ],
      "published_date": "2025-12-03",
      "full_text": "Modelling the Impact of Device Imperfections on Electron Shuttling in SiMOS devices\n\nExtensive theoretical and experimental work has established high-fidelity electron shuttling in Si/SiGe systems, whereas demonstrations in Si/SiO2 (SiMOS) remain at an early stage. To help address this, we perform full 3D simulations of conveyor-belt charge shuttling in a realistic SiMOS device, building on earlier 2D modelling. We solve the Poisson and time-dependent Schrodinger equations for varying shuttling speeds and gate voltages, focusing on potential pitfalls of typical SiMOS devices such as oxide-interface roughness, gate fabrication imperfections, and charge defects along the transport path. The simulations reveal that for low clavier-gate voltages, the additional oxide screening in multi-layer gate architectures causes conveyor-belt shuttling to collapse to the bucket-brigade mode, inducing considerable orbital excitation in the process. Increasing the confinement restores conveyor-belt operation, which we find to be robust against interface roughness, gate misalignment, and charge defects buried in the oxide. However, our results indicate that defects located at the Si/SiO2-interface can induce considerable orbital excitation. For lower conveyor gate biases, positive defects in the transport channel can even capture passing electrons. Hence we identify key challenges and find operating regimes for reliable charge transport in SiMOS architectures."
    },
    {
      "id": "2512.03850v1",
      "title": "Density of states of quantum systems from free probability theory: a brief overview",
      "abstract": "We provide a brief overview of approaches for calculating the density of states of quantum systems and random matrix Hamiltonians using the tools of free probability theory. For a given Hamiltonian of a quantum system or a generic random matrix Hamiltonian, which can be written as a sum of two non-commutating operators, one can obtain an expression for the density of states of the Hamiltonian from the known density of states of the two component operators by assuming that these operators are mutually free and by using the free additive convolution. In many examples of interacting quantum systems and random matrix models, this procedure is known to provide a reasonably accurate approximation to the exact numerical density of states. We review some of the examples that are known in the literature where this procedure works very well, and also discuss some of the limitations of this method in situations where the free probability approximation fails to provide a sufficiently accurate description of the exact density of states. Subsequently, we describe a perturbation scheme that can be developed from the subordination formulas for the Cauchy transform of the density of states and use it to obtain approximate analytical expressions for the density of states in various models, such as the Rosenzweig-Porter random matrix ensemble and the Anderson model with on-site disorder.",
      "authors": [
        "Keun-Young Kim",
        "Kuntal Pal"
      ],
      "published_date": "2025-12-03",
      "full_text": "Density of states of quantum systems from free probability theory: a brief overview\n\nWe provide a brief overview of approaches for calculating the density of states of quantum systems and random matrix Hamiltonians using the tools of free probability theory. For a given Hamiltonian of a quantum system or a generic random matrix Hamiltonian, which can be written as a sum of two non-commutating operators, one can obtain an expression for the density of states of the Hamiltonian from the known density of states of the two component operators by assuming that these operators are mutually free and by using the free additive convolution. In many examples of interacting quantum systems and random matrix models, this procedure is known to provide a reasonably accurate approximation to the exact numerical density of states. We review some of the examples that are known in the literature where this procedure works very well, and also discuss some of the limitations of this method in situations where the free probability approximation fails to provide a sufficiently accurate description of the exact density of states. Subsequently, we describe a perturbation scheme that can be developed from the subordination formulas for the Cauchy transform of the density of states and use it to obtain approximate analytical expressions for the density of states in various models, such as the Rosenzweig-Porter random matrix ensemble and the Anderson model with on-site disorder."
    },
    {
      "id": "2512.03808v1",
      "title": "Solution of the Electric Field Integral Equation Using a Hybrid Quantum-Classical Scheme: Investigation of Accuracy and Efficiency",
      "abstract": "Conventional classical solvers are commonly used for solving matrix equation systems resulting from the discretization of SIEs in computational electromagnetics (CEM). However, the memory requirement would become a bottleneck for classical computing as the electromagentic problems become much larger. As an alternative, quantum computing has a natural \"parallelization\" advantage with much lower storage complexity due to the superposition and entanglement in quantum mechanics. Even though several quantum algorithms have been applied for the SIEs-based methods in the literature, the size of the matrix equation systems solvable using them is still limited. In this work, we use a hybrid quantum-classical scheme to solve the EFIE for analyzing electromagentic scattering from three-dimensional (3D) perfect electrically conducting objects with arbitrary shapes in CEM for the first time. Instead of directly solving the original EFIE matrix equation system using the quantum algorithms, the hybrid scheme first designs the preconditioned linear system and then uses a double-layer iterative strategy for its solution, where the external iteration layer builds subspace matrix equation systems with smaller dimension and the internal iteration layer solves the smaller systems using the quantum algorithms. Two representative quantum algorithms, HHL and VQLS, are considered in this work, which are executed on the quantum simulator and quantum computer platforms. We present the theoretical time complexity analysis of the hybrid quantum-classical scheme and perform numerical experiments to investigate the accuracy and efficiency of the hybrid scheme. The results show that the computational complexity of the hybrid VQLS-classical scheme is lower than the conventional fast solvers in classical computing, which indicates the hybrid scheme is more promising for analyzing large-scale electromagnetic problems.",
      "authors": [
        "Rui Chen",
        "Teng-Yang Ma",
        "Meng-Han Dou",
        "Chao-Fu Wang"
      ],
      "published_date": "2025-12-03",
      "full_text": "Solution of the Electric Field Integral Equation Using a Hybrid Quantum-Classical Scheme: Investigation of Accuracy and Efficiency\n\nConventional classical solvers are commonly used for solving matrix equation systems resulting from the discretization of SIEs in computational electromagnetics (CEM). However, the memory requirement would become a bottleneck for classical computing as the electromagentic problems become much larger. As an alternative, quantum computing has a natural \"parallelization\" advantage with much lower storage complexity due to the superposition and entanglement in quantum mechanics. Even though several quantum algorithms have been applied for the SIEs-based methods in the literature, the size of the matrix equation systems solvable using them is still limited. In this work, we use a hybrid quantum-classical scheme to solve the EFIE for analyzing electromagentic scattering from three-dimensional (3D) perfect electrically conducting objects with arbitrary shapes in CEM for the first time. Instead of directly solving the original EFIE matrix equation system using the quantum algorithms, the hybrid scheme first designs the preconditioned linear system and then uses a double-layer iterative strategy for its solution, where the external iteration layer builds subspace matrix equation systems with smaller dimension and the internal iteration layer solves the smaller systems using the quantum algorithms. Two representative quantum algorithms, HHL and VQLS, are considered in this work, which are executed on the quantum simulator and quantum computer platforms. We present the theoretical time complexity analysis of the hybrid quantum-classical scheme and perform numerical experiments to investigate the accuracy and efficiency of the hybrid scheme. The results show that the computational complexity of the hybrid VQLS-classical scheme is lower than the conventional fast solvers in classical computing, which indicates the hybrid scheme is more promising for analyzing large-scale electromagnetic problems."
    },
    {
      "id": "2512.03788v1",
      "title": "Quantum Algorithm for Searching for the Longest Segment and the Largest Empty Rectangle",
      "abstract": "In the paper, we consider the problem of searching for the Largest empty rectangle in a 2D map, and the one-dimensional version of the problem is the problem of searching for the largest empty segment. We present a quantum algorithm for the Largest Empty Square problem and the Largest Empty Rectangle of a fixed width  for -rectangular map. Query complexity of the algorithm is  for the square case, and  for the rectangle with a fixed width  case, respectively. At the same time, the lower bounds for the classical case are , and , respectively. The Quantum algorithm for the one-dimensional version of the problem has  query complexity. The quantum lower bound for the problem is  which is almost equal to the upper bound up to a log factor. The classical lower bound is . So, we obtain the quadratic speed-up for the problem.",
      "authors": [
        "Kamil Khadiev",
        "Vladislav Remidovskii",
        "Timur Bikmullin",
        "Aliya Khadieva"
      ],
      "published_date": "2025-12-03",
      "full_text": "Quantum Algorithm for Searching for the Longest Segment and the Largest Empty Rectangle\n\nIn the paper, we consider the problem of searching for the Largest empty rectangle in a 2D map, and the one-dimensional version of the problem is the problem of searching for the largest empty segment. We present a quantum algorithm for the Largest Empty Square problem and the Largest Empty Rectangle of a fixed width  for -rectangular map. Query complexity of the algorithm is  for the square case, and  for the rectangle with a fixed width  case, respectively. At the same time, the lower bounds for the classical case are , and , respectively. The Quantum algorithm for the one-dimensional version of the problem has  query complexity. The quantum lower bound for the problem is  which is almost equal to the upper bound up to a log factor. The classical lower bound is . So, we obtain the quadratic speed-up for the problem."
    },
    {
      "id": "2512.03770v1",
      "title": "Quantum Simulations of Opinion Dynamics",
      "abstract": "Quantum computing offers powerful new approaches for modeling complex social phenomena. Here, we propose and demonstrate quantum simulations of opinion dynamics, leveraging quantum superposition, measurement-induced state collapse, and entanglement to model realistic psychological and social processes. Specifically, we develop quantum models of opinion dynamics, solving exactly and simulating on IBM Quantum hardware. Our results, based on quantum devices and validated with practical quantum circuits, illustrate how quantum effects can enhance understanding of consensus formation, polarization, and collective decision-making. These findings pave the way for further exploration into quantum-enhanced social modeling, highlighting the potential of near-term quantum computers for simulating collective behavior in complex systems.",
      "authors": [
        "Xingyu Guo",
        "Xiaoyang Wang",
        "Lingxiao Wang"
      ],
      "published_date": "2025-12-03",
      "full_text": "Quantum Simulations of Opinion Dynamics\n\nQuantum computing offers powerful new approaches for modeling complex social phenomena. Here, we propose and demonstrate quantum simulations of opinion dynamics, leveraging quantum superposition, measurement-induced state collapse, and entanglement to model realistic psychological and social processes. Specifically, we develop quantum models of opinion dynamics, solving exactly and simulating on IBM Quantum hardware. Our results, based on quantum devices and validated with practical quantum circuits, illustrate how quantum effects can enhance understanding of consensus formation, polarization, and collective decision-making. These findings pave the way for further exploration into quantum-enhanced social modeling, highlighting the potential of near-term quantum computers for simulating collective behavior in complex systems."
    },
    {
      "id": "2512.03769v1",
      "title": "Metrological Sensitivity beyond Gaussian Limits with Cubic Phase States",
      "abstract": "Cubic phase states provide the essential non-Gaussian resource for continuous-variable quantum computing. We show that they also offer significant potential for quantum metrology, surpassing the phase-sensing sensitivity of all Gaussian states at equal average photon number. Optimal sensitivity requires only moderate initial squeezing, and the non-Gaussian advantage remains robust against loss and detection noise. We identify optimal measurement strategies and show that several experimentally relevant preparation schemes surpass Gaussian limits, in some cases reaching the sensitivity of cubic phase states. Our results establish cubic phase states as a promising resource for quantum-enhanced precision measurements beyond Gaussian limits.",
      "authors": [
        "Jiajie Guo",
        "Shuheng Liu",
        "Boxuan Jing",
        "Qiongyi He",
        "Manuel Gessner"
      ],
      "published_date": "2025-12-03",
      "full_text": "Metrological Sensitivity beyond Gaussian Limits with Cubic Phase States\n\nCubic phase states provide the essential non-Gaussian resource for continuous-variable quantum computing. We show that they also offer significant potential for quantum metrology, surpassing the phase-sensing sensitivity of all Gaussian states at equal average photon number. Optimal sensitivity requires only moderate initial squeezing, and the non-Gaussian advantage remains robust against loss and detection noise. We identify optimal measurement strategies and show that several experimentally relevant preparation schemes surpass Gaussian limits, in some cases reaching the sensitivity of cubic phase states. Our results establish cubic phase states as a promising resource for quantum-enhanced precision measurements beyond Gaussian limits."
    },
    {
      "id": "2512.03758v1",
      "title": "An end-to-end quantum algorithm for nonlinear fluid dynamics with bounded quantum advantage",
      "abstract": "Computational fluid dynamics (CFD) is a cornerstone of classical scientific computing, and there is growing interest in whether quantum computers can accelerate such simulations. To date, the existing proposals for fault-tolerant quantum algorithms for CFD have almost exclusively been based on the Carleman embedding method, used to encode nonlinearities on a quantum computer. In this work, we begin by showing that these proposals suffer from a range of severe bottlenecks that negate conjectured quantum advantages: lack of convergence of the Carleman method, prohibitive time-stepping requirements, unfavorable condition number scaling, and inefficient data extraction. With these roadblocks clearly identified, we develop a novel algorithm for the incompressible lattice Boltzmann equation that circumvents these obstacles, and then provide a detailed analysis of our algorithm, including all potential sources of algorithmic complexity, as well as gate count estimates. We find that for an end-to-end problem, a modest quantum advantage may be preserved for selected observables in the high-error-tolerance regime. We lower bound the Reynolds number scaling of our quantum algorithm in dimension  at Kolmogorov microscale resolution with , where  is a multiplicative overhead for data extraction with  for the drag force. This upper bounds the scaling improvement over classical algorithms by . However, our numerical investigations suggest a lower speedup, with a scaling estimate of  for . Our results give robust evidence that small, but nontrivial, quantum advantages can be achieved in the context of CFD, and motivate the need for additional rigorous end-to-end quantum algorithm development.",
      "authors": [
        "David Jennings",
        "Kamil Korzekwa",
        "Matteo Lostaglio",
        "Richard Ashworth",
        "Emanuele Marsili",
        "Stephen Rolston"
      ],
      "published_date": "2025-12-03",
      "full_text": "An end-to-end quantum algorithm for nonlinear fluid dynamics with bounded quantum advantage\n\nComputational fluid dynamics (CFD) is a cornerstone of classical scientific computing, and there is growing interest in whether quantum computers can accelerate such simulations. To date, the existing proposals for fault-tolerant quantum algorithms for CFD have almost exclusively been based on the Carleman embedding method, used to encode nonlinearities on a quantum computer. In this work, we begin by showing that these proposals suffer from a range of severe bottlenecks that negate conjectured quantum advantages: lack of convergence of the Carleman method, prohibitive time-stepping requirements, unfavorable condition number scaling, and inefficient data extraction. With these roadblocks clearly identified, we develop a novel algorithm for the incompressible lattice Boltzmann equation that circumvents these obstacles, and then provide a detailed analysis of our algorithm, including all potential sources of algorithmic complexity, as well as gate count estimates. We find that for an end-to-end problem, a modest quantum advantage may be preserved for selected observables in the high-error-tolerance regime. We lower bound the Reynolds number scaling of our quantum algorithm in dimension  at Kolmogorov microscale resolution with , where  is a multiplicative overhead for data extraction with  for the drag force. This upper bounds the scaling improvement over classical algorithms by . However, our numerical investigations suggest a lower speedup, with a scaling estimate of  for . Our results give robust evidence that small, but nontrivial, quantum advantages can be achieved in the context of CFD, and motivate the need for additional rigorous end-to-end quantum algorithm development."
    },
    {
      "id": "2512.03748v1",
      "title": "Widefield Quantum Sensor for Vector Magnetic Field Imaging of Micromagnetic Structures",
      "abstract": "Many spintronic, magnetic-memory, and neuromorphic devices rely on spatially varying magnetic fields. Quantitatively imaging these fields with full vector information over extended areas remains a major challenge. Existing probes either offer nanoscale resolution at the cost of slow scanning, or widefield imaging with limited vector sensitivity or material constraints. Quantum sensing with nitrogen-vacancy (NV) centers in diamond promises to bridge this gap, but a practical camera-based vector magnetometry implementation on relevant microstructures has not been demonstrated. Here we adapt a commercial widefield microscope to implement a camera-compatible pulsed optically detected magnetic resonance protocol to reconstruct stray-field vectors from microscale devices. By resolving the Zeeman shifts of the four NV orientations, we reconstruct the stray-field vector generated by microfabricated permalloy structures that host multiple stable remanent states. Our implementation achieves a spatial resolution of  across an  field of view and a peak sensitivity of , with acquisition times of only a few minutes. These results establish pulsed widefield NV magnetometry on standard microscopes as a practical and scalable tool for routine vector-resolved imaging of complex magnetic devices.",
      "authors": [
        "Orlando D. Cunha",
        "Filipe Camarneiro",
        "João P. Silva",
        "Hariharan Nhalil",
        "Ariel Zaig",
        "Lior Klein",
        "Jana B. Nieder"
      ],
      "published_date": "2025-12-03",
      "full_text": "Widefield Quantum Sensor for Vector Magnetic Field Imaging of Micromagnetic Structures\n\nMany spintronic, magnetic-memory, and neuromorphic devices rely on spatially varying magnetic fields. Quantitatively imaging these fields with full vector information over extended areas remains a major challenge. Existing probes either offer nanoscale resolution at the cost of slow scanning, or widefield imaging with limited vector sensitivity or material constraints. Quantum sensing with nitrogen-vacancy (NV) centers in diamond promises to bridge this gap, but a practical camera-based vector magnetometry implementation on relevant microstructures has not been demonstrated. Here we adapt a commercial widefield microscope to implement a camera-compatible pulsed optically detected magnetic resonance protocol to reconstruct stray-field vectors from microscale devices. By resolving the Zeeman shifts of the four NV orientations, we reconstruct the stray-field vector generated by microfabricated permalloy structures that host multiple stable remanent states. Our implementation achieves a spatial resolution of  across an  field of view and a peak sensitivity of , with acquisition times of only a few minutes. These results establish pulsed widefield NV magnetometry on standard microscopes as a practical and scalable tool for routine vector-resolved imaging of complex magnetic devices."
    },
    {
      "id": "2512.03740v1",
      "title": "Quantum Max Cut for complete tripartite graphs",
      "abstract": "The Quantum Max--Cut (-QMC) problem is a special instance of a -local Hamiltonian problem, representing the quantum analog of the classical Max--Cut problem. The -QMC problem seeks the largest eigenvalue of a Hamiltonian defined on a graph with  vertices, where edges correspond to swap operators acting on . In recent years, progress has been made by investigating the algebraic structure of the -QMC Hamiltonian. Building on this approach, this article solves the -QMC problem for complete tripartite graphs for small local dimensions, .",
      "authors": [
        "Tea Štrekelj"
      ],
      "published_date": "2025-12-03",
      "full_text": "Quantum Max Cut for complete tripartite graphs\n\nThe Quantum Max--Cut (-QMC) problem is a special instance of a -local Hamiltonian problem, representing the quantum analog of the classical Max--Cut problem. The -QMC problem seeks the largest eigenvalue of a Hamiltonian defined on a graph with  vertices, where edges correspond to swap operators acting on . In recent years, progress has been made by investigating the algebraic structure of the -QMC Hamiltonian. Building on this approach, this article solves the -QMC problem for complete tripartite graphs for small local dimensions, ."
    }
  ]
}