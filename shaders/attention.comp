#version 450

layout(local_size_x = 256, local_size_y = 1, local_size_z = 1) in;

// Buffers for Q, K, V and Output
// Q: [batch_size, seq_len, num_heads, head_dim]
// K: [batch_size, seq_len, num_key_value_heads, head_dim]
// V: same
// O: same as Q
layout(binding = 0) readonly buffer Query { float Q[]; };
layout(binding = 1) readonly buffer Key { float K[]; };
layout(binding = 2) readonly buffer Value { float V[]; };
layout(binding = 3) writeonly buffer Output { float O[]; };

// Uniforms for dimensions and parameters
layout(binding = 4) uniform Params {
    uint batch_size;
    uint seq_len;
    uint head_dim;
    uint num_heads;
    uint num_key_value_heads;
    bool causal;
} params;

// Shared memory for attention scores within a workgroup
shared float shared_scores[256];
shared float shared_max_val;
shared float shared_sum_exp;

void main() {
    uint global_id = gl_GlobalInvocationID.x;
    uint local_id = gl_LocalInvocationID.x;
    uint workgroup_id = gl_WorkGroupID.x;

    // Each workgroup handles one (batch, head, seq_i) attention computation
    uint total_workgroups = params.batch_size * params.num_heads * params.seq_len;
    if (workgroup_id >= total_workgroups) return;

    // Compute workgroup indices
    uint batch = workgroup_id / (params.num_heads * params.seq_len);
    uint head = (workgroup_id / params.seq_len) % params.num_heads;
    uint seq_i = workgroup_id % params.seq_len;

    // Step 1: Compute attention scores for all j in this sequence
    if (local_id < params.seq_len) {
        uint j = local_id;
        float score = 0.0;

        // Compute QK dot product
        for (uint d = 0; d < params.head_dim; ++d) {
            uint q_idx = batch * params.seq_len * params.num_heads * params.head_dim +
                         seq_i * params.num_heads * params.head_dim +
                         head * params.head_dim + d;
            uint k_idx = batch * params.seq_len * params.num_key_value_heads * params.head_dim +
                         j * params.num_key_value_heads * params.head_dim +
                         head * params.head_dim + d;
            score += Q[q_idx] * K[k_idx];
        }

        score /= sqrt(float(params.head_dim));

        // Apply causal mask
        if (params.causal && j > seq_i) {
            score = -1e30f;
        }

        shared_scores[j] = score;
    }

    // Synchronize to ensure all scores are computed
    barrier();

    // Step 2: Find max value for numerical stability (first thread only)
    if (local_id == 0) {
        float max_val = -1e30f;
        for (uint j = 0; j < params.seq_len; ++j) {
            max_val = max(max_val, shared_scores[j]);
        }
        shared_max_val = max_val;

        // Compute sum of exp(scores - max)
        float sum_exp = 0.0f;
        for (uint j = 0; j < params.seq_len; ++j) {
            sum_exp += exp(shared_scores[j] - max_val);
        }
        shared_sum_exp = sum_exp;

        // Normalize scores to get attention weights
        for (uint j = 0; j < params.seq_len; ++j) {
            shared_scores[j] = exp(shared_scores[j] - max_val) / sum_exp;
        }
    }

    // Synchronize to ensure softmax is complete
    barrier();

    // Step 3: Compute weighted sum with values
    // Each thread handles one dimension of the output
    for (uint d = local_id; d < params.head_dim; d += 256) {
        float out_val = 0.0f;
        for (uint j = 0; j < params.seq_len; ++j) {
            uint v_idx = batch * params.seq_len * params.num_key_value_heads * params.head_dim +
                         j * params.num_key_value_heads * params.head_dim +
                         head * params.head_dim + d;
            out_val += shared_scores[j] * V[v_idx];
        }

        uint o_idx = batch * params.seq_len * params.num_heads * params.head_dim +
                     seq_i * params.num_heads * params.head_dim +
                     head * params.head_dim + d;
        O[o_idx] = out_val;
    }
}
